---
title: MIT-RF系列论文
mathjax: true
tags:
- MIT
- 姿态识别
- RF signal
categories:
- 学习记录
- 论文笔记
---


本文系MIT实验室的一系列文章的笔记，主要有RF-pose，RF-capture以及RFpose3D。

<!-- more -->

## RF pose

RFpose这篇文章原名是

监控摄像头+RF信号，同步采集，手工标注2000张RGB图像。可见RFpose 的AP 62.4 %。Vision based的方法是 68.8%。不可见AP情况下准确度是58.1%，而用skelton去识别多用户可达83%准确率。

RF是用Bottom-up的方法去做，即先检测keypoints，然后聚类。

**自底向上的方法：**首先从[射频](https://so.csdn.net/so/search?q=射频&spm=1001.2101.3001.7020)信号图像中学习检测出姿态的所有关键点，然后使用后处理来关联属于同一个人的关键点。但区别在于RFpose用的是RF信号作为输入。最终用一个跨模态-师生网络解决。

**跨模态师生网络：**它将以一种数据模态学到的知识转移到另一种数据模态。过去的工作只传输分类层次的判别知识，而这一网络在密集的关键点置信度图上传输更丰富的知识。



定位用的波段分为两类：高频如毫米波太赫兹，低频如GHz。高频的不易穿透，低频的有干扰的问题。



主动感知的一个方法：高精度的感知+多个设备完成高精度的识别任务。

被动感知依赖反射。

一些特性：

- 能穿墙的RF波空间分辨率很低，大概在几十厘米。和FMCW的贷款和天线阵列的间距有关。角度分辨率是15度。
- 穿墙的波正好会被人体反射。波长比物体表面大的时候就可以做到。波大概是5cm的波长。

### 算法流程

![image-20230331161441976](https://s2.loli.net/2023/03/31/9vgEw6URfGC1Dmu.png)

图像输入的Teacher Network作为监督者，预测输出14个keypoint的置信度，用于指导以RF信号作为输入的Student Network的网络学习。学生网络的目的是最小化和教师网络的输出差距：
$$
\min_\mathrm{S}\sum_{(\mathrm{I,R})}L(\mathrm{T(I),S(R)})
$$
损失函数使用了一个CELoss：
$$
L(\mathrm{T,S})=-\sum_c\sum_{i,j}\mathrm{S}_{i,j}^{c}\log \mathrm{T}_{i,j}^{c}+(1-\mathrm{S}_{i,j}^{c})\log(1-\mathrm{T}_{i,j}^{c})
$$
$\mathrm{T}_{i,j}^{c}$和$\mathrm{S}_{i,j}^{c}$都是第$i,j$个像素的置信度映射$c$的分数。

#### 穿透影响：

由于RF的频段正好在穿透人体的范围里因此不能只从单一的RF帧中进行姿态的预计，因为可能会有确实部分的肢体信息，而且RF信号的空间分辨率比较低。作者的方法是不使用单个帧作为输入，而是让网络查看帧序列。当网络一次查看多个RF帧的剪辑时，它仍然为输入中的每一帧输出一个姿态估计。

#### 时空影响：

希望网络在空间和时间上不受平移的影响，这样它就可以从可见场景推广到贯穿墙的场景。因此使用时空卷积( spatio-temoral convolutions )作为学生网络的基本构建块。

#### 输出方法：

模型必须首先学习在原始空间中没有编码的RF信号中的信息表示，然后将该表示解码为摄像机视图中的关键点。因此，如图3所示，学生网络结构：1) 两个分别用于水平和垂直热图流的RF编码网络Eh(·)和Ev(·)；2) 一个位姿解码网络D(·)，它以水平和垂直RF编码的信道级联作为输入，预测关键点置信度图。RF编码网络使用条纹卷积网络( strided convolutional networks )来去除空间维度，以便从原始视图中总结信息。位姿解码网络使用微条纹卷积网络( fractionally strided convolutional networks )来解码相机视图中的关键点。

### 训练细节

**RF encoding network：**RF编码网络，每个编码网络需要100帧(3.3秒)射频热图作为输入。RF编码网络在空间维度上每隔一层采用9×5×5的时空卷积( spatio-temporal convolutions )，步长1×2×2，共10层。在每一层之后执行ReLU激活函数进行批处理规范化。

**Pose decoding network：**位姿解码网络，将时空卷积和微条纹卷积结合起来解码姿态。解码网络有4层，3×6×6，分步步长为1×1/2×1/2，最后一层分步步长为1×1/4×1/4。除了输出层，在每一层之后都使用参数ReLu，在输出层中使用sigmoid。

**Training Details：**用存储实部和虚部的两个实部通道表示一个复值射频热图。使用的批大小为24。网络是在PyTorch中实现的。首先对关键点置信度图进行非最大抑制，得到候选关键点的离散峰。为了将不同人的关键点关联起来，使用参考文献[10]中提到的松弛法，并使用欧氏距离表示两个候选点的权重。根据学习到的关键点置信度图逐帧执行关联。将关键点映射到骨架。

### 实验结果

使用70%的数据用于训练，剩余的30%用于测试验证。使用object keypoint similarity（OKS）作为准确值的评估指标。

![image-20230331165548606](https://s2.loli.net/2023/03/31/NCxyUDk6clebjsS.png)

OKS在0.7左右的时候是性能最好，和视觉模型对比好在没有海报之类的误报。之后的精读就下降了，这是因为RF信号本身的低空间分辨率的原因。RFpose的关键点定位能力是由于不完美的热力图于真实图片的同步产生的。表二分析了身体部分的AP，在头部和颈部、臀部的性能表现较好，四肢部分表现较差，这是因为肢体大小的原因。（头部的反射空间更大、动作幅度较小），另外还有一些比OpenPose性能好的原因可能是RFpose观察的是一段序列，而openpose是观察单张图片。

<img src="https://s2.loli.net/2023/03/31/QbNHX28VIzDqJ4a.png" alt="image-20230331170557927" style="zoom:50%;" />

## RF-Capture（待补充）

RF-capture





## RFpose3D

《RF-Based 3D Skeletons》

首个从RF信号中推断人体三维骨架的工作（2018年），穿墙、多人场景都可实现。使用了一个新型的CNN架构，首先先放大聚焦场景中的个体，然后对每一个个体定位他们的身体部位，在空间上的平均误差为4.2cm,4.0cm以及4.9cm（x,y,z轴）。

文章旨在通过从人体反射的信号进行挖掘场景的任务丰富细节信息，而不仅仅是单纯的定位。3D的识别是很难的，关键在于建模观测的波形和人体，同时也要考虑不同人体部位的位置和动作。采取FMCW波作为输入，滑动窗口时间为3秒，产生一个连续的3D场景人体骨架运动图，骨架的关键点有14个，对应neck, shoulders, elbows, wrists, hip, knees, and feet。

### 网络的设计

输入的RF信号是四维的，包含了空间和时间的信息。因此使用了四维卷积层（自行搭建），考虑到分解特性和计算I/O的问题etc.，将四维的数据拆分成在两个平面（纵向和水平）和时间维度上的三维卷积。同时CNN的训练和推理也在这两个平面上进行，方法经实验验证有效并且和四维卷积等价。CNN部分的介绍直接跳过，FMCW部分的内容如下：

天线阵列和FMCW的组合可以测量三维空间中的每一个体素，本文使用的布局是一根发送天线，两个一维接收天线阵列。从$(x,y,z)$反射的信号可被计算如下：
$$
a(x,y,z,t)=\sum_k \sum_is_{k,i}^{t}\cdot e^{j2\pi \frac{d_k(x,y,z)}{\lambda_i}}
$$
其中$s_{k,i}^{t}$是在时刻$t$下第$k$个接收天线第$i$个FMCW的扫描样本。$\lambda_i$是这一样本对应的扫描波长，$d_k(x,y,z)$是发射天线到空间点$(x,y,z)$并返回到第$k$根接收天线的round-trip距离。

网络由RPN和CNN两部分组成，前者用于产生定位框，后者用于生成骨架。

### 多人场景的尺度变换

多人场景下需要一个可以将人区分开来的元素。可选方法有：

* 先定位然后对每一定位结果的位置进行信号放大，随后进行骨架的推理，但是：
  * 定位错误将导致后续的一系列错误
  * 多径效应会引入假象
* 使用神经网络直接学习这个过程，但不是对定位到的方向进行信号放大，而是先将RF信号转换到一个融合多样相关信息的“抽象的域”，然后在这个域区分不同个体。

> 其实本质是Embedding，可能18年的时候这个词还没流行起来吧。这个逻辑是说得通的，因为Embedding的过程可以建模成过滤噪声+聚类，相比于直接的定位确实可能有一定降低false-alarm的效果。

### 网络的训练

训练数据由12个相机在不同角度进行拍摄，然后拍摄的3秒的video clip通过一个多视角的几何重建的优化问题来找到人体的关键点。当然**相机只用来产生训练数据的标签，并没有参与到网络的输入。**数据很多，由超过一百人的不同室内动作组成，并在新环境下测试算法的性能。

![image-20230510101436108](https://s2.loli.net/2023/05/10/Dwup2YSN1TZ9zlo.png)

小总结：定位的误差在三个方向上的中值误差在1.7cm，2.8cm以及2.3cm，关键点的误差在水平方向是6.5cm，纵向的误差是4.0cm。

### 问题建模

关键点的定位问题建模成分类问题，然后用CNN求解。空间中的所有体素将会被划分到不同的类别，最终的目的是确认每一个关键点对应体素的空间位置。特别的，对于关键点的定位，CNN输出不同类别的得分$s\in\{s_v\}_{v\in V}$，其中$v\in V$是空间中的所有体素，目标体素$v^\star$是其中包含关键点的那个体素。损失函数为Softmaxloss$L_{\mathrm{Softmax}}(s, v^\star)$。使用单个CNN去输出14个关键点的分数而非对每一个单独类别使用CNN去输出分数，以期望可以学习到遮挡场景时借助其他关键点对遮挡的关键点进行检测。

> 目标：找到每个关键点的位置->找到关键点对应的体素->对体素进行分类。

损失函数此时就是所有组件的加和：
$$
L_{pose}=\sum_kL_{\mathrm{Softmax}} (s^k,v^{k^*})
$$
其中的k是某一个关键点。推理输出结果取最高的分数对应的那个类别，$\arg \max s_v^k$即可。

### 多人的识别

类似目标识别任务，最简单的办法就是将原始的RF信号经由RPN输出可能的region，然后对每一个region使用CNN进行识别分类。作者从最后一部分中分离了CNN模型，并使RPN在中间层的输出（即特征图）上运行，因为该层被压缩过的信息的语义信息更为丰富。原始RF信号混乱，并受到多径效应的影响。因此，在要求RPN裁剪特定区域之前，使用一些卷积层来压缩信息并消除混乱；其次，当有多个人在场时，他们可能会彼此从RF设备中遮挡，从而导致被遮挡的人丢失反射。因此，进行4D时空卷积以组合跨时空的信息，以允许RPN检测到暂时被遮挡的人。

单人网络总共包含18个卷积层。我们将前12层划分为特征网络（FN），将其余6层划分为姿态估计网络（PEN）。在何处分割不是唯一的，但是通常FN应该具有足够的层来聚合后续RPN和PEN的空间和时间信息。

### RPN

> RPN使用在Faster-RCNN时直接生成检测框，效率比较高。RPN包含以下部分：
>
> * 生成锚框anchor boxes
> * 判断每个anchor box为foreground和background两部分，二分类
> * 边界框回归bounding box regression对anchor box进行微调，使得positive anchor和ground truth box更接近

RPN将FN输出的特征图作为输入，并输出一组矩形区域建议，每个区域建议都有一个分数来描述包含人的区域的概率。作者使用的是二维的bounding box，因为人体大致都位于同一个平面。

我们知道输出的特征图有H x W个结果，我们随机取一点，它跟原图肯定是有个一一映射关系的，由于原图和特征图大小不同，所以特征图上的一个点对应原图肯定是一个框，然而这个框很小，比如说8 x 8，这里8是指原图和特征图的比例，所以这个并不是我们想要的框，那我们不妨把框的左上角或者框的中心作为锚点（Anchor），然后想象出一堆框，具体多少，聪明的读者肯定已经猜到，K个，这也就是图中所说的K anchor boxes（由锚点产生的K个框）；换句话说，H x W个点，每个点对应原图有K个框，那么就有H x W x k个框默默的在原图上，那RPN的结果其实就是判断这些框是不是物体以及他们的偏移；那么K个框到底有多大，长宽比是多少？这里是预先设定好的，共有9种组合，所以k等于9，最后我们的结果是针对这9种组合的，所以有H x W x 9个结果，也就是18个分数和36个坐标。

RPN被实现为标准的CNN。滑动窗口可以用3x3的卷积完成实现，最后的两个输出分数和坐标位置也可以用1x1的卷积，所以全都用卷积来实现了。训练RPN的一种方法是尝试所有可能的区域，并且如果每个区域都紧密围绕场景中的真实人物，则将其分类为正确的。由于存在许多可能的区域，因此此方法非常慢。相反，我们使用滑动窗口对潜在区域进行采样。对于每个采样窗口，我们使用分类器检查它与真实人的交点是否合理。如果是这样，RPN会尝试调整该窗口的边界以使其更合适。为每个训练窗口分配一个二进制标签，以指示其是否包含一个人。要设置标签，我们使用一个简单的交并集（IoU）指标，其定义为：
$$
\mathrm{IoU=\frac{Area\, of\,Intersection}{Area\,of\,Union}}
$$
因此，（1）将与任何地面真实区域（即，对应于真实人物的区域）重叠大于0.7IoU的窗口设置为正；（2）将与所有地面真相重叠的窗口小于0.3设置为负；对于不满足以上两个条件的其他窗口，我们在训练阶段将其忽略。

参考：[(83条消息) RPN 解析_懒人元的博客-CSDN博客](https://blog.csdn.net/lanran2/article/details/54376126)

### 3D标签的生成

用的树莓派，12个camera

![image-20230512094908182](https://s2.loli.net/2023/05/12/3ZLRiQaUMxlqhJD.png)

多相机系统：系统有12个相机节点，每个节点都由Raspberry Pi，电池和相机模块板组成。我们的节点小巧，轻巧，并且通过将其附着在墙上而易于部署。摄像机节点通过NTP同步，并使用标准的多摄像机校准技术相对于一个全局坐标系进行校准。部署后，摄像头会从不同的角度对人成像。

2D骨骼生成：接下来，我们的系统使用摄像机捕获的图像生成2D骨骼。为此，我们利用了称为OpenPose 的计算机视觉系统，该系统给出的图像返回其中的人的2D骨骼，如图7所示。理想情况下，我们希望相同的骨骼出现在图像中。全部12台摄像机。但是，由于遮挡和放置12个摄像头以覆盖不同区域的事实，每个摄像头可能会看到不同的人或同一人的不同关键点。

2D骨骼关联：接下来，我们确定同一个人的2D骨骼，并将它们关联在一起，如图7所示。要判断一对2D骨骼是否来自同一个人，我们查看它们之间的几何关系。具体而言，给定2D关键点（例如头部），原始3D关键点必须位于3D空间中与摄像机视图垂直的线上，并在2D关键点处与之相交。直觉是，当一对2D骨骼都来自同一个人时，对应于特定关键点潜在位置的那两条线将在3D空间中相交。另一方面，如果这对2D骨骼来自两个不同的人，则3D空间中的这两条线将具有较大的距离且没有相交。基于这种直觉，我们将与各个关键点相对应的3D线之间的平均距离用作两个2D骨架的距离度量，并使用层次聚类对来自同一个人的2D骨架进行聚类。

三角剖分3D骨架：一旦我们从同一个人获得了多个2D骨架，就可以对它们的关键点进行三角剖分，以生成相应的3D骨架。我们以特定关键点p的2D投影$p_i$作为空间中的点来估计特定关键点p的3D位置，其投影使与所有此类2D投影的距离之和最小化，即：
$$
\mathcal p=\arg\min_\mathcal{p}\sum_{i\in I}||\mathcal{C_i p-p^i}||_2
$$
$C_i$是将矩阵坐标转换为摄像机$i$视角中的图像坐标的校准矩阵。

### 实际测试

FMCW扫频范围为5.4至7.2 GHz。传输功率小于一毫瓦。使用标准FMCW和天线阵列方程对RF信号进行处理，以每秒生成30个垂直和水平热图，然后将其与摄像头帧同步。摄像机使用网络时间协议（NTP）进行同步。使用本地NTP服务器时，时钟同步错误平均小于1ms。在实验期间，我们为所有RF热图和视频帧加上时间戳，并根据它们的时间戳同步不同的流。同步后，所有RF和视频流的FPS为30。
