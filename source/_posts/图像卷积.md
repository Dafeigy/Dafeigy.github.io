---
title: 从一维卷积到二维卷积：浅析图像卷积
mathjax: true
tags:
- 计算机视觉
categories:
- 学习记录
---



## 什么是卷积

首先先给出一维情况的卷积的定义。
$$
f(x)*g(x)=\int_{-\infty}^{\infty}f(x)g(\tau-x)d\tau
$$
这个式子很难从直观层面进行理解，因为它包含了乘积、翻转、积分三个部分的内容。如果你是信通学子，那么你肯定会知道两个函数的卷积结果可以看作是他们对应的傅里叶变换在频域的乘积的反变换，也就是说把一个函数作为输入信号，另一个函数作为描述系统的一个方程，当这个系统接收到输入信号后会做出响应，这个响应$y(t)$就是输入信号$x(t)$与系统函数$h(t)$卷积的结果：
$$
y(t)=x(t)*h(t)=\mathcal F^{-1}[X(jw)H(jw)]
$$
但很明显不是所有人都学过信号与系统，所以我想了个更~~好~~恶心的例子进行描述。<!--more-->

### 一个~~恶心~~的例子

试想这样一个场景：一个热门旅游景点的厕所。我们知道厕所需要连连不断地接待客人（？），因此厕所的囤粪速度可以用函数$F(x)$表示，如图1。

<img src="https://s2.loli.net/2022/08/03/7FHNVEs1uM5RCIB.png" alt="image-20220724172639005" style="zoom:50%;" />

<center><small>图1粪便囤积变化率曲线</small></center>

但是厕所连通的化粪池它的处理能力有限，它对排泄物的处理效率如图2所示，我们用函数$H(x)$来表示。

<img src="https://s2.loli.net/2022/08/03/BtJzpROGo3rTNYF.png" alt="image-20220724172618665" style="zoom:50%;" />

<center><small>图2 化粪池分解效率</small></center>

下面我们的问题就是需要讨论，**化粪池在某个时候还有多少粪便啦！**

### 怎么计算？

每一个粪便都是独立的个体，它们都会随着化粪池分解而减少。如果不考虑粪便的分解的话，那么粪便总量可以这样表示：
$$
n =\int_{-\infty}^{\infty}F(x)dx
$$
但是我们的粪便不是无线增加的，因此，需要考虑分解的情况。现在假设在$t$时刻时进行统计，经过时间间隔$\tau$后统计存在的粪便总量为：
$$
n' = F(t)·H(\tau)
$$
也就是说在$t+\tau$时刻，不考虑其他情况下的粪便输入，此时一共有$n'$这么多的粪便。这里则是对应了卷积操作中的乘积部分。我们换一种写法，令$T=t+\tau$，则在$T$时刻不考虑其他粪便的增加的情况下，粪便量可以记为:
$$
n'=F(t)·H(T-t)
$$
$H(t)$函数的变量出现了x轴反转的情况，我们离卷积的定义也越来越近啦！下一步就是积分的问题了。积分其实也很好理解，因为上面这些式子都是只考虑了单个时刻不考虑其他时刻的输入信息，因此如果要考虑其他时刻的输入的话，则是把其他时刻的粪便量进行一个叠加（离散域就是使用$\sum$求和，连续域就是使用$\int$积分）。因此$t$时刻的粪便总量可以表示为：
$$
Y(t)=\int_{-\infty}^{t}F(t)H(\tau-t)d\tau
$$
是不是这样就很好理解了？

### 图像的卷积是什么？

卷积的本质在刚刚已经说了，就是反转、乘积、求和。到了二维也是这样子的：\
$$
Y(x,y)=F(x,y)*H(x,y)=\sum_{n_1=-\infty}^{\infty}\sum_{n_2=-\infty}^{\infty}F(n_1,n_2)·H(x-n_1,y-n_2)
$$
好！下面我们给出图像卷积的一个步骤：

![](https://img-blog.csdnimg.cn/20200308112430934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hcnVoaW5h,size_16,color_FFFFFF,t_70)

其中这个`Convolution Kernel`叫做卷积核，`Source Pixel`就是我们的图像。在这张图里红色通道对应的地方就是卷积处理的第一个像素，计算的公式在右上角，输入是1，输出是8。随着卷积核往下一个像素移动，优惠计算出新的像素值，直到按设定的规则遍历完图像。

![](https://pic3.zhimg.com/v2-8a6695c2e086525ac5a61610348739b2_b.gif)

唯一需要指出的是，在这里体现了乘积和加和，那么反转是在哪里体现的呢？其实是图中卷积核已经是翻转了180°了，也就是说原本的卷积核它的“4"是在”-4“的位置的。所以这就是图像卷积的操作，是不是很好理解捏？

## 为什么是卷积

上结论：卷积的本质是提取图像中的特征。许多常用的图像处理方法都可以用卷积的方式表示，因此选择卷积。卷积的输出是`out_channel `个`feature_map`。其中的`feature_map`就是上面文章计算每一个求和的结果组成的一张新的“图像”，这个图像的尺寸大小是这样计算的：

* `input`:$(N,C_{in},H_{in},W_{in})$or$(C_{in},H_{in},W_{in})$
* `output`:$(N,C_{out},H_{out},W_{out})$or$(C_{out},H_{out},W_{out})$

$$
\begin{aligned}
&H_{out}=[\frac{H_{in}+2\times \mathrm {padding[0]-\mathrm{dialation}[0]\times (\mathrm{kernal\_size[0]-1})-1}}{\mathrm{stride[0]}}+1]\\
&W_{out}=[\frac{W_{in}+2\times \mathrm {padding[1]-\mathrm{dialation}[1]\times (\mathrm{kernal\_size[1]-1})-1}}{\mathrm{stride[1]}}+1]\\
\end{aligned}
$$

其中的$N$是`pytorch`的批量大小。

## Pytorch的图像卷积

![](https://img-blog.csdnimg.cn/20201121195053531.gif)

终于来到这里了。我们先看看pytorch卷积层继承的父类`_ConvNd`:

```python
class _ConvNd(Module):

    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: Tuple[int, ...],
                 stride: Tuple[int, ...],
                 padding: Tuple[int, ...],
                 dilation: Tuple[int, ...],
                 transposed: bool,
                 output_padding: Tuple[int, ...],
                 groups: int,
                 bias: bool,
                 padding_mode: str,
                 device=None,
                 dtype=None) -> None:
```



主要关注`in_channels`,`out_channels`,`kernel_size`,`stride`,`padding`这几个参数。

* in_channels

`in_channels`的输入是按`(N,C,H,W)`排列的，`N`是mini-batch大小，`C`是通道数channel，`H`和`W`分别是输入的高和宽，**这和`Tensorflow`的不太一样。**

* out_channels

在定义好了`in_channels`后，`out_channels`只需要关注输出通道数`K`即可。

* kernel_size

卷积核大小，如果设为一个整数的话那么卷积核就是一个正方形的，否则就按长宽的顺序输入一个`tuple`作为卷积核形状。

* stride

步长。也就是卷积核移动的距离，可以是一个整数也可以是一个`tuple`，类似`kernel_size`。

* padding

补丁，就是只要使用了大小大于1的卷积核去做卷积的话，输出图像的大小是比原本的图像大小要小的，因此为了保证维度的大小一致，会引入padding填充原图像的边缘位置，该值表示的是沿原图边缘进行该值大小的延拓。

## 那反卷积是干啥的？

从上面的文章我们知道了卷积是从高维的数据中不断地输出尺寸变小的特征图，是一个从大到小的变换过程，而反卷积就是从小变大的一个相反过程。但是需要注意：**反卷积只能恢复图像原有的大小，而不能完全恢复图像原本的信息。**它在`Pytorch`中的操作如下：

* input:$(N,C_{in},H_{in},W_{in})$or$(C_{in},H_{in},W_{in})$
* `output`:$(N,C_{out},H_{out},W_{out})$or$(C_{out},H_{out},W_{out})$

$$
\begin{aligned}
&H_{out}=(H_{in}-1)\times \mathrm{stride}[0]-2\times \mathrm{padding}[0]+\mathrm{dilation}[0]\times(\mathrm{kernel\_size}[0]-1)+\mathrm{output\_padding}[0]+1\\
&W_{out}=(H_{in}-1)\times \mathrm{stride}[1]-2\times \mathrm{padding}[1]+\mathrm{dilation}[1]\times(\mathrm{kernel\_size}[1]-1)+\mathrm{output\_padding}[1]+1
\end{aligned}
$$

在`Pytorch`中的使用示例如下：

```python
# With square kernels and equal stride
m = nn.ConvTranspose2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
input = torch.randn(20, 16, 50, 100)
output = m(input)
# exact output size can be also specified as an argument
input = torch.randn(1, 16, 12, 12)
downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
h = downsample(input)
h.size()
output = upsample(h, output_size=input.size())
output.size()
```

参数其实和卷积的一样，我就不解释了。

