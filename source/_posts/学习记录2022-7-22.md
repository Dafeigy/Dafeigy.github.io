---
title: 【学习记录】2022-7-22-2022-8-5
mathjax: true
tags:
- 计算机视觉
- Yolov5
- FaceNet
categories:
- 学习记录
---



要写一个yolov5+faceNet的人脸识别，没给定数据集和模型要求，所以我按~~最高性价比~~最摸鱼的原则进行构建。这个任务的由三部分组成的，首先是使用`Yolov5`完成人脸的识别并裁剪，随后用`FaceNet`进行特征的提取，最后使用提取的特征通过一个`classifier`完成人脸所属对象的识别。



测试用的demo使用了B站Up主[马哥巨离谱](https://space.bilibili.com/298054634)的作品：[穿山甲之【广东分甲】](https://www.bilibili.com/video/BV1z34y167KD?spm_id_from=333.999.0.0&vd_source=ab34db443b112b108b42c31ac575fd1f)，人物共有五个，人脸信息分布也很足；FaceNet作为局部的信息提取，我是使用MobileNet作为主干网络，并使用了一些预训练的权重进行训练。至于最后的分类器，我觉就只用了一个FC就完成了，效果还算不错。

<!--more-->

# Yolov5

![yolov5s_1](https://s2.loli.net/2022/07/25/AtufwTK238Z1sPR.png)



<center><small>Yolov5s结构，图源知乎@江大白</small></center>

Yolov5其实就是一个大锅炉，整个网络就分为Backbone、Neck、head和prediction四个部分。可以看到Yolov5用到的模块还挺多的，下面简单的分析一下每个模块。

## Backbone部分

Backbone就是用来提取主要的特征的。Backbone网络一般都很大，且有人训练好了一些权重，不用我们自己从头再训练，直接到官网下载模型权重就可以了。在训练时只需Finetune一下就可以了。

### CBL模块

`CBL`其实就是卷积层BN层再加一个激活函数LeakyReLU。卷积层和BN层没啥好说的了，现在成功的网络结构的基本模块都是这样搭配，值得一提的是这个LeakyReLU，区别于传统ReLU的，它增强了非线性能力。

### Res模块

就是改进过后的残差块。Kaiming的`ResNet`出来之后`ResBlock`的使用就广泛起来了，当今的网络发展趋势就是不断的加深、加参数，ResBlock的出现就是为了解决深度加大后的梯度弥散问题，跳跃连接可以将梯度信息不那么快的消失掉。

### Focus模块

这个其实我觉得挺关键的，作者也在[Issue](https://github.com/ultralytics/yolov5/discussions/3181)有给出设计的目的。结论就是：减少参数和计算量。

![image-20220725125709651](https://s2.loli.net/2022/07/25/kBKaHcQCPdO3RmV.png)

首先`Focus`模块是放在图像输入后的第一个位置，它使用了4个slice对图像进行分割，具体的操作是这样子的：



![slice操作](https://s2.loli.net/2022/07/25/9scCXZ3rOhavUBQ.png)

<center><small>Focus模块的Slice操作</small></center>

具体就是在一张图片中每隔一个像素拿到一个值，按照图中这样的Slice方法可以得到4张图片（原始图像是`4*4*3`，新图像是`2*2*12`）。这样做的目的我的理解是将图像的宽、高信息转换到了通道channel中，然后这时图像大小类似下采样虽然缩小了一倍，但它蕴含的信息量并未减少，只是信息转换到了通道中，为后续的处理预留了方便吧。为什么这样说呢，首先要明确网络的参数和计算量的概念。

**网络的参数量（parmas）决定了网络的大小**，由于一般使用`float32`进行保存，因此模型大小是参数量的4倍；**计算量（FLOPs）即浮点运算数**，可以用来衡量算法/模型的复杂度，这关系到算法速度，大模型的单位通常为G，小模型单位通常为M；通常只考虑乘加操作的数量，而且只考虑Conv和FC等参数层的计算量，忽略BN和PReLU等，一般情况下，Conv和FC层也会忽略仅纯加操作的计算量，如bias偏置加和shoutcut残差加等，目前技术有BN和CNN可以不加bias。

经过`Focus`模块后的图像类似经过了下采样的操作，但是和卷积实现的下采样不同，Slice不但减少了计算量也减少了参数。作者做了一个和Yolov3对比的实验：

![image-20220725130745790](https://s2.loli.net/2022/07/25/mhyW6EGT37RCYuA.png)

### CSP模块

和Yolov4不同，CSP的结构在Yolov5中有两种，分别运用在Backbone和Neck部分，而在Yolov4中只有Backbone使用了CSP结构。CSP结构就是**跨阶段局部网络（Cross Stage Paritial Network）**，用于缓解以往工作需要从网络架构角度进行大量推理计算的问题，CSPNet作者认为这个问题的**出现原因是网络优化中的重复梯度信息。**

简单总结一下CSP结构的有点：

* 增强CNN的学习能力，在轻量化和准确性这两方面两手抓。
* 降低计算瓶颈。
* 降低内存成本。

## Neck部分

Neck的作用就是更好的利用Backbone提取的特征进行更精细的划分。Yolov5最新的模型是使用了特征金字塔网络(FPN,Feature Pyramid Network)+路径聚合网络(PAN，Path Aggregation Network)结构。





## 使用方法

### 制作数据集和配置文件

首先使用`git clone`把Yolov5的代码给下载下来：

![image-20220727114737933](https://s2.loli.net/2022/08/03/B9C1MgU4N7XrHdW.png)

然后再这个目录下新建一个文件夹，命名随意，但不用用已有的`data`。我这里用的是`Facedata`，然后在`Facedata`下面再新建两个文件夹`Annotations`和`images`，分别用于存储用`labelimg`标注的数据(`.xml`)和图片(`.jpg`)。

```
- yolov5
	- Facedata
		- Annotations
			- 1.xml
			- 2.xml
			- ...
		- images
			- 1.jpg
			- 2.jpg
			- ...
```

然后在自己数据集的文件夹下新建一个用于分离训练集和测试集的脚本`split_train_test.py`:

```python
# coding:utf-8

import os
import random
import argparse

parser = argparse.ArgumentParser()
#xml文件的地址，根据自己的数据进行修改 xml一般存放在Annotations下
parser.add_argument('--xml_path', default='Annotations', type=str, help='input xml label path')
#数据集的划分，地址选择自己数据下的ImageSets/Main
parser.add_argument('--txt_path', default='ImageSets/Main', type=str, help='output txt label path')
opt = parser.parse_args()

trainval_percent = 1.0  # 训练集和验证集所占比例。 这里没有划分测试集
train_percent = 0.9     # 训练集所占比例，可自己进行调整
xmlfilepath = opt.xml_path
txtsavepath = opt.txt_path
total_xml = os.listdir(xmlfilepath)
if not os.path.exists(txtsavepath):
    os.makedirs(txtsavepath)

num = len(total_xml)
list_index = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list_index, tv)
train = random.sample(trainval, tr)

file_trainval = open(txtsavepath + '/trainval.txt', 'w')
file_test = open(txtsavepath + '/test.txt', 'w')
file_train = open(txtsavepath + '/train.txt', 'w')
file_val = open(txtsavepath + '/val.txt', 'w')

for i in list_index:
    name = total_xml[i][:-4] + '\n'
    if i in trainval:
        file_trainval.write(name)
        if i in train:
            file_train.write(name)
        else:
            file_val.write(name)
    else:
        file_test.write(name)

file_trainval.close()
file_train.close()
file_val.close()
file_test.close()
```

然后同样是在这个目录下新建`xml_to_yolo.py`文件，这个主要是用来转换yolov5训练的格式(`.txt`)，代码如下：

```python
import xml.etree.ElementTree as ET
import os
from os import getcwd

sets = ['train', 'val', 'test']
classes = ["face"]   # 改成自己的类别
abs_path = os.getcwd()

def convert(size, box):
    dw = 1. / (size[0])
    dh = 1. / (size[1])
    x = (box[0] + box[1]) / 2.0 - 1
    y = (box[2] + box[3]) / 2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x * dw
    w = w * dw
    y = y * dh
    h = h * dh
    return x, y, w, h

def convert_annotation(image_id):
    in_file = open('yolov5\\Facedata\\Annotations\\%s.xml' % (image_id), encoding='UTF-8')
    out_file = open('yolov5\\Facedata\\labels\\%s.txt' % (image_id), 'w')
    tree = ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        
        #difficult = obj.find('Difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult) == 1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
             float(xmlbox.find('ymax').text))
        b1, b2, b3, b4 = b
        # 标注越界修正
        if b2 > w:
            b2 = w
        if b4 > h:
            b4 = h
        b = (b1, b2, b3, b4)
        bb = convert((w, h), b)
        
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')
        
        

wd = getcwd()
for image_set in sets:
    if not os.path.exists('yolov5\\Facedata\\labels'):
        os.makedirs('yolov5\\Facedata\\labels')
    image_ids = open('yolov5\\Facedata\\ImageSets\\Main\\%s.txt' % (image_set)).read().strip().split()
    
    if not os.path.exists('yolov5\\Facedata\\dataSet_path'):
        os.makedirs('yolov5\\Facedata\\dataSet_path')
     
    list_file = open('yolov5\\Facedata\\dataSet_path\\%s.txt' % (image_set), 'w')

    for image_id in image_ids:
        list_file.write('yolov5\\Facedata\\images\\%s.jpg\n' % (image_id))
        convert_annotation(image_id)
    list_file.close()
```

然后到Yolov5文件夹下的data文件夹新建自己的数据库索引文件`Facedata.yaml`:

```yaml
train: F:/Programme/Demo/FaceDetection/yolov5/Facedata/dataSet_path/train.txt
val: F:/Programme/Demo/FaceDetection/yolov5/Facedata/dataSet_path/val.txt

# number of classes
nc: 1

# class names
names: ["face"]

```

最后在`model`文件夹中选择使用的模型，我选的是`Yolov5s.yaml`，然后修改一下nc对应的数量就可以了。

### 正式训练

正式训练是在这个`train.py`的文件里。打开看一下可以看到很多的参数：

![image-20220727170725337](https://s2.loli.net/2022/07/27/jIfv8VZgLep4OHt.png)

主要是修改以下几个东西：

* `--weights`：初始权重，要和模型对应，脚本会从官网下载预训练好的权重。
* `--data`：训练数据配置文件`xxx.yaml`，存放在Yolov5文件夹下的`data`文件夹中
* `--epochs`：训练轮数，一般来说小数据（不到一千张）100轮左右就有结果了。
* `--batch-size`：取决于设备内存大小，我设置为16。
* `device`：用于训练的设备，一般个人单显卡且有CUDA环境支持的话就设成0就好。
* `workers`：似乎是CPU核心控制的线程数量，电脑配置低一点的就设成0或者1就好。

### 使用训练后的模型

测试模型的话，在yolov5的文件夹下方有一个`detect.py`的文件，按照代码的说明修改程序就可以测试了。测试的结果是放在`runs/detect/expx/`的文件夹中。

### 如何部署自定义模型

如果图省事的话，可以用`pytorchhub`直接加载集成好的环境，具体的操作可以见[作者写的Documentation](https://github.com/ultralytics/yolov5/issues/36)：

```python
import torch

# Model
model = torch.hub.load('yolov5', 'custom', path='best', source='local') 
# Image
img = r'dataset\video\VideoCapture\9.jpg'
# Inference
results = model(img)
print(results.pandas().xyxy[0])
#         xmin        ymin        xmax        ymax  confidence  class  name
#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face
```

甚至可以使用`ndarrays`作为输入：

```python
import torch
import cv2
# Model
model = torch.hub.load('yolov5', 'custom', path='best', source='local') 
# Image
img = r'F:\Programme\Demo\FaceDetection\dataset\video\VideoCapture\9.jpg'
img = cv2.imread(img)
print(type(img))
# <class 'numpy.ndarray'>
# Inference
results = model(img)
print(results.pandas().xyxy[0])
#         xmin        ymin        xmax        ymax  confidence  class  name
#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face
```



但如果想进一步拓展Yolov5的输入端性能的话，`Yolov5`的作者把跟`Pytorchhub`加载的模型相关的内容放在了`models/common.py`里面，它的核心功能靠两个类`AutoShape`和`Detections`完成，其中`AutoShape`完成了模型的核心，`Detections`完成结果的展示以及渲染。用好这两个类足够完成大部分的输入任务了。

## 用Yolov5的预测结果获取人脸数据的训练集

观察到模型输出的信息：

```python
#         xmin        ymin        xmax        ymax  confidence  class  name
#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face
```

是使用`pandas.DataFrame`类表示的，且内取值为浮点数。因此需要对数据进行抽取排序以及取整处理：

```python
def crop_images(file_path, file_name, save_dir):
    # file_path: 文件路径
    # file_name: 文件名称
    # save_dir:  保存路径
    img = cv2.imread(file_path)
    results = model(img)
    df = results.pandas().xyxy[0]
    for index, row in df.iterrows():
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        crop = img[ymin : ymax, xmin : xmax]
        cv2.imwrite(r'{}\{}-{}.jpg'.format(save_dir, file_name[:-4], index), crop)   # index是图像识别中的第x个对象
```

随后使用模型裁剪人脸数据：

```python
# Easy way to use customed yolov5 model to predict
import torch
import cv2
import os

# Model
model = torch.hub.load('yolov5', 'custom', path='best', source='local') 
# Inference

def crop_images(file_path, file_name, save_dir):
    # file_path: 文件路径
    # file_name: 文件名称
    # save_dir:  保存路径
    img = cv2.imread(file_path)
    results = model(img)
    df = results.pandas().xyxy[0]
    for index, row in df.iterrows():
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        crop = img[ymin : ymax, xmin : xmax]
        cv2.imwrite(r'{}\{}-{}.jpg'.format(save_dir, file_name[:-4], index), crop)   # index是图像识别中的第x个对象
        

path = r'dataset\raw'
files= os.listdir(path) #得到文件夹下的所有文件名称
for each in files:
    crop_images(r'dataset\raw\{}'.format(each), each, save_dir=r'dataset\train')
    print('file {} has been crop.'.format(each))

```

得到后人工对人脸进行分类放入不同类别的文件夹中。

# FaceNet+

论文链接：[[1503.03832\] FaceNet: A Unified Embedding for Face Recognition and Clustering (arxiv.org)](https://arxiv.org/abs/1503.03832)。FaceNet是谷歌提出的，特点是CNN的端到端训练、Triplet loss 以及 特征欧几里得距离，模型大小仅有128Bytes，达到了当时的SOTA水平。论文使用的模型如下：

<img src="https://s2.loli.net/2022/08/02/bWgmUj4fKGweoMr.png" alt="image-20220802190434704" style="zoom: 80%;" />

## 网络部分

论文当时的Backbone使用的是`Inception-net`的结构，然后后续为了更加轻便的部署我又更换成了`MobileNet-v1`的结构。下面的内容主要针对的是`MobileNet-V1`。

### 深度可分离卷积

首先`MobileNet`也是谷歌提出来的，它的核心组件是一种叫做`“深度可分离卷积块”`的结构组成的，其结构如下：

![img](https://s2.loli.net/2022/08/02/WAr5wIh4ZbStVuF.jpg)

其具体的解析在[这篇文章](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)有。深度可分离卷积块由两部分组成，分别为`Depthwise Convolutional Filters`和`Pointwise Convolutional Filters`。深度可分离卷积块的目的是使用更少的参数来代替普通的卷积。下面给出普通卷积和可分离卷积的对比：

<img src="https://s2.loli.net/2022/08/02/ChxozZ3PRFp2nyO.png" alt="img" style="zoom: 33%;" />

<center><small>普通卷积，对于12*12*3->8*8*256的映射需要256个[5,5,3]的卷积核，<br>参数量为256 x 5 x 5 x 3 = 19200</small></center>

<img src="https://s2.loli.net/2022/08/02/LKJy9svTaERXUZS.png" alt="img" style="zoom: 33%;" />

<img src="https://s2.loli.net/2022/08/02/quI9WnQZ4kzRDKo.png" alt="img" style="zoom: 33%;" />

<center><small>可分离卷积，对于12*12*3->8*8*256的映射需要3个[5,5,1]的卷积核(Depthwise)和256个[1,1,3]的卷积核(Pointwise)<br>参数量为3 x 5 x 5 x 1 + 256 x 1 x 1 x 3 = 843</small></center>

可以看出来深度可分离卷积结构块可以减少模型的参数。

### 网络结构

网络结构由Backbone+Classifier两个部分组成。Backbone前文也提及到使用的是`MobileNet`，而`Classifier`则是一个多层感知机。师兄给了一篇参考的博客给我看，他的`Classifier`用的是`SVM`，我能理解这样做的目的，舍弃了端到端的直观过程，让神经网络的黑盒少了最后那部分的可解释性差的问题。但是这样就很麻烦，部署起来很费时间，所以我最终还是选择了用MLP作为分类器，还有一个原因我会放在后面的训练技巧部分那里说。

Backbone部分的代码：

```python
import torch.nn as nn


def conv_bn(inp, oup, stride = 1):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU6()
    )
    
def conv_dw(inp, oup, stride = 1):
    return nn.Sequential(
        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),
        nn.BatchNorm2d(inp),
        nn.ReLU6(),

        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU6(),
    )

class MobileNetV1(nn.Module):
    def __init__(self):
        super(MobileNetV1, self).__init__()
        self.stage1 = nn.Sequential(
            # 160,160,3 -> 80,80,32
            conv_bn(3, 32, 2), 
            # 80,80,32 -> 80,80,64
            conv_dw(32, 64, 1), 

            # 80,80,64 -> 40,40,128
            conv_dw(64, 128, 2),
            conv_dw(128, 128, 1),

            # 40,40,128 -> 20,20,256
            conv_dw(128, 256, 2),
            conv_dw(256, 256, 1),
        )
        self.stage2 = nn.Sequential(
            # 20,20,256 -> 10,10,512
            conv_dw(256, 512, 2),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
        )
        self.stage3 = nn.Sequential(
            # 10,10,512 -> 5,5,1024
            conv_dw(512, 1024, 2),
            conv_dw(1024, 1024, 1),
        )

        self.avg = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(1024, 1000)

    def forward(self, x):
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.avg(x)
        # x = self.model(x)
        x = x.view(-1, 1024)
        x = self.fc(x)
        return x
```

FaceNet的代码：

```python
class Facenet(nn.Module):
    def __init__(self, backbone="mobilenet", dropout_keep_prob=0.5, embedding_size=128, num_classes=None, mode="train"):
        super(Facenet, self).__init__()
        if backbone == "mobilenet":
            self.backbone = mobilenet()
            flat_shape = 1024
        else:
            raise ValueError('Unsupported backbone - `{}`, Use mobilenet.'.format(backbone))
        self.avg = nn.AdaptiveAvgPool2d((1,1))
        self.Dropout = nn.Dropout(1 - dropout_keep_prob)
        self.Bottleneck = nn.Linear(flat_shape, embedding_size,bias=False)
        self.last_bn = nn.BatchNorm1d(embedding_size, eps=0.001, momentum=0.1, affine=True)
        if mode == "train":
            self.classifier = nn.Linear(embedding_size, num_classes)

    def forward(self, x):
        x = self.backbone(x)
        x = self.avg(x)
        x = x.view(x.size(0), -1)
        x = self.Dropout(x)
        x = self.Bottleneck(x)
        x = self.last_bn(x)
        x = F.normalize(x, p=2, dim=1)
        return x

    def forward_feature(self, x):
        x = self.backbone(x)
        x = self.avg(x)
        x = x.view(x.size(0), -1)
        x = self.Dropout(x)
        x = self.Bottleneck(x)
        before_normalize = self.last_bn(x)
        x = F.normalize(before_normalize, p=2, dim=1)
        return before_normalize, x

    def forward_classifier(self, x):
        x = self.classifier(x)
        return x
```

## Loss部分

FaceNet使用了`Triplet Loss`。这个Loss的目的是：对于一张人脸图片`Anchor`输入$x_i^a$，通过神经网络$f(x)$的映射到$d$维特征空间$\Bbb R^d$的结果$f(x_i^a)$和所有的正样本`Positive`的结果(和Anchor输入的类别相同的所有照片的$d$维特征空间结果$f(x_i^p)$)的欧式距离最短，而与所有的负样本`Negative`（和Anchor输入的类别不同的所有照片的$d$维特征空间结果$f(x_i^n)$）的欧氏距离最长，如图所示：

<img src="https://s2.loli.net/2022/08/02/OYi8dzawNkR26Vf.png" alt="image-20220802195743710" style="zoom:80%;" />

用公式表示一下这个**约束条件**就是：
$$
||f(x_i^a)-f(x_i^p)||_2^2+\alpha<||f(x_i^a)-f(x_i^n)||_2^2,\\
\forall (f(x_i^a),f(x_i^p),f(x_i^n))\in\tau
$$
其中，$\alpha$是规定的正负样本的边界距离，$\tau$代表的是三元组的每一组基组成的$N$维空间。对于这种情况，网络的优化目标就是最小化这个式子：
$$
\sum_i^N{[||f(x_i^a)-f(x_i^p)||_2^2+\alpha-||f(x_i^a)-f(x_i^n)||_2^2]}_+
$$

## 训练技巧

### 三元组Triplet的选择

上文提到，把所有可能的三元组构建出来是非常耗时的，而且大部分的式子都能满足**约束条件**，而神经网络是一个基于差别进行反馈修正的系统，因此较小的Loss其实对训练的结果修改并没有很大的意义。举个例子，备战高考，在做往年题的时候发现自己的立体几何做得很强，但是导数题做得一塌糊涂漏洞百出。这个时候就需要专攻薄弱环节导数题进行训练，因为薄弱环节的提分可以最大程度地拉升和满分之间的差距（减小Loss，提高Accuracy）。基于这种想法，我们直接选择那些违背了约束条件的三元组进行训练即可。（但在实际的训练中，我将直接选择更改为了随机选择，因为考虑到模型的泛化性能我对违背约束条件的三元组的选择是以0.2的概率进行抽样送往网络训练）

### 分类器的辅助训练

FaceNet输出的其实是要给长度为128的特征向量，下一步的分类器根据特征向量分类才输出最终的分类结果。仅仅使用Triplet Loss会难以让网络收敛，因此在训练过程中可以再加上一个MLP作为训练器，使用Cross-Entropy作为辅助Triplet-Loss收敛。这样做的话其实就已经自带了一个`Classifier`了，不再需要额外的训练。这就是为什么之前提到不使用SVM等等一类的分类算法进行最后的分类器。

### 数据增强

* **随机水平翻转。**给定正态分布$X\sim \delta(0,1)$，对于随机变量$x$，当其取值小于0.5时，将输入的图像进行左右的翻转。在`opencv`中仅需要`cv2.flip(image,1)`即可完成操作。

* **灰度边框+等比缩放。**首先获取输入图片的原始尺寸，计算其长宽比$r_1$；然后根据长宽中的最大值和MobileNet的输入尺寸`160*160`进行比较，得到缩放倍率$d_{wh}=\frac{160}{\max(w,h)}$，随后根据长宽比和缩放倍率对原始图像进行缩放，使得长宽中的一边达到160；随后，针对不足160的部分使用灰色`RGB:(128,128,128)`矩形从两端均匀padding。代码如下：

  ```python
  def letterbox(im, new_shape=(160, 160), color=(128, 128, 128), auto=False, scaleFill=False, scaleup=True, stride=32):
      
      # Resize and pad image while meeting stride-multiple constraints
      shape = im.shape[:2]  # current shape [height, width]
      if isinstance(new_shape, int):
          new_shape = (new_shape, new_shape)
  
      # Scale ratio (new / old)
      r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
      if not scaleup:  # only scale down, do not scale up (for better val mAP)
          r = min(r, 1.0)
  
      # Compute padding
      ratio = r, r  # width, height ratios
      new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
      dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
      if auto:  # minimum rectangle
          dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
      elif scaleFill:  # stretch
          dw, dh = 0.0, 0.0
          new_unpad = (new_shape[1], new_shape[0])
          ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios
  
      dw /= 2  # divide padding into 2 sides
      dh /= 2
  
      if shape[::-1] != new_unpad:  # resize
          im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)
      top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
      left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
      im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
      return im# , ratio, (dw, dh)
  ```

  

# 整合收尾

终于到这里了！到了我第一喜欢的UI设计环节！！！

<img src="https://s2.loli.net/2022/08/05/ldiUFYzgjBrqyQN.png" alt="image-20220805181658926"  />

<center><small>其实也没啥设计，就是选了几个颜色而已</small></center>

## 性能测试

Yolo推理速度直接搬运官网的0.01s左右，`FaceNet`推理时间为0.003秒左右。这里其实有个坑，就是一开始测试的时候发现时间差异太大了，然后查了一下发现好像是在部署到设备上进行推理的时候，需要使用一些数据进行`Warmup`的操作：

```python
device = torch.device('cuda')
dummy_input = torch.rand(1,3,160,160).to(device)
print('Warming Up')
    for _ in range(20):
        _ = model(dummy_input)
```

然后进行性能的测试。首先是推理时间：

![image-20220807092856305](https://s2.loli.net/2022/08/07/JsfkZiAtdFRXL3P.png)

基本上可以做到100FPS的水平，也就是说，基本上实时检测是没问题的了。那么准确度如何呢？我使用了两个验证集进行测试，其中一个验证集是训练集数据来源的视频的下半部分（这部分没有任何数据用于训练、测试），另一个则是完全独立的新视频。先说几个存在的问题吧，首先无论是在哪个验证集上面进行效果测试，脸部画面占据画面20-50%的情况下，推理准确度为100%。然而在人脸屏占比为10~15%左右的时候，会出现分类抖动的情况，就是说分类结果会很不稳定，出现的原因可能是`FaceNet`要求的输入图像大小为`160*160`导致的，加上Resize函数和LetterBox的操作会导致一些特征的变换有所丢失。解决方法则是对对所有的小尺寸图片划分为子类，并固定Backbone参数重新训练FaceNet以及分类器，最后导出结果的时候再把子类合并到大类里面。

## 探索实验

然后这里我还额外做了个实验，研究模型的卷积层的关注点，对重点区域生成热力图，观察到底是什么特征区分了人脸：



<img src="https://s2.loli.net/2022/08/05/qnpZ3vXkxS9uG78.png" alt="image-20220805183241174" style="zoom: 50%;" />

<center><small>Grad-CAM钩在Backbone最后的卷积层上生成的热力图</small></center>

可以看出网络主要是重点关注鼻子部分的信息进行分类决策的。就这个简单的分类任务而言基本上准确率处于一个可以接受的范围，但是就更广泛一点的人脸识别任务来说，这个网络的设计其实还是有缺陷的，因为我们希望的是网络**更加关注五官的信息而不是针对数据集的差异关注某一局部的信息。**所以如果要构建更通用的人脸识别系统的话，可以参考目标识别设定中的预设描框对人脸的五官局部进行裁剪或者滑动窗口+注意力机制综合运用，对局部信息进行特征向量的提取与embedding，甚至还可以降低特征向量的维度（FaceNet的输出特征向量维度是128），在我调参的过程中我也研究过`Embedding Size`的大小设置，针对这次使用的数据集而言，128的长度其实对5个类别来说其实有些冗余了，可以进行一定的缩减；其次，五官信息的特征可以以组合的形式输出一个新的特征向量，最后只需要对这个新的特征向量做分类工作即可。我更改了网络的主干部分，使用并修改了Swim Transformer的结构，但碍于设备性能未能进行实验探索，日后有机会的话可能会从这方面进行进一步的探索工作。
