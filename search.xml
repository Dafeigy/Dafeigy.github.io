<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Street Fighter vs. DQN Fighter</title>
    <url>/2022/07/29/DQN%E8%87%AA%E6%88%91%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>I would like to apply an improved DQN to the Game Street Fighter V, an action game published by CAPCOM in 2016. This project was inspired by LinYi and I try to use DQN without HPE(Human Pose Estimation) due to the limitation of my hardware.</p>
<p><img src="https://s2.loli.net/2022/05/27/DSw9adIUvW3q86K.png" style="zoom:67%;" /></p>
<p>I try to train a model with less time training and more robust to any other Acion fighting game. This may be cool and difficult since I need to prepare my exam while handling other projects in this very year. </p>
<span id="more"></span>
<h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><h3 id="Before-we-start"><a href="#Before-we-start" class="headerlink" title="Before we start"></a>Before we start</h3><p>We need access to some in-game factors that help game agent better comprehend the situation(Though these factors are simply shown in the screen,I would like to use in-game memory to get which is faster and relieve GPU’s burden if use CV method to convert these images’ info).</p>
<p><img src="https://i.loli.net/2021/09/08/84P5qBKMUWn6JLb.png" alt="image-20210908230408019" style="zoom: 67%;" /></p>
<p>However,CAPCOM use anti-spam to block the access to read memory while the game runs,which may crash the game if you turn some basic memory search tools like <code>CheatEngine</code>.I use this <a href="https://bbs.pediy.com/thread-195729.htm">Tool</a> to avoid detection and it actually works!(By the way ,the unzip code is <code>qq295991</code>).</p>
<p>In addition,you should open the software before you launch the game,and change the default settings to this:</p>
<p><img src="https://i.loli.net/2021/09/08/sGI6nJjkTADOZbd.png" alt="image-20210908230121851" style="zoom: 80%;" /></p>
<p>Then you can search the factors you need and locate their base&amp;offset address.</p>
<p><img src="https://i.loli.net/2021/09/08/13NdY7S6WtCLczw.png" alt="image-20210908230121851" style="zoom: 80%;" /></p>
<p>The Following Problem is that when I use <code>openProcess</code> in <code>win32api</code> to get value of the factors it returns the error like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;G:/SFV/Tool/factors.py&quot;</span>, line <span class="number">180</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    hp = Gamefactors()</span><br><span class="line">  File <span class="string">&quot;G:/SFV/Tool/factors.py&quot;</span>, line <span class="number">55</span>, <span class="keyword">in</span> __init__</span><br><span class="line">    hProcess = win32api.OpenProcess(</span><br><span class="line">pywintypes.error: (<span class="number">5</span>, <span class="string">&#x27;OpenProcess&#x27;</span>, <span class="string">&#x27;Access Denied.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>It really takes me a long time to find the solution:( I look up the MSDN and find this:</p>
<blockquote>
<p>If the caller has enabled the SeDebugPrivilege privilege, the requested access is granted regardless of the contents of the security descriptor.</p>
</blockquote>
<p>Which means I need the <code>SeDebugPrivilege</code> huh…So I write a function to get more privileges:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_extra_privs</span>():  <span class="comment"># get more access to the game</span></span><br><span class="line">    <span class="comment"># Try to give ourselves some extra privs (only works if we&#x27;re admin):</span></span><br><span class="line">    <span class="comment"># SeBackupPrivilege   - so we can read anything</span></span><br><span class="line">    <span class="comment"># SeDebugPrivilege    - so we can find out about other processes (otherwise OpenProcess will fail for some)</span></span><br><span class="line">    <span class="comment"># SeSecurityPrivilege - ??? what does this do?</span></span><br><span class="line"></span><br><span class="line">    th = win32security.OpenProcessToken(win32api.GetCurrentProcess(),</span><br><span class="line">                                        win32con.TOKEN_ADJUST_PRIVILEGES | win32con.TOKEN_QUERY)</span><br><span class="line">    privs = win32security.GetTokenInformation(th, win32security.TokenPrivileges)</span><br><span class="line">    newprivs = []</span><br><span class="line">    <span class="keyword">for</span> privtuple <span class="keyword">in</span> privs:</span><br><span class="line">        <span class="keyword">if</span> privtuple[<span class="number">0</span>] == win32security.LookupPrivilegeValue(<span class="literal">None</span>, <span class="string">&quot;SeBackupPrivilege&quot;</span>) <span class="keyword">or</span> privtuple[</span><br><span class="line">            <span class="number">0</span>] == win32security.LookupPrivilegeValue(<span class="literal">None</span>, <span class="string">&quot;SeDebugPrivilege&quot;</span>) <span class="keyword">or</span> privtuple[</span><br><span class="line">            <span class="number">0</span>] == win32security.LookupPrivilegeValue(<span class="literal">None</span>, <span class="string">&quot;SeSecurityPrivilege&quot;</span>):</span><br><span class="line">            <span class="comment"># print(&quot;Added privilege &quot; + str(privtuple[0]))</span></span><br><span class="line">            newprivs.append((privtuple[<span class="number">0</span>], <span class="number">2</span>))  <span class="comment"># SE_PRIVILEGE_ENABLED</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newprivs.append((privtuple[<span class="number">0</span>], privtuple[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust privs</span></span><br><span class="line">    privs = <span class="built_in">tuple</span>(newprivs)</span><br><span class="line">    win32security.AdjustTokenPrivileges(th, <span class="literal">False</span>, privs)</span><br></pre></td></tr></table></figure>
<p> Now we need to access these factors: </p>
<ul>
<li>Player&amp;Rival’s HP</li>
<li>Player&amp;Rival’s EX</li>
<li>Player&amp;Rival’s VT</li>
<li>Player&amp;Rival’s location</li>
</ul>
<p>Yet we need to notice that player’s &amp; rival’s location are not in float type while HPs are in normal byte type, so we need a function to transfer Byte type to Float type:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">byte2Float</span>(<span class="params">s</span>):  <span class="comment"># Convert byte to float</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        i = <span class="built_in">int</span>(s, <span class="number">10</span>)  <span class="comment"># convert from Dec to a Python int</span></span><br><span class="line">        cp = pointer(c_int(i))  <span class="comment"># make this into a c integer</span></span><br><span class="line">        fp = cast(cp, POINTER(c_float))  <span class="comment"># cast the int pointer to a float pointer</span></span><br><span class="line">        a = fp.contents.value</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        a = <span class="number">0</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<h3 id="Why-not-define-a-class"><a href="#Why-not-define-a-class" class="headerlink" title="Why not define a class?"></a>Why not define a class?</h3><p>We can add function to this class to reduce the time spent on accessing these factors:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Hp_getter</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        get_extra_privs()</span><br><span class="line">        hd = win32gui.FindWindow(<span class="literal">None</span>, <span class="string">&quot;StreetFighterV&quot;</span>)</span><br><span class="line">        pid = win32process.GetWindowThreadProcessId(hd)[<span class="number">1</span>]</span><br><span class="line">        self.process_handle = win32api.OpenProcess(<span class="number">0x1F0FFF</span>, <span class="literal">False</span>, pid)</span><br><span class="line">        self.kernal32 = ctypes.windll.LoadLibrary(<span class="string">r&quot;C:\\Windows\\System32\\kernel32.dll&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.hx = <span class="number">0</span></span><br><span class="line">        <span class="comment"># get dll address</span></span><br><span class="line">        hProcess = Kernel32.OpenProcess(</span><br><span class="line">            PROCESS_QUERY_INFORMATION | PROCESS_VM_READ,</span><br><span class="line">            <span class="literal">False</span>, pid)</span><br><span class="line">        hModule = EnumProcessModulesEx(hProcess)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hModule:</span><br><span class="line">            temp = win32process.GetModuleFileNameEx(self.process_handle, i.value)</span><br><span class="line">            <span class="keyword">if</span> temp[-<span class="number">18</span>:] == <span class="string">&quot;StreetFighterV.exe&quot;</span>:</span><br><span class="line">                self.StreetFighter=i.value</span><br><span class="line">                <span class="built_in">print</span>(self.StreetFighter)</span><br></pre></td></tr></table></figure>
<p>And by the way no thanks:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">EnumProcessModulesEx</span>(<span class="params">hProcess</span>):</span><br><span class="line">    buf_count = <span class="number">256</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        LIST_MODULES_ALL = <span class="number">0x03</span></span><br><span class="line">        buf = (ctypes.wintypes.HMODULE * buf_count)()</span><br><span class="line">        buf_size = ctypes.sizeof(buf)</span><br><span class="line">        needed = ctypes.wintypes.DWORD()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Psapi.EnumProcessModulesEx(hProcess, ctypes.byref(buf), buf_size, ctypes.byref(needed),</span><br><span class="line">                                          LIST_MODULES_ALL):</span><br><span class="line">            <span class="keyword">raise</span> OSError(<span class="string">&#x27;EnumProcessModulesEx failed&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> buf_size &lt; needed.value:</span><br><span class="line">            buf_count = needed.value // (buf_size // buf_count)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        count = needed.value // (buf_size // buf_count)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">map</span>(ctypes.wintypes.HMODULE, buf[:count])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>强化学习</category>
        <category>游戏智能</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>街头霸王</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title>区块链技术的基本原理以及简单应用</title>
    <url>/2022/07/29/BlockChain/</url>
    <content><![CDATA[<p>2008年11月1日，一位自称中本聪(Satoshi Nakamoto)的人发表了《比特币:一种点对点的电子现金系统》一文，阐述了基于P2P网络技术、加密技术、时间戳技术、区块链技术等的电子现金系统的构架理念，这标志着比特币的诞生。两个月后理论步入实践，2009年1月3日第一个序号为0的创世区块诞生。几天后2009年1月9日出现序号为1的区块，并与序号为0的创世区块相连接形成了链，标志着区块链的诞生。下面将阐述区块链的基本原理与功能并用代码实现区块链仿真以及一个简单的应用，同时讨论它的安全性与可靠性。</p>
<p><img src="https://s2.loli.net/2022/05/27/kQPiEU64zp9b3fR.png" style="zoom:33%;" /></p>
<span id="more"></span>
<h2 id="区块链技术的原理"><a href="#区块链技术的原理" class="headerlink" title="区块链技术的原理"></a>区块链技术的原理</h2><h3 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h3><p>区块链技术源于中本聪提出的一个问题：<strong>能否搭建一种基于密码学原理而非信用的电子现金系统，使得交易双方不需要通过第三方或其他金融机构？</strong></p>
<p>这个问题在现实中就是：能否实现一种没有第三方监管，但又能保证每个人都不会造假的交易方案？在日常生活中常见的网络支付手段是先从端发起通过支付软件查询对应的人民银行（第三方）账户检验账户余额是否充足，随后在这个账户中记录这笔钱的去向。当每笔交易都被双方正确记录下时，我们就认为这是一个可靠可行的支付系统。这个系统可行的前提是，我们相信从软件到人民银行这个过程中，所有数据都是不会被修改的。如果没有这些可信任的第三方，我们又如何完成一笔交易？</p>
<h3 id="一个解决方案"><a href="#一个解决方案" class="headerlink" title="一个解决方案"></a>一个解决方案</h3><p>中本聪的提出的解决方法是构建一个去中心化的网络，即大家一起记账。系统中每产生一笔交易就要广播一次，目的是让系统中的每个人知道发生了一笔交易。交易会被记录在一个”账本“里，也就是“区块”，当区块连接起来后就成为了区块链。区块的结构组成如下表格所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>大小</th>
<th>字段</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>4 Bytes</td>
<td>区块大小</td>
<td>用字节表示区块大小</td>
</tr>
<tr>
<td>80 Bytes</td>
<td>区块头</td>
<td>组成区块头的大小</td>
</tr>
<tr>
<td>1-9（可变整数）</td>
<td>交易计数器</td>
<td>交易数量</td>
</tr>
<tr>
<td>/</td>
<td>交易</td>
<td>记录区块链的交易消息</td>
</tr>
</tbody>
</table>
</div>
<p>在这个系统中每一个”人”都拿着一个相同且实时更新的”账本“。显然账本的可靠性奠定了支付的安全性，因此接下来有两个新的问题：</p>
<ul>
<li>谁去记账？</li>
<li>如何保证账本没有造假？</li>
</ul>
<h3 id="共识机制-amp-工作量证明的引入"><a href="#共识机制-amp-工作量证明的引入" class="headerlink" title="共识机制&amp;工作量证明的引入"></a>共识机制&amp;工作量证明的引入</h3><p>假设系统中每一个人都参与记账，那么会出现大家记录的交易信息以及记录的时间都不一样，这导致了账本无法被认可的问题，所以我们需要推选出一个可以承担”记账人“角色的人完成这一工作，也就是”<strong>共识机制（Consensus Mechanism）</strong>“。中本聪提出的解决方法是做题：”每一个区块前面加入一个随机数，然后进行散列函数运算（如SHA256），但只接受前面有若干个（Nonce）0的结果。通过改变随机数使得前面的0的个数等于Nonce，那么就完成一次记账的工作”。</p>
<p>实际中的区块的哈希值是由区块头数据进行二次SHA256生成的，即：</p>
<p><code>Hash=SHA256(SHA256(blockHeader))</code></p>
<p>比特币中的区块头包含了如下信息：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>大小</th>
<th>字段</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>4 Bytes</td>
<td>版本</td>
<td>版本号，追踪区块链版本</td>
</tr>
<tr>
<td>32 Bytes</td>
<td>父区块哈希值</td>
<td>引用区块链中父区块的哈希值</td>
</tr>
<tr>
<td>32 Bytes</td>
<td>Merkel根</td>
<td>该区块中交易的Merkel树根的哈希值</td>
</tr>
<tr>
<td>4 Bytes</td>
<td>时间戳</td>
<td>该区块产生的近似时间(精确到秒的Unix时间戳)</td>
</tr>
<tr>
<td>4 Bytes</td>
<td>难度目标</td>
<td>该区块工作量证明算法的难度目标</td>
</tr>
<tr>
<td>4 Bytes</td>
<td>Nonce</td>
<td>用于工作量证明算法的计数器</td>
</tr>
</tbody>
</table>
</div>
<p>由于SHA256单向散列函数的特性，只能通过暴力运算才能试出满足条件的随机数值，暴力运算所消耗的资源（计算机的算力、电力等）被称为”<strong>工作量证明（Proof-of-Work）</strong>“。当某一个人先试验出来这个随机数后，便会向系统其他人广播自己运算的结果。其他人可以马上拿到他算出的结果进行检验，并保存下来，这样就保证了系统中每一个人都能拿到实时最新的账本。为了鼓励记账，系统会对第一个完成运算的实施奖励（比特币）以鼓动记账的积极性。SHA256的实现原理如下图所示：</p>
<p><img src="https://i.loli.net/2021/06/25/XvJDGnlSqyLkhF8.png" alt="image-20210625165355912" style="zoom:67%;" /></p>
<h3 id="解决造假的问题。"><a href="#解决造假的问题。" class="headerlink" title="解决造假的问题。"></a>解决造假的问题。</h3><p>规定在区块头里加入上一个区块的哈希值，也称为哈希指针，通过哈希指针可以让每一个区块首尾相连直到第一个区块（创世区块）。如果改变任意一个区块里的数据，对应的哈希值就会改变，当哈希指针无法与上一个区块的哈希值对应时，该区块往后的区块就失效了，因此要重新计算往后区块的哈希值，直到修改完这个区块后的所有区块。</p>
<h3 id="原理框图"><a href="#原理框图" class="headerlink" title="原理框图"></a>原理框图</h3><p>下图展示的是当前的比特币区块链的组成结构以及运行原理图：</p>
<p><img src="https://i.loli.net/2021/06/25/xJGw7n2bdMYuKZ9.png" style="zoom: 80%;" /></p>
<h2 id="区块链的基本模块实现"><a href="#区块链的基本模块实现" class="headerlink" title="区块链的基本模块实现"></a>区块链的基本模块实现</h2><p>上文讨论了区块链的基本原理，下文即用代码实现区块链各个模块以及功能。尽量对照中本聪原始论文进行模拟、设计编程。</p>
<h3 id="交易"><a href="#交易" class="headerlink" title="交易"></a>交易</h3><p>我们定义，一枚电子货币（an electronic coin）是这样的一串数字签名：每一位所有者通过对前一次交易和下一位拥有者的公钥(Public key) 签署一个随机散列的数字签名，并将这个签名附加在这枚电子货币的末尾，电子货币就发送给了下一位所有者。而收款人通过对签名进行检验，就能够验证该链条的所有者。在程序实现中，我们采用<code>json</code>数据进行交易信息的存储。之所以应用json，是因为它的通用性和高效解析性，方便程序运算。</p>
<h3 id="时间戳服务器-区块的类"><a href="#时间戳服务器-区块的类" class="headerlink" title="时间戳服务器/区块的类"></a>时间戳服务器/区块的类</h3><p>原始论文中指出，”时间戳服务器通过对以区块(block)形式存在的一组数据实施随机散列而加上时间戳，并将该随机散列进行广播，就像在新闻或世界性新闻组网络（Usenet）的发帖一样……每个时间戳应当将前一个时间戳纳入其随机散列值中，每一个随后的时间戳都对之前的一个时间戳进行增强(reinforcing)，这样就形成了一个链条（Chain）。“因此我们可以将区块类的属性简化成如下模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, index, timestamp, transactions, previous_proof, previous_hash</span>):</span><br><span class="line">        <span class="comment"># 一个区块包含的基本元素（索引，时间戳，工作量证明，交易信息，前一个区块的哈希值）</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.previous_hash = previous_hash</span><br><span class="line">        self.proof = self.proof_of_work(previous_proof)</span><br><span class="line">        self.<span class="built_in">hash</span> = self.hash_block()</span><br></pre></td></tr></table></figure>
<h3 id="工作量证明函数"><a href="#工作量证明函数" class="headerlink" title="工作量证明函数"></a>工作量证明函数</h3><p>工作量证明本质上是一个计数器，在中本聪的论文中，他提到的方法是寻找满足条件的随机数。在这里无意检验算SHA256的运算难度，仅仅观察系统运行性能，因此更改了工作量证明函数，即不断寻找能同时被9和上一个符合的随机数整除的数。代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">proof_of_work</span>(<span class="params">self, previous_proof</span>):</span><br><span class="line">    <span class="comment"># index为0表明为创世区块，工作量证明设置为0</span></span><br><span class="line">    <span class="keyword">if</span> self.index == <span class="number">0</span>:</span><br><span class="line">        incrementor = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> incrementor</span><br><span class="line">    <span class="comment"># 非创世区块工作量证明计算</span></span><br><span class="line">    <span class="keyword">if</span> previous_proof == <span class="number">0</span>:  <span class="comment"># 避免除0出错的问题</span></span><br><span class="line">        previous_proof = <span class="number">1</span></span><br><span class="line">    incrementor = previous_proof + <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> (incrementor % <span class="number">9</span> == <span class="number">0</span> <span class="keyword">and</span> incrementor % previous_proof == <span class="number">0</span>):</span><br><span class="line">        incrementor += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> incrementor</span><br></pre></td></tr></table></figure>
<p>如果执意要使用中本聪的方案，下面也有实现方法，但不建议在模拟使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">proof_of_work</span>(<span class="params">self, previous_proof</span>):</span><br><span class="line">    <span class="comment"># index为0表明为创世区块，工作量证明设置为0</span></span><br><span class="line">    <span class="keyword">if</span> self.index == <span class="number">0</span>:</span><br><span class="line">        incrementor = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> incrementor</span><br><span class="line">    <span class="comment"># 非创世区块工作量证明计算</span></span><br><span class="line">    <span class="keyword">if</span> previous_proof == <span class="number">0</span>:  <span class="comment"># 避免除0出错的问题</span></span><br><span class="line">        previous_proof = <span class="number">1</span></span><br><span class="line">    incrementor = previous_proof + <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> (sha256(<span class="built_in">str</span>(incrementor)+transactions) [<span class="number">0</span>:diff]!= diff * <span class="string">&#x27;0&#x27;</span>:<span class="comment">#diff是设定的难度，现行比特币一般取72</span></span><br><span class="line">        incrementor += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> incrementor</span><br></pre></td></tr></table></figure>
<h3 id="哈希值的生成"><a href="#哈希值的生成" class="headerlink" title="哈希值的生成"></a>哈希值的生成</h3><p>这里为了简化了一下运算难度，仅进行一次SHA256运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hash_block</span>(<span class="params">self</span>):</span><br><span class="line">    sha = hashlib.sha256()</span><br><span class="line">    sha.update((<span class="built_in">str</span>(self.index) + <span class="built_in">str</span>(self.timestamp) + <span class="built_in">str</span>(self.transactions) + <span class="built_in">str</span>(self.previous_hash) + <span class="built_in">str</span>(</span><br><span class="line">        self.proof)).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> sha.hexdigest()</span><br></pre></td></tr></table></figure>
<h3 id="创世区块的生成"><a href="#创世区块的生成" class="headerlink" title="创世区块的生成"></a>创世区块的生成</h3><p>创世区块是最特殊的一个区块。我们仅需规定它的时间戳属性即可，其他的函数已经完成了它的其他属性计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_genesis_block</span>():</span><br><span class="line">    <span class="comment"># index为0，时间为生成该区块的时间，工作量证明为0，交易信息为空，哈希值为0</span></span><br><span class="line">    <span class="keyword">return</span> Block(<span class="number">0</span>, date.datetime.now(), [], <span class="number">0</span>, <span class="string">&quot;0&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="区块链中添加区块"><a href="#区块链中添加区块" class="headerlink" title="区块链中添加区块"></a>区块链中添加区块</h3><p>这一步的函数只完成了添加的操作，本质上是列表的添加。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_block_to_blockchain</span>(<span class="params">blockchain, new_transactions</span>):</span><br><span class="line">    <span class="comment"># 新区块各个要素的值</span></span><br><span class="line">    last_block = blockchain[-<span class="number">1</span>]</span><br><span class="line">    this_index = last_block.index + <span class="number">1</span></span><br><span class="line">    this_timestamp = date.datetime.now()</span><br><span class="line">    current_transactions = new_transactions</span><br><span class="line">    previous_proof = last_block.proof</span><br><span class="line">    previous_hash = last_block.<span class="built_in">hash</span></span><br><span class="line">    <span class="comment"># 生成新块并添加到链</span></span><br><span class="line">    blockchain.append(Block(this_index, this_timestamp, current_transactions, previous_proof, previous_hash))</span><br><span class="line">    <span class="keyword">return</span> blockchain</span><br></pre></td></tr></table></figure>
<p>在实际应用中，可以增加条件判断来完成哈希指针的检验工作。至此为止，区块链所有的基本模块已经实现。更具有现实意义的、更复杂的如Merkel树、Merkel树根等属性在这里不再加以展开。</p>
<h3 id="网络节点"><a href="#网络节点" class="headerlink" title="网络节点"></a>网络节点</h3><p>区块链网络工作的过程是：</p>
<ol>
<li>新的交易向全网进行广播； </li>
<li>每一个节点（Node）都将收到的交易信息纳入一个区块中； </li>
<li>每个节点都尝试在自己的区块中找到一个具有足够难度的工作量证明； </li>
<li>当一个节点找到了一个工作量证明，它就向全网进行广播； </li>
<li>当且仅当包含在该区块中的所有交易<strong>都是有效的且之前未存在过的</strong>，其他节点才认同该区块的有效性； </li>
<li>其他节点表示它们接受该区块，表示接受的方法则是：在跟随该区块的末尾，制造新的区块以延长该链条，而将被接受区块的随机哈希值视为先于新区快的随机哈希值。</li>
</ol>
<p>在区块链的运作过程所，每一个节点在争取“记账”的权力——有共识的区块，因此节点们都在努力算，首先算出来的可被接受。</p>
<p>在我们的程序实现中，我们可以采用Flask或Django等Web框架进行仿真模拟各个节点。</p>
<h2 id="区块链的简单应用"><a href="#区块链的简单应用" class="headerlink" title="区块链的简单应用"></a>区块链的简单应用</h2><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>由于区块链具有的难以篡改的特性，因此用于记录一些重要的但数量多的数据有较大优势。在区块链中，传递的数据是<code>json</code>文件，在网页端中传递数据通常都会使用到它。引入一个场景：在当今自媒体呈爆炸式增长的时代，每个人获取的信息也越来越多，其中有很多信息存在时效性以及疑义性，二者不可兼得。目前的情况是由于碎片化的时间，很少有人会去追溯新闻事件的发展，因此造成了信息传播的误差。在这里我们试图利用区块链技术完成一个可溯的新闻客户端，持续追踪新闻信息的发展与变动。</p>
<h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><p>利用Flask框架搭建一个Web测试平台，观察区块链的实现。通过添加节点、以及模块功能分离，可以模拟节点运作。为了节约前端开发时间，故不进行前端页面的渲染，使用Postman作为简单的交互工具进行区块链系统中的操作模拟。在这之前，我们要先获取合适的数据。通过对<a href="houxv.app">后续</a>网站爬虫，我们可以得到<code>json.json</code>中的数据：</p>
<p><img src="https://i.loli.net/2021/06/25/FfMlk6P9gUbcIWa.png" alt="image-20210625192801969"></p>
<p>这些数据可以作为一个交易，流通在网络中。为更好模拟，我们采取功能分离的方法，设置三个节点完成对应的操作：</p>
<ul>
<li>挖矿节点<code>Mine</code></li>
<li>交易放入节点<code>Transaction</code></li>
<li>广播节点<code>chain</code></li>
</ul>
<p>挖矿节点用于调用工作量证明函数，交易放入节点只是提供POST方法接口，放入新的交易，广播节点用于观察数据变化。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 实例化节点（创建一个节点）</span></span><br><span class="line">node = Flask(__name__)</span><br><span class="line"><span class="comment"># 为该节点生成一个全局唯一的地址（为节点创建一个随机的名字）</span></span><br><span class="line">node_identifier = <span class="built_in">str</span>(uuid4()).replace(<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看运行信息</span></span><br><span class="line"><span class="meta">@node.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_message</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello! Runing on http://127.0.0.1:5000/&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 挖矿</span></span><br><span class="line"><span class="meta">@node.route(<span class="params"><span class="string">&#x27;/mine&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mine</span>():</span><br><span class="line">    <span class="comment"># 获取交易信息的一个拷贝（非引用），避免“广播”错误，否则仅发送交易信息，就直接导致原有区块交易信息的改变</span></span><br><span class="line">    <span class="comment"># 只有在挖矿的时候，才会将new_transaction信息加入到新块中</span></span><br><span class="line">    new_transactions = this_node_transactions.copy()</span><br><span class="line">    add_block_to_blockchain(blockchain, new_transactions)</span><br><span class="line">    mined_block = blockchain[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment">#    # 添加到此处</span></span><br><span class="line">    <span class="keyword">del</span> (this_node_transactions[:])</span><br><span class="line">    <span class="keyword">return</span> jsonify([&#123;</span><br><span class="line">        <span class="string">&quot;index&quot;</span>: mined_block.index,</span><br><span class="line">        <span class="string">&quot;timestamp&quot;</span>: <span class="built_in">str</span>(mined_block.timestamp),</span><br><span class="line">        <span class="string">&quot;transactions&quot;</span>: mined_block.transactions,</span><br><span class="line">        <span class="string">&quot;proof&quot;</span>: mined_block.proof,</span><br><span class="line">        <span class="string">&quot;hash&quot;</span>: mined_block.<span class="built_in">hash</span></span><br><span class="line">    &#125;])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看区块链</span></span><br><span class="line"><span class="meta">@node.route(<span class="params"><span class="string">&#x27;/chain&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">full_chain</span>():</span><br><span class="line">    response = []</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blockchain:</span><br><span class="line">        block_index = <span class="built_in">str</span>(block.index)</span><br><span class="line">        block_timestamp = <span class="built_in">str</span>(block.timestamp)</span><br><span class="line">        block_transactions = <span class="built_in">str</span>(block.transactions)</span><br><span class="line">        block_proof = <span class="built_in">str</span>(block.proof)</span><br><span class="line">        block_hash = block.<span class="built_in">hash</span></span><br><span class="line">        block = &#123;</span><br><span class="line">            <span class="string">&quot;index&quot;</span>: block_index,</span><br><span class="line">            <span class="string">&quot;timestamp&quot;</span>: block_timestamp,</span><br><span class="line">            <span class="string">&quot;transactions&quot;</span>: block_transactions,</span><br><span class="line">            <span class="string">&quot;proof&quot;</span>: block_proof,</span><br><span class="line">            <span class="string">&quot;hash&quot;</span>: block_hash</span><br><span class="line">        &#125;</span><br><span class="line">        response.append(block)</span><br><span class="line">    response.append(&#123;<span class="string">&#x27;len&#x27;</span>: <span class="built_in">len</span>(blockchain)&#125;)</span><br><span class="line">    <span class="keyword">return</span> jsonify(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送交易</span></span><br><span class="line"><span class="meta">@node.route(<span class="params"><span class="string">&#x27;/transaction&#x27;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transaction</span>():</span><br><span class="line">    <span class="comment">#    if request.method == &#x27;POST&#x27;:</span></span><br><span class="line">    <span class="comment"># 检查所需的字段是否在POST数据中</span></span><br><span class="line">    values = request.get_json()</span><br><span class="line">    required = [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;type&#x27;</span>, <span class="string">&#x27;object&#x27;</span>,<span class="string">&#x27;publish_at&#x27;</span>,<span class="string">&#x27;object_key&#x27;</span>,<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;summary&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(k <span class="keyword">in</span> values <span class="keyword">for</span> k <span class="keyword">in</span> required):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Missing values&#x27;</span>, <span class="number">400</span></span><br><span class="line">    <span class="comment"># Then we add the transaction to our list</span></span><br><span class="line">    this_node_transactions.append(values)</span><br><span class="line">    <span class="comment"># Because the transaction was successfully</span></span><br><span class="line">    <span class="comment"># submitted, we log it to our console</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;New transaction&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;type&#x27;</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;object: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;object&#x27;</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;publish_at: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;publish_at&#x27;</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;object_key: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;object_key&#x27;</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;title: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;title&#x27;</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;summary: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(values[<span class="string">&#x27;summary&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Transaction submission successful\n&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h3><p>由于没有编写前端页面，故采用POSTMAN观察数据交互过程：</p>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>在打开<a href="http://127.0.0.1:5000/chain">http://127.0.0.1:5000/chain</a> 并点击Send后，得到如下结果：</p>
<p><img src="https://i.loli.net/2021/06/25/NQ2Oeiv1In3tBjA.png" alt="image-20210625193627211"></p>
<p>可以清楚看到创世区块的建立时间，以及交易部分的空白。</p>
<h4 id="放入交易"><a href="#放入交易" class="headerlink" title="放入交易"></a>放入交易</h4><p>当在<code>transaction</code>节点中放入一笔交易后（使用Post方法在transaction上传一段json数据），结果如图所示：</p>
<p><img src="https://i.loli.net/2021/06/25/qyJjXxwnQvFDcPW.png" alt="image-20210625193920139"></p>
<p>返回结果显示”交易“成功。观察刚刚的广播节点：</p>
<p><img src="https://i.loli.net/2021/06/25/xIuKo9gpzfdXkiQ.png" alt="image-20210625194104079" style="zoom:80%;" /></p>
<p>没有观察到”交易”的产生，因为还没有人“挖矿”。</p>
<h4 id="挖矿并广播"><a href="#挖矿并广播" class="headerlink" title="挖矿并广播"></a>挖矿并广播</h4><p>在<code>mine</code>节点中使用GET方法，完成广播工作：</p>
<p><img src="https://i.loli.net/2021/06/25/fbs5SeYVW9yXGpI.png" alt="image-20210625194642407" style="zoom:80%;" /></p>
<p>回到广播节点页，观察此时的数据：</p>
<p><img src="https://i.loli.net/2021/06/25/JwNlAGfiaI8OKcv.png" alt="image-20210625194808133" style="zoom:67%;" /></p>
<p>可以观察到此时的<code>Transaction</code>已经不是空数组了，同时还记录了挖矿完成的时间。放入一笔交易对同一节点重复多次挖矿操作后再操作，观察时间消耗：</p>
<p><img src="https://i.loli.net/2021/06/25/8dtEQsLnqFzBKvC.png" alt="image-20210625195249084"></p>
<p>在后台中的记录：</p>
<p><img src="https://i.loli.net/2021/06/25/HfFb6KNSLBPIC71.png" alt="image-20210625195816783" style="zoom: 67%;" /></p>
<p>通过简单的Timestamp相减计算，可以观察到计算的时间在成倍的增加，而工作量证明的数据也正如我们所设的那般指数增加，满足如下数学关系：</p>
<script type="math/tex; mode=display">
proof=9\times2^{n-1}</script><h2 id="区块链安全性的讨论"><a href="#区块链安全性的讨论" class="headerlink" title="区块链安全性的讨论"></a>区块链安全性的讨论</h2><p>从上面的仿真应用中我们可以看出，运算的难度其实是随着所设的工作量证明函数有关的。因此设计合理的工作量证明函数在区块链技术中非常重要。设想如下场景：</p>
<p><em>“一个攻击者试图比诚实节点产生链条更快地制造替代性区块链。即便它达到了这一目的，但是整个系统也并非就此完全受制于攻击者的独断意志了，比方说凭空创造价值，或者掠夺本不属于攻击者的货币。这是因为节点将不会接受无效的交易，而诚实的节点永远不会接受一个包含了无效信息的区块。一个攻击者能做的，最多是更改他自己的交易信息，并试图拿回他刚刚付给别人的钱。”</em></p>
<p>要篡改区块链，攻击节点就要比诚信节点更多地挖出区块，让自己的虚假链生产速度大于诚信节点的生产速度。我们可以简化一下这个数学问题：</p>
<blockquote>
<p>假设攻击者制造出下一区块的成功率$p$，诚信节点制造下一个区块的成功率为$q$，$q_z$为消除了$z$个差距的成功率，则有</p>
<script type="math/tex; mode=display">
q_z=\begin {cases}
{1,q\geq p\\
(\frac{p}{q} )^z,q<p}
\end {cases}</script><p>假如攻击者算力不如诚实节点算力，那么它攻破这条链的成功几率就随着时间呈指数下降越来越小。现实中我们的交易需要一个确认时间，在这种情况下，运气好的攻击者在等待的时间攻破了这条长链的话，可以将准备好的假链接上在区块链中，从而攻破区块链，因此要设定一个合适的等待时间。</p>
<p>假设诚信节点的区块生产速度是恒定的一个定值$t$,而且它并不知道攻击者的运算进展，这样潜在攻击者的运算进展就成为一个泊松分布，分布期望为：</p>
<script type="math/tex; mode=display">
E（\lambda）=z ·\frac{q}{p}</script><p>当此情形，为了计算攻击者追赶上的概率，将攻击者取得进展区块数量的泊松分布的概率密度，乘以在该数量下攻击者依然能够追赶上的概率。</p>
<script type="math/tex; mode=display">
P(z)={\sum_{k!}^{\lambda^ke^{-\lambda}}·
\begin {cases}
(\frac{q}{p})^{z-k},k\leq z\\
1,k>z
\end {cases}}\\
=1-\sum_{k=0}^{z}\frac{\lambda^k e^{- \lambda}}{k!}[1-(\frac{q}{p})^{z-k}]</script></blockquote>
<p>编程对该函数求和，并计算取不同q、p计算z值。</p>
<center><img src="https://i.loli.net/2021/06/25/XJNlPAZEFV9Ujy3.png" alt="image-20210625212342045" style="zoom: 67%;" /></center>
<center><img src="https://i.loli.net/2021/06/25/Rn7jpoF9dZXJikD.png" alt="image-20210625212409923" style="zoom: 67%;" /></center>
<center><img src="https://i.loli.net/2021/06/25/M7XT16cxKHgq9wk.png" alt="image-20210625212746175" style="zoom: 67%;" /></center>



<p>纵向比较可以观察到，不论等待时间取多大，在攻击成功概率为0.5时，都能成功篡改区块链，因此破坏区块链的条件是拥有超过全网50%的算力进行运算；</p>
<p>横向对比可以观察到，增加等待时间后，同等攻击成功概率的情况下，篡改区块链的成功率越来越低，接近为零，因此增加等待时间是一个有效的保护措施，但这个保护时间不宜太长，否则不利于交易池中放入新的交易。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过python、Postman实现基本的区块链仿真以及简单应用，让我收获良多，其中区块链的去中心化思想、区块化、工作量证明机制非常吸引人，相信能在未来拥有更多的应用场景。本次编程考虑到了时间紧迫以及主力机性能，故没有完全复现中本聪所提出的其他机制如硬盘空间回收、简化支付确认、价值组合与分割以及隐私的实现，但在日后会完成进一步的应用实践与区块链的能力探索。配套代码见我的<a href="https://gitee.com/dafeigy/block-chain">Gitee仓库</a>。</p>
]]></content>
      <categories>
        <category>通信技术</category>
        <category>加密技术</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>通信</tag>
        <tag>数字签名</tag>
      </tags>
  </entry>
  <entry>
    <title>Polar码编码及SC译码Matlab仿真</title>
    <url>/2022/07/29/G1831/</url>
    <content><![CDATA[<p>2016年11月18日，在美国内华达州里诺召开的3GPP RAN1 #87次会议，确定Polar Code作为5G eMBB（增强移动宽带）场景下控制信道编码方案。极化码具有确定性的构造方法，并且是已知的唯一一种能够被严格证明“达到”信道容量的信道编码方法，因此我们将其作为研究对象，研究其编码/译码的实现。</p>
<p><img src="https://s2.loli.net/2022/05/27/t4Gi5PnegJYhRkv.png" alt=""></p>
<span id="more"></span>
<h1 id="研究讨论"><a href="#研究讨论" class="headerlink" title="研究讨论"></a>研究讨论</h1><h3 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h3><p>在针对时延扩展较长的信道时，频域选择性严重衰落，会出现残留冗余造成带宽浪费，通过采用以信源状态转移关系为基础构建信源信道联合译码网格图，综合信源转移概率和信道转移概率计算统一的序列后验概率的方式，可以实现了信源译码和信道译码的一体化联合优化，进而在不降低通信速率的情况下提高传输可靠性，基本实现无误码通信。不仅如此，在对6G$^{[1]}$的展望中，消除冗余的Polar码仍具有较LDPC码更优的BER性能，对于高阶调制的Polar码和多核Polar码设计也处于研究阶段，但纠错性能有一定的局限性，因此，具有较高纠错性能的多核Polar码的设计仍在发展中，也许成功的研究会应用于6G的信道编码技术。不过本次我们的仿真旨在理解Polar Code的基础原理，因此采用最基础的原始理论进行编写。</p>
<h3 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a>研究意义</h3><p>作为一种新兴的码种，极化码在推出之时就获得学术界的广泛关注。它是第一个能达到香农极限的编码方式，因此在推动通信系统的性能发展中有不可限量的应用前途。作为通信系的学生，尝试研读、复现论文成果一来能锻炼科研的理解能力、实践能力，同时也能从中汲取到通信这一学科的前沿知识，能获得较高的实用价值。极化码的编码、译码是极化码的核心部分，也是小组成员拟研究的方向。</p>
<h3 id="研究计划"><a href="#研究计划" class="headerlink" title="研究计划"></a>研究计划</h3><ul>
<li>研读极化码的原始论文《Channel polarization: A method for constructing capacity-achieving codes for symmetric binary-input memoryless channels》，掌握信道极化的思想以及编码/译码思路；</li>
<li>在Matlab中搭建仿真平台，完成极化码编码、译码实现；</li>
<li>对比其他编码方式以及译码方式，对极化码实用性进行评估，并针对仿真结果，对编码译码方式进行考量并做出评价。</li>
</ul>
<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><h3 id="编码部分"><a href="#编码部分" class="headerlink" title="编码部分"></a>编码部分</h3><h4 id="编码原理"><a href="#编码原理" class="headerlink" title="编码原理"></a>编码原理</h4><p>Polar Code是通过引入信道极化概念而构建的。</p>
<p>信道极化分为两个阶段，分别是信道联合和信道分裂。通过信道的联合与分裂，各个子信道的对称容量将呈现两级分化的趋势：随着码长（即联合信道数）N的增加，一部分子信道的容量趋于1，而其余子信道的容量趋于0。Polar Code正是利用这一信道极化的现象，在容量趋于1的K个子信道上传输消息比特，在其余子信道上传输冻结比特（即收发双方已知的固定比特，通常设置为全零）。由此构成的编码即为Polar Code，码率为$K/N$。</p>
<p>可以看出，编码问题的关键在于生成矩阵和信息位、冻结位的判决选取。最终的编码输出结果，可以简化为矩阵运算：</p>
<script type="math/tex; mode=display">
u^N=c_{i}^N·G_N</script><p>其中，$u^N$代表编码结果，$c_{i}^N$代表了输入数据，$G_N$为生成矩阵，$N$代表输入码长，且该值规定为2的正整数幂。</p>
<h4 id="生成矩阵的产生"><a href="#生成矩阵的产生" class="headerlink" title="生成矩阵的产生"></a>生成矩阵的产生</h4><p>首先先引入<strong>克罗内克积($\otimes$)</strong>运算：</p>
<p> 如果<em>A</em>是一个<em>m</em>×<em>n</em>的矩阵，而<em>B</em>是一个<em>p</em>×<em>q</em>的矩阵，则它们的克罗内克积则是一个<em>mp</em>×<em>nq</em>的分块矩阵，记为：</p>
<script type="math/tex; mode=display">
\mathrm{A} \otimes \mathrm{B}=\left[\begin{array}{ccc}
a_{11} B & \cdots & a_{1 n} B \\
\vdots & \ddots & \vdots \\
a_{1 n} B & \cdots & a_{n n} B
\end{array}\right]</script><p>以下举A，B为二阶矩阵时运算克罗内克积为例：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}\right] \otimes\left[\begin{array}{ll}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{array}\right]=\left[\begin{array}{llll}
a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
a_{21} b_{11} & a_{21} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
a_{21} b_{21} & a_{21} b_{22} & a_{22} b_{21} & a_{22} b_{22}
\end{array}\right]</script><p>下图为文献$^{[1]}$中的编码示意图：</p>
<p><img src="https://i.loli.net/2021/09/03/sohiuSG6P1JtvKw.png" alt="image-20210903221730402" style="zoom: 80%;" /></p>
<p>该过程可以简化为如下流程：</p>
<p>首先输入的码字经过了向量元素的重排操作，即通过图中的$R_N$矩阵相乘运算，然后将翻转后的码字经过信道进行联合、分裂。</p>
<p>同时，也可以将整个生成矩阵的产生过程可以简化为如下运算：</p>
<script type="math/tex; mode=display">
G_{N}=B_{N} F^{\otimes n}</script><p>下面对该表达式进行说明：</p>
<ul>
<li><p>$B_N$为元素经过扩展重排后的结果，定义为：</p>
<script type="math/tex; mode=display">
B_{N}=R_{N}\left(I_{2} \otimes B_{N / 2}\right)</script><p>其中：</p>
<ul>
<li><p>$I_2$为二阶单位矩阵。从定义式可以看出，$B_N$的生成为一递归过程，给定递归初值为$B_2=I_2$。</p>
</li>
<li><p>$R_N$为置换矩阵，即将输入序列完成奇序元素和偶序元素的分离。用符号表示为：</p>
<script type="math/tex; mode=display">
\left(u_{1}, u_{2}, u_{3}, u_{4}, \cdots, u_{N}\right) \times R_{N}=u_{1}, u_{3}, u_{5}, \cdots, u_{N-1}, u_{2}, u_{4}, u_{6}, \cdots, u_{N}</script></li>
</ul>
<p>经过上述操作后，$B_N$完成了比特反序重排。即将每个原序列的十进制序号$i \in\{1,2,3,…,N\}$先减一得到$i-1$，再将$i-1$按二进制表示，随后将该二进制的高低位进行倒叙排列，将得到的一串二进制重新转化为十进制数$j-1$，然后将第$j$位对应的数变成原来的第$i$位数。以元素$u_7$举例如下：</p>
<script type="math/tex; mode=display">
u_{7} \stackrel{7-1}{\longrightarrow} u_{6} \stackrel{\text { 十进制转二进制 }}{\longrightarrow} 110 \stackrel{\text { reverse }}{\longrightarrow} 011 \stackrel{\text { 二进制转十进制 }}{\longrightarrow} 3 \stackrel{3 * 1}{\rightarrow} 4 \rightarrow u_7^*=u_{4}</script><p>其他元素可依次依照此流程重新排序，这里不再赘述。</p>
</li>
<li><p>$F^{\otimes n}$为F的n次克罗内克积,即对矩阵$F$进行n次克罗内克积运算。文献$^{[1]}$中给出的$F$矩阵定义为：</p>
</li>
</ul>
<script type="math/tex; mode=display">
F=\left[\begin{array}{ll}
1 & 0 \\
1 & 1
\end{array}\right]</script><h4 id="信息位与冻结位的选取"><a href="#信息位与冻结位的选取" class="headerlink" title="信息位与冻结位的选取"></a>信息位与冻结位的选取</h4><p>信道极化过程中，有一部分信道的信道容量$I(W)$可以到达1，另一部分则趋近于0。引入文献$^{[1]}$的巴氏参数$Z(W)$来衡量信道容量的趋向性：</p>
<script type="math/tex; mode=display">
Z\left(W_{N}^{(i)}\right)=\sum_{y_{1}^{N} \in Y^{N}} \sum_{u_{1}^{i-1} \in X^{i-1}} \sqrt{W_{N}^{(i)}\left(y_{1}^{N}, u_{1}^{i-1} \mid 0\right) W_{N}^{(i)}\left(y_{1}^{N}, u_{1}^{i-1} \mid 1\right)}</script><p>式中的$W_N^{i}(y,u|x)$表示第i个信道的条件转移概率。</p>
<p>对于一个给定信道，巴氏参数越大说明该信道越不可靠。因此我们只需计算出联合、分裂后信道的巴氏参数，并对它们进行排序，然后根据码率选择巴氏参数较小的信道作为信息位，剩余信道作为冻结位。</p>
<h4 id="编码输出"><a href="#编码输出" class="headerlink" title="编码输出"></a>编码输出</h4><p>得到巴氏参数的序列后，将该序列从小到大进行排列，前一半作为信息位，后一半为冻结位。即将原始的输入$u$重新排序为$c_i$。最后将排序后的信息$c_i$与生成矩阵相乘：</p>
<script type="math/tex; mode=display">
u^N=c_{i}^N·G_N</script><h3 id="信道部分"><a href="#信道部分" class="headerlink" title="信道部分"></a>信道部分</h3><p>我们在常见的高斯加性白噪声信道中进行模拟。将编码结果经过BPSK调制，即将$\{0,1\}$符号映射为$\{1,-1\}$，随后用不同信噪比的高斯加性白噪声与调制结果叠加。</p>
<h3 id="译码部分"><a href="#译码部分" class="headerlink" title="译码部分"></a>译码部分</h3><h4 id="译码原理"><a href="#译码原理" class="headerlink" title="译码原理"></a>译码原理</h4><p>由之前提到的编码可以看出极化码的构造就是一个信道选择的过程，而极化信道的选择实际上就是按照最优SC译码性能为标准的。因此对极化码而言，最合适的译码算法应当是基于SC译码的，只有这类译码算法才能充分利用极化码的结构，并且同时保证在码长足够长时容量可达。</p>
<p>SC译码全称为successive cancellation decoder即连续消除译码，采用蝶形算法，通过递归的方式来进行串行解码。而根据蝶形算法的特性，从右向左进行计算，因此在解码时我们首先要确定最右端所判决得到的近似值。</p>
<h4 id="近似判决"><a href="#近似判决" class="headerlink" title="近似判决"></a>近似判决</h4><p>先对信息位和冻结位进行判决：</p>
<script type="math/tex; mode=display">
\hat{\mathrm{u}}_{1}^{(\mathrm{i})}\left\{\begin{array}{c}
\mathrm{h}_{\mathrm{i}}\left(\mathrm{y}_{\mathrm{N}}^{(\mathrm{i})}, \hat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right), \text { if } \mathrm{i} \in \mathrm{A} \\
\mathrm{u}_{\mathrm{i}}, \text { if } \mathrm{i} \in \mathrm{A}^{\mathrm{c}}
\end{array}\right.</script><p>其中$\mathrm{i} \in \mathrm{A}^{\mathrm{c}}$表明该比特为冻结比特，$\mathrm{i} \in \mathrm{A}$表明该比特为承载信息的比特。</p>
<p>随后是对信息位进行判决：</p>
<script type="math/tex; mode=display">
\widehat{\mathrm{h}}_{1}^{(\mathrm{i})}\left\{\begin{array}{l}
0, \text { if } \mathrm{L}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \hat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right) \geq 0 \\
1, \text { if if } \mathrm{L}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \hat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right)<0
\end{array}\right.</script><p>其中$\mathrm{L}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \widehat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right)$为对数似然比（Log-Likelihood Ratio）：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \widehat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right) \triangleq \ln \left(\frac{\mathrm{W}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \widehat{\mathrm{u}}_{1}^{(\mathrm{i}-1)} \mid 0\right)}{\mathrm{W}_{\mathrm{N}}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{i})}, \hat{\mathrm{u}}_{1}^{(\mathrm{i}-1)} \mid 1\right)}\right)</script><p>该式说明LLR的值为在接收端得到$\left(\mathrm{y}_{1}^{(\mathrm{i})}, \widehat{\mathrm{u}}_{1}^{(\mathrm{i}-1)}\right)$发送端原本的发送”0”和”1”的比值取对数，当该对数值大于等于0时，说明原本发送”0”的概率要大，因此我们可以将其判决为接收端收到的应该为”0”，反之则为”1”。</p>
<h4 id="递归求解"><a href="#递归求解" class="headerlink" title="递归求解"></a>递归求解</h4><p>给定递归求解的公式：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm{L}_{\mathrm{N}}^{(2 \mathrm{i}-1)}\left(\mathrm{y}_{1}^{(\mathrm{N})}, \widehat{\mathrm{u}}_{1}^{(2 \mathrm{i}-2)}\right)=\mathrm{f}\left(\mathrm{L}_{\mathrm{N} / 2}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{N} / 2)}, \widehat{\mathrm{u}}_{1,0}^{(2 \mathrm{i}-2)} \oplus \hat{\mathrm{u}}_{1, \mathrm{e}}^{(2 \mathrm{i}-2)}\right), \mathrm{L}_{\mathrm{N} / 2}^{(\mathrm{i})}\left(\mathrm{y}_{\mathrm{N} / 2+1}^{(\mathrm{N}}, \hat{\mathrm{u}}_{1, \mathrm{e}}^{(2 \mathrm{i}-2)}\right)\right. \\
\mathrm{L}_{\mathrm{N}}^{(2 \mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{N})}, \hat{\mathrm{u}}_{1}^{(2 \mathrm{i}-1)}\right)=\mathrm{g}\left(\mathrm{L}_{\mathrm{N} / 2}^{(\mathrm{i})}\left(\mathrm{y}_{1}^{(\mathrm{N} / 2)}, \hat{\mathrm{u}}_{1,0}^{(2 \mathrm{i}-2)} \oplus \hat{\mathrm{u}}_{1, \mathrm{e}}^{(2 \mathrm{i}-2)}\right), \mathrm{L}_{\mathrm{N} / 2}^{(\mathrm{i})}\left(\mathrm{y}_{\mathrm{N} / 2+1}^{(\mathrm{N}} \hat{\mathrm{u}}_{1, \mathrm{e}}^{(2 \mathrm{i}-2)}\right), \hat{\mathrm{u}}_{2 \mathrm{i}-1}\right).
\end{gathered}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{gathered}
f(a, b)=\ln \left(\frac{1+e^{a+b}}{e^{a}+e^{b}}\right) \\
g\left(a, b, u_{s}\right)=(-1)^{u_{s}} a+b
\end{gathered}</script><p>上述递归的的终止条件为$N=1$,即递归到达了最左端的$W$端。此时：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{1}^{1}\left(\mathrm{y}_{\mathrm{j}}\right)=\ln \frac{\mathrm{W}\left(\mathrm{y}_{\mathrm{j}} \mid 0\right)}{\mathrm{W}\left(\mathrm{y}_{\mathrm{j}} \mid 1\right)}</script><p>由于我们在高斯信道下进行仿真，因此：</p>
<script type="math/tex; mode=display">
\mathrm{W}(\mathrm{y} \mid \mathrm{x})=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(\mathrm{y}-\mathrm{x})^{2}}{2 \sigma^{2}}\right)</script><p>给定递归出口：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{1}^{1}\left(\mathrm{y}_{\mathrm{j}}\right)=\frac{2 \mathrm{y}}{\sigma^{2}}</script><h4 id="译码结果"><a href="#译码结果" class="headerlink" title="译码结果"></a>译码结果</h4><p>循环求出最后一列所有节点的似然值，然后倒推计算之前各个节点的似然值，重复该过程直到最右端即可完成译码。</p>
<h2 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h2><h3 id="结果评估"><a href="#结果评估" class="headerlink" title="结果评估"></a>结果评估</h3><p>我们在信噪比为5-20dB的情况下进行不同码长的极化码编码译码仿真，并统计了误码率，结果如下所示：</p>
<p><img src="https://i.loli.net/2021/09/06/OkXdnBWDjHc1oRS.jpg" style="zoom:50%;" /></p>
<center>码长为8时SNR-BER</center>

<p><img src="https://i.loli.net/2021/09/09/rbHnPJFvyMx7a1f.jpg" style="zoom:50%;" /></p>
<center>码长为16时SNR-BER</center>

<p>信噪比提高时时，误码率也在减小。码长增大时，抗噪性能越好。</p>
<h3 id="改进以及探究"><a href="#改进以及探究" class="headerlink" title="改进以及探究"></a>改进以及探究</h3><h4 id="巴氏参数的递归求解"><a href="#巴氏参数的递归求解" class="headerlink" title="巴氏参数的递归求解"></a>巴氏参数的递归求解</h4><p>在文献$^{[1]}$中，给定的巴氏参数计算方法较为复杂。且考虑条件转移概率和信道特性后，我们可以将该过程用递归过程实现：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm{Z}\left(\mathrm{W}_{\mathrm{N}}^{(2 \mathrm{j}-1)}\right)=2 \mathrm{Z}\left(\mathrm{W}_{\mathrm{N} / 2}^{(\mathrm{j})}\right)-\mathrm{Z}\left(\mathrm{W}_{\mathrm{N} / 2}^{(\mathrm{j})}\right)^{2} \\
\mathrm{Z}\left(\mathrm{W}_{\mathrm{N}}^{(2 \mathrm{j})}\right)=\mathrm{Z}\left(\mathrm{W}_{\mathrm{N} / 2}^{(\mathrm{j})}\right)^{2}
\end{gathered}</script><h4 id="近似LLR硬件实现优化"><a href="#近似LLR硬件实现优化" class="headerlink" title="近似LLR硬件实现优化"></a>近似LLR硬件实现优化</h4><p>文献$^{[2]}$中指出，同时含有指数运算以及对数运算的过程不利于硬件实现，因此我们做出如下近似：</p>
<p>$$<br>f(a, b)=\ln \left(\frac{1+e^{a+b}}{e^{a}+e^{b}}\right)\thickapprox sgn(a·b)·\min\</p>
]]></content>
      <categories>
        <category>通信技术</category>
      </categories>
      <tags>
        <tag>5G通信</tag>
        <tag>极化码</tag>
      </tags>
  </entry>
  <entry>
    <title>MD5加密算法的Python实现流程</title>
    <url>/2022/07/29/MD5/</url>
    <content><![CDATA[<p><strong>MD5信息摘要算法</strong>（英语：MD5 Message-Digest Algorithm），一种被广泛使用的<a href="https://baike.baidu.com/item/密码散列函数/14937715">密码散列函数</a>，可以产生出一个128位（16<a href="https://baike.baidu.com/item/字节/1096318">字节</a>）的散列值（hash value），用于确保信息传输完整一致。MD5由美国密码学家<a href="https://baike.baidu.com/item/罗纳德·李维斯特/700199">罗纳德·李维斯特</a>（Ronald Linn Rivest）设计，于1992年公开，用以取代<a href="https://baike.baidu.com/item/MD4/8090275">MD4</a>算法。这套算法的程序在 RFC 1321 标准中被加以规范。1996年后该算法被证实存在弱点，可以被加以破解，对于需要高度安全性的数据，专家一般建议改用其他算法，如<a href="https://baike.baidu.com/item/SHA-2/22718180">SHA-2</a>。2004年，证实MD5算法无法防止碰撞（collision），因此不适用于安全性认证，如<a href="https://baike.baidu.com/item/SSL/320778">SSL</a>公开密钥认证或是<a href="https://baike.baidu.com/item/数字签名/212550">数字签名</a>等用途。</p>
<span id="more"></span>
<p>首先先要对MD5使用的初始数据进行初始化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A=<span class="number">0x67452301</span></span><br><span class="line"></span><br><span class="line">B=<span class="number">0xEFCDAB89</span></span><br><span class="line"></span><br><span class="line">C=<span class="number">0x98BADCFE</span></span><br><span class="line"></span><br><span class="line">D=<span class="number">0x10325476</span></span><br><span class="line">T = [<span class="number">0xD76AA478</span>,<span class="number">0xE8C7B756</span>,<span class="number">0x242070DB</span>,<span class="number">0xC1BDCEEE</span>,<span class="number">0xF57C0FAF</span>,<span class="number">0x4787C62A</span>,<span class="number">0xA8304613</span>,<span class="number">0xFD469501</span>,</span><br><span class="line">    <span class="number">0x698098D8</span>,<span class="number">0x8B44F7AF</span>,<span class="number">0xFFFF5BB1</span>,<span class="number">0x895CD7BE</span>,<span class="number">0x6B901122</span>,<span class="number">0xFD987193</span>,<span class="number">0xA679438E</span>,<span class="number">0x49B40821</span>,</span><br><span class="line">    <span class="number">0xF61E2562</span>,<span class="number">0xC040B340</span>,<span class="number">0x265E5A51</span>,<span class="number">0xE9B6C7AA</span>,<span class="number">0xD62F105D</span>,<span class="number">0x02441453</span>,<span class="number">0xD8A1E681</span>,<span class="number">0xE7D3FBC8</span>,</span><br><span class="line">    <span class="number">0x21E1CDE6</span>,<span class="number">0xC33707D6</span>,<span class="number">0xF4D50D87</span>,<span class="number">0x455A14ED</span>,<span class="number">0xA9E3E905</span>,<span class="number">0xFCEFA3F8</span>,<span class="number">0x676F02D9</span>,<span class="number">0x8D2A4C8A</span>,</span><br><span class="line">    <span class="number">0xFFFA3942</span>,<span class="number">0x8771F681</span>,<span class="number">0x6D9D6122</span>,<span class="number">0xFDE5380C</span>,<span class="number">0xA4BEEA44</span>,<span class="number">0x4BDECFA9</span>,<span class="number">0xF6BB4B60</span>,<span class="number">0xBEBFBC70</span>,</span><br><span class="line">    <span class="number">0x289B7EC6</span>,<span class="number">0xEAA127FA</span>,<span class="number">0xD4EF3085</span>,<span class="number">0x04881D05</span>,<span class="number">0xD9D4D039</span>,<span class="number">0xE6DB99E5</span>,<span class="number">0x1FA27CF8</span>,<span class="number">0xC4AC5665</span>,</span><br><span class="line">    <span class="number">0xF4292244</span>,<span class="number">0x432AFF97</span>,<span class="number">0xAB9423A7</span>,<span class="number">0xFC93A039</span>,<span class="number">0x655B59C3</span>,<span class="number">0x8F0CCC92</span>,<span class="number">0xFFEFF47D</span>,<span class="number">0x85845DD1</span>,</span><br><span class="line">    <span class="number">0x6FA87E4F</span>,<span class="number">0xFE2CE6E0</span>,<span class="number">0xA3014314</span>,<span class="number">0x4E0811A1</span>,<span class="number">0xF7537E82</span>,<span class="number">0xBD3AF235</span>,<span class="number">0x2AD7D2BB</span>,<span class="number">0xEB86D391</span>]</span><br><span class="line"></span><br><span class="line">s = [<span class="number">7</span>,<span class="number">12</span>,<span class="number">17</span>,<span class="number">22</span>,<span class="number">7</span>,<span class="number">12</span>,<span class="number">17</span>,<span class="number">22</span>,<span class="number">7</span>,<span class="number">12</span>,<span class="number">17</span>,<span class="number">22</span>,<span class="number">7</span>,<span class="number">12</span>,<span class="number">17</span>,<span class="number">22</span>,</span><br><span class="line">    <span class="number">5</span>,<span class="number">9</span>,<span class="number">14</span>,<span class="number">20</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">14</span>,<span class="number">20</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">14</span>,<span class="number">20</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">14</span>,<span class="number">20</span>,</span><br><span class="line">    <span class="number">4</span>,<span class="number">11</span>,<span class="number">16</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">16</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">16</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">16</span>,<span class="number">23</span>,</span><br><span class="line">    <span class="number">6</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">21</span>,<span class="number">6</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">21</span>,<span class="number">6</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">21</span>,<span class="number">6</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">21</span>]</span><br><span class="line"></span><br><span class="line">m = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,</span><br><span class="line">    <span class="number">1</span>,<span class="number">6</span>,<span class="number">11</span>,<span class="number">0</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">4</span>,<span class="number">9</span>,<span class="number">14</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">13</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">12</span>,</span><br><span class="line">    <span class="number">5</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">14</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">10</span>,<span class="number">13</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">15</span>,<span class="number">2</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">7</span>,<span class="number">14</span>,<span class="number">5</span>,<span class="number">12</span>,<span class="number">3</span>,<span class="number">10</span>,<span class="number">1</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">6</span>,<span class="number">13</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">2</span>,<span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>MD5加密需要经过输入转换、填充、四轮循环操作、置换以及最后的拼接等步骤。其中为了方便循环操作中的值的读取与置换，可以定义一个<code>MD5</code>类，并赋予此类一些初始的属性与函数简化后续的函数操作。现将类定义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">md5</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, message</span>):</span><br><span class="line">        self.message = message</span><br><span class="line">        self.A = A</span><br><span class="line">        self.B = B</span><br><span class="line">        self.C = C</span><br><span class="line">        self.D = D</span><br><span class="line">        self.init_A = A</span><br><span class="line">        self.init_B = B</span><br><span class="line">        self.init_C = C</span><br><span class="line">        self.init_D = D</span><br><span class="line">        self.temptext=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        self.s=s</span><br><span class="line">        self.m=m</span><br><span class="line">        self.T=T</span><br></pre></td></tr></table></figure>
<p>对于一个MD5对象，其拥有如下几个主要属性：</p>
<ul>
<li>message：  输入的明文</li>
<li>A,B,C,D：  四轮循环中的操作对象</li>
<li>temptext： 暂存的结果，用于缓冲与临时存放</li>
<li>s,m,T:    均为MD5加密时用到的操作参数，以列表的形式存储</li>
</ul>
<h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><ul>
<li>将输入的明文转换成ASCII码，并按照规则进行填充。填充规则如下：</li>
<li>当转换的ASCII码的长度的二进制长度不为$512*n+448$的整数倍时，先在末尾补一个“1”；</li>
<li>当此时的长度依然不为$512<em>n+448$时，继续在末尾补“0”直至长度为$512</em>n+448$</li>
<li>将输入密文的长度转换成64位的二进制数，若输入字符串的长度大于$2^{64}$，则取最后的64位。</li>
<li>将得到的64位进行小端排序，即按8bit/组进行倒序排列</li>
<li>将小端排序得到的结果拼接在步骤3得到的字符后，完成填充</li>
</ul>
<p>完成此项工作需要一个函数将输入的十进制字符串转换城标准长度的二进制数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decToBin</span>(<span class="params">num</span>):  <span class="comment"># 十进制转标准4位二或8位进制字符串函数 注意此为专用函数不要用于其他程序中</span></span><br><span class="line">    arry = []</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        arry.append(<span class="built_in">str</span>(num % <span class="number">2</span>))</span><br><span class="line">        num = num // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(arry)%<span class="number">4</span>!=<span class="number">0</span>:</span><br><span class="line">        arry.append(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(arry[::-<span class="number">1</span>])  <span class="comment"># 列表切片倒叙排列后再用join拼接</span></span><br></pre></td></tr></table></figure>
<p>下面进行填充的操作。将填充的方法归属于MD5对象专用，以防意外调用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fill</span>(<span class="params">self</span>):</span><br><span class="line">    bininput = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    raw_input=self.message</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(raw_input)):</span><br><span class="line">        c = decToBin(<span class="built_in">ord</span>(raw_input[i]))</span><br><span class="line">        bininput += c</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(bininput) % <span class="number">512</span> != <span class="number">448</span>):</span><br><span class="line">        <span class="keyword">if</span> ((<span class="built_in">len</span>(bininput) + <span class="number">1</span>) % <span class="number">512</span> != <span class="number">448</span>):</span><br><span class="line">            bininput += <span class="string">&#x27;1&#x27;</span>  <span class="comment"># 补1</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">len</span>(bininput) % <span class="number">512</span> != <span class="number">448</span>):</span><br><span class="line">            bininput += <span class="string">&#x27;0&#x27;</span>  <span class="comment"># 其余位补0直到满足长度为512*n+448</span></span><br><span class="line">    length = <span class="built_in">len</span>(raw_input) * <span class="number">8</span>  <span class="comment"># ASCII码位数</span></span><br><span class="line">    <span class="keyword">if</span> length &lt;= <span class="number">255</span>:</span><br><span class="line">        length = decToBin(length)  <span class="comment"># 二进制转换，此时length保留位数为8</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(length) &lt; <span class="number">8</span>:</span><br><span class="line">            length = <span class="string">&#x27;0&#x27;</span> + length</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        temp = decToBin(length)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(length) &lt; <span class="number">16</span>:</span><br><span class="line">            length = <span class="string">&#x27;0&#x27;</span> + length</span><br><span class="line">        length = length[<span class="number">8</span>:<span class="number">12</span>] + length[<span class="number">12</span>:<span class="number">16</span>] + length[<span class="number">0</span>:<span class="number">4</span>] + length[<span class="number">4</span>:<span class="number">8</span>]</span><br><span class="line">    bininput += length</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(bininput) % <span class="number">512</span> != <span class="number">0</span>:</span><br><span class="line">        bininput += <span class="string">&#x27;0&#x27;</span></span><br><span class="line">    self.temptext=bininput</span><br></pre></td></tr></table></figure>
<h2 id="4轮循环"><a href="#4轮循环" class="headerlink" title="4轮循环"></a>4轮循环</h2><h3 id="分解消息子块"><a href="#分解消息子块" class="headerlink" title="分解消息子块"></a>分解消息子块</h3><p>下面将得到的临时变量进行分组。将其分解成$16*32$的矩阵，并将其存放于<code>Mlist</code>中。后续将配合MD5类中的m矩阵进行操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Cycolyce</span>(<span class="params">self</span>):</span><br><span class="line">    Mlist=[]<span class="comment">#分16组</span></span><br><span class="line">    bininput=self.temptext</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">512</span>,<span class="number">32</span>):<span class="comment">#16</span></span><br><span class="line">        sym=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(bininput[i:i+<span class="number">32</span>]),<span class="number">4</span>):</span><br><span class="line">            temp=<span class="built_in">hex</span>(<span class="built_in">int</span>((bininput[i:i+<span class="number">32</span>][j:j+<span class="number">4</span>]),<span class="number">2</span>))<span class="comment">#取十六进制</span></span><br><span class="line">            sym+=temp[<span class="number">2</span>]<span class="comment">#只取字母 忽略前面的0x 为下面的小端排序做准备</span></span><br><span class="line"></span><br><span class="line">        sym_tmp=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>,<span class="number">0</span>,-<span class="number">2</span>):<span class="comment">#小端排序</span></span><br><span class="line">            temp=sym[j-<span class="number">2</span>:j]</span><br><span class="line">            sym_tmp+=temp</span><br><span class="line"></span><br><span class="line">        fn_sym=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sym_tmp)):</span><br><span class="line">            fn_sym+=decToBin(<span class="built_in">int</span>(sym_tmp[j], <span class="number">16</span>))<span class="comment">#再次转换成二进制</span></span><br><span class="line">        Mlist.append(fn_sym)</span><br></pre></td></tr></table></figure>
<h3 id="四轮循环的操作"><a href="#四轮循环的操作" class="headerlink" title="四轮循环的操作"></a>四轮循环的操作</h3><p>我们需要定义如下四个函数：</p>
<script type="math/tex; mode=display">
F(x,y,z)=(x\land y)\lor (\overline x \land z)\\
G(x,y,z)=(x\land z)\lor (y \lor \overline z)\\
H(x,y,z)=x\oplus y \oplus z\\
I(x,y,z)=y\oplus(x\lor \overline z)</script><p>以及定义四个操作：</p>
<script type="math/tex; mode=display">
FF(a,b,c,d,Mj,s,Ti)=(F(b,c,d)+a+mj+Ti)<<s\\
GG(a,b,c,d,Mj,s,Ti)=(G(b,c,d)+a+mj+Ti)<<s\\
HH(a,b,c,d,Mj,s,Ti)=(H(b,c,d)+a+mj+Ti)<<s\\
II(a,b,c,d,Mj,s,Ti)=(I(b,c,d)+a+mj+Ti)<<s\\</script><p>其中，$&lt;&lt;$是循环左移，$\oplus$是异或操作。在程序实现中，将四个函数定义在MD5类外，四个操作定义在MD5类以内。如此操作有两个原因：</p>
<ul>
<li>定义在外部的函数可以根据需求进行更改，对于封装后的类调用而言更具操作性</li>
<li>定义在类内部的操作是MD5的专用函数，必须专类专用</li>
<li>二者分离有助于设置断点、检查中间结果进行调试分析</li>
</ul>
<p>定义完函数以及相关操作后，我们将进行如下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">16</span>, <span class="number">4</span>):</span><br><span class="line">    self.FF(Mlist[m[j]], s[j], T[j])</span><br><span class="line">    self.FF(Mlist[m[j+<span class="number">1</span>]], s[j+<span class="number">1</span>], T[j+<span class="number">1</span>])</span><br><span class="line">    self.FF(Mlist[m[j+<span class="number">2</span>]], s[j+<span class="number">2</span>], T[j+<span class="number">2</span>])</span><br><span class="line">    self.FF(Mlist[m[j+<span class="number">3</span>]], s[j+<span class="number">3</span>], T[j+<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">16</span>, <span class="number">4</span>):</span><br><span class="line">    self.GG(Mlist[m[<span class="number">16</span>+j]], s[<span class="number">16</span>+j], T[<span class="number">16</span>+j])</span><br><span class="line">    self.GG(Mlist[m[<span class="number">16</span>+j+<span class="number">1</span>]], s[<span class="number">16</span>+j+<span class="number">1</span>], T[<span class="number">16</span>+j+<span class="number">1</span>])</span><br><span class="line">    self.GG(Mlist[m[<span class="number">16</span>+j+<span class="number">2</span>]], s[<span class="number">16</span>+j+<span class="number">2</span>], T[<span class="number">16</span>+j+<span class="number">2</span>])</span><br><span class="line">    self.GG(Mlist[m[<span class="number">16</span>+j+<span class="number">3</span>]], s[<span class="number">16</span>+j+<span class="number">3</span>], T[<span class="number">16</span>+j+<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">16</span>, <span class="number">4</span>):</span><br><span class="line">    self.HH(Mlist[m[<span class="number">32</span>+j]], s[<span class="number">32</span>+j], T[<span class="number">32</span>+j])</span><br><span class="line">    self.HH(Mlist[m[<span class="number">32</span>+j+<span class="number">1</span>]], s[<span class="number">32</span>+j+<span class="number">1</span>], T[<span class="number">32</span>+j+<span class="number">1</span>])</span><br><span class="line">    self.HH(Mlist[m[<span class="number">32</span>+j+<span class="number">2</span>]], s[<span class="number">32</span>+j+<span class="number">2</span>], T[<span class="number">32</span>+j+<span class="number">2</span>])</span><br><span class="line">    self.HH(Mlist[m[<span class="number">32</span>+j+<span class="number">3</span>]], s[<span class="number">32</span>+j+<span class="number">3</span>], T[<span class="number">32</span>+j+<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">16</span>, <span class="number">4</span>):</span><br><span class="line">    self.II(Mlist[m[<span class="number">48</span>+j]], s[<span class="number">48</span>+j], T[<span class="number">48</span>+j])</span><br><span class="line">    self.II(Mlist[m[<span class="number">48</span>+j+<span class="number">1</span>]], s[<span class="number">48</span>+j+<span class="number">1</span>], T[<span class="number">48</span>+j+<span class="number">1</span>])</span><br><span class="line">    self.II(Mlist[m[<span class="number">48</span>+j+<span class="number">2</span>]], s[<span class="number">48</span>+j+<span class="number">2</span>], T[<span class="number">48</span>+j+<span class="number">2</span>])</span><br><span class="line">    self.II(Mlist[m[<span class="number">48</span>+j+<span class="number">3</span>]], s[<span class="number">48</span>+j+<span class="number">3</span>], T[<span class="number">48</span>+j+<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h2 id="初值相加与拼接输出"><a href="#初值相加与拼接输出" class="headerlink" title="初值相加与拼接输出"></a>初值相加与拼接输出</h2><p>完成四轮循环后，需要将此时的ABCD与初始的ABCD相加，并进行小端排序。将小端排序后的结果拼接即可得到MD5加密的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.A = (self.A+self.init_A)%<span class="built_in">pow</span>(<span class="number">2</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="built_in">print</span>(self.A)</span><br><span class="line">        self.B = (self.B+self.init_B)%<span class="built_in">pow</span>(<span class="number">2</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="built_in">print</span>(self.B)</span><br><span class="line">        self.C = (self.C+self.init_C)%<span class="built_in">pow</span>(<span class="number">2</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="built_in">print</span>(self.C)</span><br><span class="line">        self.D = (self.D+self.init_D)%<span class="built_in">pow</span>(<span class="number">2</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="built_in">print</span>(self.D)</span><br><span class="line"></span><br><span class="line">        answer = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> [self.A, self.B, self.C, self.D]:</span><br><span class="line">            each = <span class="built_in">hex</span>(each)[<span class="number">2</span>:]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>, <span class="number">0</span>, -<span class="number">2</span>):</span><br><span class="line">                answer += <span class="built_in">str</span>(each[i - <span class="number">2</span>:i])</span><br><span class="line">        <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>
<p>我们可以运行一下检验成果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MD=md5(<span class="string">&#x27;abc&#x27;</span>)</span><br><span class="line">MD.fill()</span><br><span class="line">result=MD.Cycolyce()</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>通信技术</category>
        <category>加密技术</category>
      </categories>
      <tags>
        <tag>数字签名</tag>
        <tag>Python</tag>
        <tag>MD5</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch训练神经网络的一般过程</title>
    <url>/2022/08/04/Pytorch%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>这个是我在使用<code>Pytorch</code>进行图像相关任务的神经网络构建和训练过程中的一个记录，没有什么参考价值的，可以忽略~</p>
<p><img src="https://s2.loli.net/2022/08/20/oxvPZeT8G7ydzct.png" alt="avata1r.png" style="zoom:25%;" /></p>
<span id="more"></span>
<h1 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h1><p><code>Pytorch</code>中构建神经网络训练自己的数据集是个挺繁琐的事情，主要流程分为数据集的构建、网络构建、训练调参以及结果测试四个部分。数据集的构建最好是使用<code>DataLoader</code>类进行构造，这是<code>Pytorch</code>写好的一个类别，我们只需要简单了解下接口即可顺利地在有监督学习中使用。网络构建则是一个比较中规中矩的过程，写法比较固定，比较好上手；训练调参包含的东西比较繁琐，包括模型参数的调整、加载，以及包含<code>Tensorboard</code>对训练结果的可视化(可选)；最后则是测试部分，这个相对也比较简单。</p>
<h1 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data.DataLoader <span class="keyword">as</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,</span></span><br><span class="line"><span class="string">batch_sampler=None, num_workers=0, collate_fn=None,</span></span><br><span class="line"><span class="string">pin_memory=False, drop_last=False, timeout=0,</span></span><br><span class="line"><span class="string">worker_init_fn=None)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>下面对几个重要的参数进行解析。</p>
<h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>dataset只接受两种类型的输入，分别是<code>map-style datasets</code>和<code>iterable-style datasets</code>。</p>
<h3 id="map-style-datasets"><a href="#map-style-datasets" class="headerlink" title="map-style datasets"></a>map-style datasets</h3><p>是一个类，需自行构建两个魔术方法<code>__getitem__()</code>和<code>__len__()</code>，用于表示数据的索引到数据的内容的映射。</p>
<p>其中：</p>
<ul>
<li><code>__getitem__()</code>用于根据索引遍历全部数据</li>
<li><code>__len__()</code>用于返回数据集长度</li>
<li>创建dataset类时可以对数据进行预处理，预处理的函数可以夹杂在<code>__getitem__()</code>方法中进行调用，或者直接在<code>__init__()</code>或<code>__getitem__()</code>中进行编写，但<code>__getitem__()</code>必须根据<code>index</code>返回响应值，因为该值会通过index传到DataLoader类中后续处理。</li>
</ul>
<p>这里给出一个基本的模板：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;An abstract class representing a :class:`Dataset`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All datasets that represent a map from keys to data samples should subclass</span></span><br><span class="line"><span class="string">    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a</span></span><br><span class="line"><span class="string">    data sample for a given key. Subclasses could also optionally overwrite</span></span><br><span class="line"><span class="string">    :meth:`__len__`, which is expected to return the size of the dataset by many</span></span><br><span class="line"><span class="string">    :class:`~torch.utils.data.Sampler` implementations and the default options</span></span><br><span class="line"><span class="string">    of :class:`~torch.utils.data.DataLoader`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">      :class:`~torch.utils.data.DataLoader` by default constructs a index</span></span><br><span class="line"><span class="string">      sampler that yields integral indices.  To make it work with a map-style</span></span><br><span class="line"><span class="string">      dataset with non-integral indices/keys, a custom sampler must be provided.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>给出几种常用的Dataset构造：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值类型的数据构造</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Num_dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    	<span class="comment"># 注意这里输入进来的每个数据是Tensor类型的，承载的部分随便</span></span><br><span class="line">        self.x = torch.randn(<span class="number">1000</span>,<span class="number">3</span>)</span><br><span class="line">        self.y = self.x.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">        self.src,  self.trg = [], []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">            self.src.append(self.x[i])</span><br><span class="line">            self.trg.append(self.y[i])</span><br><span class="line">           </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.src[index], self.trg[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.src) </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 图片类型的数据构造</span></span><br><span class="line"><span class="comment"># 还不完善 等我修补下</span></span><br><span class="line">train_dir = <span class="string">&quot;../data/hotdog/train&quot;</span></span><br><span class="line">test_dir = <span class="string">&quot;../data/hotdog/test&quot;</span></span><br><span class="line"></span><br><span class="line">mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Img_dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path</span>):</span><br><span class="line">        data_root = pathlib.Path(path)</span><br><span class="line">        all_image_paths = <span class="built_in">list</span>(data_root.glob(<span class="string">&#x27;*/*&#x27;</span>))</span><br><span class="line">        self.all_image_paths = [<span class="built_in">str</span>(path) <span class="keyword">for</span> path <span class="keyword">in</span> all_image_paths]</span><br><span class="line">        label_names = <span class="built_in">sorted</span>(item.name <span class="keyword">for</span> item <span class="keyword">in</span> data_root.glob(<span class="string">&#x27;*/&#x27;</span>) <span class="keyword">if</span> item.is_dir())</span><br><span class="line">        label_to_index = <span class="built_in">dict</span>((label, index) <span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_names))</span><br><span class="line">        self.all_image_labels = [label_to_index[path.parent.name] <span class="keyword">for</span> path <span class="keyword">in</span> all_image_paths]</span><br><span class="line">        self.mean = np.array(mean).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        self.std = np.array(std).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        img = cv.imread(self.all_image_paths[index])</span><br><span class="line">        img = cv.resize(img, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        img = img / <span class="number">255.</span></span><br><span class="line">        img = (img - self.mean) / self.std</span><br><span class="line">        img = np.transpose(img, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        label = self.all_image_labels[index]</span><br><span class="line">        img = torch.tensor(img, dtype=torch.float32)</span><br><span class="line">        label = torch.tensor(label)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.all_image_paths)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Iterable-style-datasets"><a href="#Iterable-style-datasets" class="headerlink" title="Iterable-style datasets"></a>Iterable-style datasets</h3><p>可迭代样式的数据集是IterableDataset的一个实例，该实例必须重写<strong>iter</strong>方法,该方法用于对数据集进行迭代。这种类型的数据集特别适合随机读取数据不太可能实现的情况，并且批处理大小batchsize取决于获取的数据。比如读取数据库，远程服务器或者实时日志等数据的时候，可使用该样式，一般时序数据不使用这种样式。我就摸了~</p>
<h2 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h2><p>一次抽取的数据多少，最好根据数据量的多少以及自己的设备的内存、GPU、显存进行选择，尽量选择2的正整数次幂作为batchsize。</p>
<h2 id="numworkers"><a href="#numworkers" class="headerlink" title="numworkers"></a>numworkers</h2><p>参与程序使用的CPU核心数，注意使用该参数后需要自写主函数入口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h2><p>是否对数据进行打乱，一般都选True，因为数据的相关性会影响网络的训练泛化性能。</p>
<h1 id="网络构造"><a href="#网络构造" class="headerlink" title="网络构造"></a>网络构造</h1><p>所有的网络构造都需要继承<code>nn.Module</code>类，然后在构造函数<code>__init__()</code>中构造一些网络层的成员，然后重写<code>forward(self,x)</code>成员函数。下面给出LeNet的一个构造示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">3</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">6</span>*<span class="number">6</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.pool1(torch.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(torch.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>Pytorch</code>构造的网络使用方式比较直观，在实例化对象后，可以直接使调用输出结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = LeNet()</span><br><span class="line">    ret = model(torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    <span class="built_in">print</span>(ret.shape)</span><br><span class="line">    <span class="comment">#torch.Size([1, 10])</span></span><br></pre></td></tr></table></figure>
<p>这是因为它继承了<code>nn.Module</code>中的<code>__call__()</code>方法，而默认的<code>__call__()</code>方法则是定义为调用<code>forward(self,x)</code>这个成员函数。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"></span><br><span class="line"><span class="comment">#Dataset</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load Data</span></span><br><span class="line"></span><br><span class="line">data_train = MNIST(<span class="string">&#x27;./data&#x27;</span>, </span><br><span class="line">                    download = <span class="literal">True</span>, </span><br><span class="line">                    transform = transforms.Compose([transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)),transforms.ToTensor()]))</span><br><span class="line"></span><br><span class="line">data_train_loader = DataLoader(data_train, batch_size = <span class="number">32</span>, shuffle= <span class="literal">True</span>, num_workers=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">model = LeNet()</span><br><span class="line"><span class="comment"># 切换状态</span></span><br><span class="line">model.train()</span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = <span class="number">0.9</span>, weight_decay = <span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">train_loss = <span class="number">0</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_train_loader):</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = loss_func(outputs, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        _, predicted = outputs.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">        total += targets.size(<span class="number">0</span>)</span><br><span class="line">        correct += predicted.eq(targets).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(batch_idx, <span class="built_in">len</span>(data_train_loader), <span class="string">&quot;Loss: %.3f | ACC: %.3f%% (%d/%d)&quot;</span> % (train_loss/(batch_idx+<span class="number">1</span>), <span class="number">100.</span>*correct/total, correct, total))</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<p>值得注意的是，这里的训练只训练了一个epoch，如果训练多个epoch则需要再套一层for循环。</p>
<p>如果要使用一些已经预训练好的模型的权重，则可以选择加载部分权重。分为几种情况：</p>
<h2 id="结构相同，某些层不加载"><a href="#结构相同，某些层不加载" class="headerlink" title="结构相同，某些层不加载"></a>结构相同，某些层不加载</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = LeNet()</span><br><span class="line">model_dict = model.state_dict()<span class="comment">#新的模型结构</span></span><br><span class="line">pre_train = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">pretrain_dict = pre_train.state_dict()<span class="comment">#已有的模型权重</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> pretrain_dict:</span><br><span class="line">    <span class="built_in">print</span>(each)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">load_pretrained_dict = &#123;key: value <span class="keyword">for</span> key, value <span class="keyword">in</span> pretrain_dict.items() <span class="keyword">if</span> (key <span class="keyword">in</span> model_dict <span class="keyword">and</span> <span class="string">&#x27;fc1&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> key)&#125;</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> load_pretrained_dict:</span><br><span class="line">    <span class="built_in">print</span>(each)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">conv1.weight</span></span><br><span class="line"><span class="string">conv1.bias</span></span><br><span class="line"><span class="string">conv2.weight</span></span><br><span class="line"><span class="string">conv2.bias</span></span><br><span class="line"><span class="string">fc1.weight</span></span><br><span class="line"><span class="string">fc1.bias</span></span><br><span class="line"><span class="string">fc2.weight</span></span><br><span class="line"><span class="string">fc2.bias</span></span><br><span class="line"><span class="string">fc3.weight</span></span><br><span class="line"><span class="string">fc3.bias</span></span><br><span class="line"><span class="string">--------------------</span></span><br><span class="line"><span class="string">conv1.weight</span></span><br><span class="line"><span class="string">conv1.bias</span></span><br><span class="line"><span class="string">conv2.weight</span></span><br><span class="line"><span class="string">conv2.bias</span></span><br><span class="line"><span class="string">fc2.weight</span></span><br><span class="line"><span class="string">fc2.bias</span></span><br><span class="line"><span class="string">fc3.weight</span></span><br><span class="line"><span class="string">fc3.bias</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="结构不同，不同层不加载"><a href="#结构不同，不同层不加载" class="headerlink" title="结构不同，不同层不加载"></a>结构不同，不同层不加载</h2><p>与上面类似，只不过把筛选语句修改以下就可以了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load_pretrained_dict = &#123;key: value <span class="keyword">for</span> key, value <span class="keyword">in</span> pretrain_dict.items() <span class="keyword">if</span> (key <span class="keyword">in</span> model_dict)&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>一种用于路径规划的栅格地图简易生成方法</title>
    <url>/2022/07/29/%E4%B8%80%E7%A7%8D%E6%A0%85%E6%A0%BC%E5%9C%B0%E5%9B%BE%E7%9A%84%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>做机器人路径规划还有避碰方面的应该都离不开栅格地图环境。最近一个项目是要使用N*N大小的栅格地图生成可视障碍物，并要保证在动作空间$\mathcal A=\{up,down.left,right\}$时，从起点到终点有至少一条可行的道路。所有的障碍物用黑色表示，可行区域用白色表示。栅格地图的生成我看网上大多数的文章是基于matlab生成的，使用python的比较少，即使有也是使用gym这个强化学习测试平台进行搭建的，它是强化学习中一个公认的标准测试平台是，用户可以通过自定义环境或修改现有环境构建一个符合自己需要的模型，但是开发文档太长了…它的内核是pygame，有些规范化的东西其实我不太喜欢，而且代码也忒长了。我参考了这位<a href="https://www.ihawo.com/archives/85.html">大佬的文章</a>尝试搭了一下，奈何发现修改起来太麻烦了。gym其实还有一个问题就是，它的环境是固定死的，也就是说你想调整栅格地图的大小和精度就必须重新编写程序，这样其实对需要做大量对比实验的人来说非常不友好，因此我构想了一种更快更容易修改的栅格地图生成法。</p>
<span id="more"></span>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>算法的核心其实就是构建一个矩阵，以0标识为可行区域，以1标识障碍区域。矩阵的构建约束为在每一个标识为0的区域在其四个方向的单步延拓至少有一个标识为0的区域，且对于设定的起点与终点均需保证他们的标识为0。我的想法是将可视化工作用热力图来表示，seaborn的热力图比plt的好看多了就选择了seaborn。可以先进行一个简单的<code>5*5</code>大小的生成测试观察可视化结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line"><span class="comment">#二维的数组的热力图，横轴和数轴的ticklabels要加上去的话，既可以通过将array转换成有column</span></span><br><span class="line"><span class="comment">#和index的DataFrame直接绘图生成，也可以后续再加上去。后面加上去的话，更灵活，包括可设置labels大小方向等。</span></span><br><span class="line">sns.heatmap(pd.DataFrame(np.<span class="built_in">round</span>(a,<span class="number">2</span>), columns = [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;3&#x27;</span>,<span class="string">&#x27;4&#x27;</span>], index = [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;3&#x27;</span>,<span class="string">&#x27;4&#x27;</span>]), linecolor=<span class="string">&#x27;gray&#x27;</span>,linewidths=<span class="number">1</span>,</span><br><span class="line">                annot=<span class="literal">True</span>, vmax=<span class="number">1</span>,vmin = <span class="number">0</span>, xticklabels= <span class="literal">True</span>, yticklabels= <span class="literal">True</span>, square=<span class="literal">True</span>,cmap=<span class="string">&quot;hot_r&quot;</span>, cbar= <span class="literal">False</span>)<span class="comment"># cmap=&quot;Blues&quot;</span></span><br><span class="line">plt.savefig(<span class="string">&#x27;5x5.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2022/05/30/4chvdXGm7U3RnfM.png" style="zoom: 33%;" /></p>
<p>这里我设定的起点是(0,0)和(4,4)。可以看到通过我人为的控制保证了一条路径生成。但是随着N的扩大，这种人为控制的方法显然是不现实的，而且这样生成的图也有明显的人为痕迹，因此我们需要进一步的拓展。我们可以先生成路径，再生成随机的地图，这样就能满足需求了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Function: Draw a grid map with the size of N*N</span></span><br><span class="line"><span class="string">Input: N -- the size of the grip map</span></span><br><span class="line"><span class="string">Output: None</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drawGrid</span>(<span class="params">N</span>):</span><br><span class="line">    routedata = []	<span class="comment">#用于存储路径</span></span><br><span class="line">    alldata = []	<span class="comment">#用于存储栅格信息</span></span><br><span class="line">    a,b = <span class="number">0</span>,<span class="number">0</span>		<span class="comment">#路径的行与列索引</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 路径生成</span></span><br><span class="line">    <span class="keyword">while</span> a!= N-<span class="number">1</span> <span class="keyword">or</span> b!=N-<span class="number">1</span>:</span><br><span class="line">        judge = random.randint(<span class="number">0</span>,<span class="number">1</span>)	<span class="comment"># 栅格50%覆盖</span></span><br><span class="line">        <span class="keyword">if</span> a &lt; N-<span class="number">1</span> <span class="keyword">and</span> b&lt;N-<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> judge==<span class="number">1</span>:</span><br><span class="line">                a += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                b += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> a&gt;=N-<span class="number">1</span> <span class="keyword">and</span> b&lt;N-<span class="number">1</span>:</span><br><span class="line">            b += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> a &lt; N-<span class="number">1</span> <span class="keyword">and</span> b &gt;= N-<span class="number">1</span>:</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;error!&quot;</span>)</span><br><span class="line">        routedata.append([a,b])</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 栅格生成</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        subdata = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            a = random.randint(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            subdata.append(a)</span><br><span class="line">        alldata.append(subdata)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 强行覆盖地图</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> routedata:</span><br><span class="line">        alldata[each[<span class="number">0</span>]][each[<span class="number">1</span>]] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 强行覆盖初始位置</span></span><br><span class="line">    alldata[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    alldata[N-<span class="number">1</span>][N-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    data = np.array(alldata)</span><br><span class="line">    fig, ax = plt.subplots(figsize = (<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line">    sns.heatmap(pd.DataFrame(np.<span class="built_in">round</span>(data,<span class="number">2</span>)), linecolor=<span class="string">&#x27;gray&#x27;</span>,linewidths=<span class="number">1</span>,</span><br><span class="line">        annot=<span class="literal">True</span>, vmax=<span class="number">1</span>,vmin = <span class="number">0</span>, xticklabels= <span class="literal">True</span>, yticklabels= <span class="literal">True</span>, square=<span class="literal">True</span>,cmap=<span class="string">&quot;hot_r&quot;</span>, cbar= <span class="literal">False</span>)<span class="comment"># cmap=&quot;Blues&quot;</span></span><br><span class="line">    <span class="comment">#plt.savefig(&#x27;N*N.png&#x27;)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>测试一下<code>N=15</code>和<code>N=20</code>的情况：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drawGrid(15)</span><br><span class="line">drawGrid(20)</span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2022/05/30/nQ2bdJofyCTG9V3.png" style="zoom: 33%;" /></p>
<center><small>10*10栅格地图</small></center>

<p><img src="https://s2.loli.net/2022/05/30/fl8qz47knpRu9O2.png" alt="20x20" style="zoom:33%;" /></p>
<center><small>20*20栅格地图</small></center>

<p>大功告成~</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果使用gym去构建的话，我估计20*20的代码量大概要两三百行…但是这种方法生成的话应该会快一点。gym的代码量大其实可以理解，毕竟它还需要负责智体的交互部分，所以大家按需选择咯。我的方案是生成图片再使用像素值得差异进行路径规划算法的研究。</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>栅格地图</tag>
      </tags>
  </entry>
  <entry>
    <title>从一维卷积到二维卷积：浅析图像卷积</title>
    <url>/2022/08/03/%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF/</url>
    <content><![CDATA[<h2 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h2><p>首先先给出一维情况的卷积的定义。</p>
<script type="math/tex; mode=display">
f(x)*g(x)=\int_{-\infty}^{\infty}f(x)g(\tau-x)d\tau</script><p>这个式子很难从直观层面进行理解，因为它包含了乘积、翻转、积分三个部分的内容。如果你是信通学子，那么你肯定会知道两个函数的卷积结果可以看作是他们对应的傅里叶变换在频域的乘积的反变换，也就是说把一个函数作为输入信号，另一个函数作为描述系统的一个方程，当这个系统接收到输入信号后会做出响应，这个响应$y(t)$就是输入信号$x(t)$与系统函数$h(t)$卷积的结果：</p>
<script type="math/tex; mode=display">
y(t)=x(t)*h(t)=\mathcal F^{-1}[X(jw)H(jw)]</script><p>但很明显不是所有人都学过信号与系统，所以我想了个更<del>好</del>恶心的例子进行描述。<span id="more"></span></p>
<h3 id="一个恶心的例子"><a href="#一个恶心的例子" class="headerlink" title="一个恶心的例子"></a>一个<del>恶心</del>的例子</h3><p>试想这样一个场景：一个热门旅游景点的厕所。我们知道厕所需要连连不断地接待客人（？），因此厕所的囤粪速度可以用函数$F(x)$表示，如图1。</p>
<p><img src="https://s2.loli.net/2022/08/03/7FHNVEs1uM5RCIB.png" alt="image-20220724172639005" style="zoom:50%;" /></p>
<center><small>图1粪便囤积变化率曲线</small></center>

<p>但是厕所连通的化粪池它的处理能力有限，它对排泄物的处理效率如图2所示，我们用函数$H(x)$来表示。</p>
<p><img src="https://s2.loli.net/2022/08/03/BtJzpROGo3rTNYF.png" alt="image-20220724172618665" style="zoom:50%;" /></p>
<center><small>图2 化粪池分解效率</small></center>

<p>下面我们的问题就是需要讨论，<strong>化粪池在某个时候还有多少粪便啦！</strong></p>
<h3 id="怎么计算？"><a href="#怎么计算？" class="headerlink" title="怎么计算？"></a>怎么计算？</h3><p>每一个粪便都是独立的个体，它们都会随着化粪池分解而减少。如果不考虑粪便的分解的话，那么粪便总量可以这样表示：</p>
<script type="math/tex; mode=display">
n =\int_{-\infty}^{\infty}F(x)dx</script><p>但是我们的粪便不是无线增加的，因此，需要考虑分解的情况。现在假设在$t$时刻时进行统计，经过时间间隔$\tau$后统计存在的粪便总量为：</p>
<script type="math/tex; mode=display">
n' = F(t)·H(\tau)</script><p>也就是说在$t+\tau$时刻，不考虑其他情况下的粪便输入，此时一共有$n’$这么多的粪便。这里则是对应了卷积操作中的乘积部分。我们换一种写法，令$T=t+\tau$，则在$T$时刻不考虑其他粪便的增加的情况下，粪便量可以记为:</p>
<script type="math/tex; mode=display">
n'=F(t)·H(T-t)</script><p>$H(t)$函数的变量出现了x轴反转的情况，我们离卷积的定义也越来越近啦！下一步就是积分的问题了。积分其实也很好理解，因为上面这些式子都是只考虑了单个时刻不考虑其他时刻的输入信息，因此如果要考虑其他时刻的输入的话，则是把其他时刻的粪便量进行一个叠加（离散域就是使用$\sum$求和，连续域就是使用$\int$积分）。因此$t$时刻的粪便总量可以表示为：</p>
<script type="math/tex; mode=display">
Y(t)=\int_{-\infty}^{t}F(t)H(\tau-t)d\tau</script><p>是不是这样就很好理解了？</p>
<h3 id="图像的卷积是什么？"><a href="#图像的卷积是什么？" class="headerlink" title="图像的卷积是什么？"></a>图像的卷积是什么？</h3><p>卷积的本质在刚刚已经说了，就是反转、乘积、求和。到了二维也是这样子的：\</p>
<script type="math/tex; mode=display">
Y(x,y)=F(x,y)*H(x,y)=\sum_{n_1=-\infty}^{\infty}\sum_{n_2=-\infty}^{\infty}F(n_1,n_2)·H(x-n_1,y-n_2)</script><p>好！下面我们给出图像卷积的一个步骤：</p>
<p><img src="https://img-blog.csdnimg.cn/20200308112430934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hcnVoaW5h,size_16,color_FFFFFF,t_70" alt=""></p>
<p>其中这个<code>Convolution Kernel</code>叫做卷积核，<code>Source Pixel</code>就是我们的图像。在这张图里红色通道对应的地方就是卷积处理的第一个像素，计算的公式在右上角，输入是1，输出是8。随着卷积核往下一个像素移动，优惠计算出新的像素值，直到按设定的规则遍历完图像。</p>
<p><img src="https://pic3.zhimg.com/v2-8a6695c2e086525ac5a61610348739b2_b.gif" alt=""></p>
<p>唯一需要指出的是，在这里体现了乘积和加和，那么反转是在哪里体现的呢？其实是图中卷积核已经是翻转了180°了，也就是说原本的卷积核它的“4”是在”-4“的位置的。所以这就是图像卷积的操作，是不是很好理解捏？</p>
<h2 id="为什么是卷积"><a href="#为什么是卷积" class="headerlink" title="为什么是卷积"></a>为什么是卷积</h2><p>上结论：卷积的本质是提取图像中的特征。许多常用的图像处理方法都可以用卷积的方式表示，因此选择卷积。</p>
<h2 id="Pytorch的图像卷积"><a href="#Pytorch的图像卷积" class="headerlink" title="Pytorch的图像卷积"></a>Pytorch的图像卷积</h2><p><img src="https://img-blog.csdnimg.cn/20201121195053531.gif" alt=""></p>
<p>终于来到这里了。我们先看看pytorch卷积层继承的父类<code>_ConvNd</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_ConvNd</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 out_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 kernel_size: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params">                 stride: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params">                 padding: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params">                 dilation: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params">                 transposed: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">                 output_padding: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...],</span></span><br><span class="line"><span class="params">                 groups: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 bias: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">                 padding_mode: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>
<p>主要关注<code>in_channels</code>,<code>out_channels</code>,<code>kernel_size</code>,<code>stride</code>,<code>padding</code>这几个参数。</p>
<h3 id="in-channels"><a href="#in-channels" class="headerlink" title="in_channels"></a>in_channels</h3><p><code>in_channels</code>的输入是按<code>(N,C,H,W)</code>排列的，<code>N</code>是mini-batch大小，<code>C</code>是通道数channel，<code>H</code>和<code>W</code>分别是输入的高和宽，<strong>这和<code>Tensorflow</code>的不太一样。</strong></p>
<h3 id="out-channels"><a href="#out-channels" class="headerlink" title="out_channels"></a>out_channels</h3><p>在定义好了<code>in_channels</code>后，<code>out_channels</code>只需要关注输出通道数<code>K</code>即可。</p>
<h3 id="kernel-size"><a href="#kernel-size" class="headerlink" title="kernel_size"></a>kernel_size</h3><p>卷积核大小，如果设为一个整数的话那么卷积核就是一个正方形的，否则就按长款的顺序输入一个<code>tuple</code>作为卷积核形状。</p>
<h3 id="stride"><a href="#stride" class="headerlink" title="stride"></a>stride</h3><p>步长。也就是卷积核移动的距离，可以是一个整数也可以是一个<code>tuple</code>，类似<code>kernel_size</code>。</p>
<h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>补丁，就是只要使用了大小大于1的卷积核去做卷积的话，输出图像的大小是比原本的图像大小要小的，因此为了保证维度的大小一致，会引入padding填充原图像的边缘位置，该值表示的是沿原图边缘进行该值大小的延拓。</p>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>【学习记录】2022-8-20-？</title>
    <url>/2022/08/20/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952022-8-20/</url>
    <content><![CDATA[<p>师兄让我别去CV卷了，让我搞搞通信。然后就是让我看看5G核心网相关的内容，笑死😂，我本科的时候移动通信的老师也和我们一起摆烂，考试开卷基本没啥东西记住的。不过想想也是，CV就业压力挺大的，当成业余兴趣爱好去研究下也不错，做成工作可能也很难受，也卷不过985的顶级实验室出来的。娄山雀说他前几天有个北大ACM区域金牌的计算机硕去面他们的后端都没有机会，然后四个清华大学的硕去面OPPO连简历复筛都没过……很可怕很可怕。然后最近看了严曦老师的《造神年代》，我再次燃起了对通信事业的热情。链接每个节点、信息的传输，和生命的波纹好像，当这些所有波纹和对事件的响应结合到一起构建成了一个系统，就完成了一个宏观定义的世界交互（我好像在说一些乱七八糟的屁话😂）这种感觉和造物其实也挺像的。所以开始吧：</p>
<span id="more"></span>
<h2 id="说在开始前"><a href="#说在开始前" class="headerlink" title="说在开始前"></a>说在开始前</h2><p>我对通信比较喜欢不上来的点在于更新速度太快，然后各种缩写在查阅资料的时候又非常麻烦，所以这次真的是破釜沉舟，把每一个缩写都给吃透了。阅读的时候如果有需要查阅的缩写可以直接在网站的搜索功能进行搜索，应该会有解释的。这次学习的内容主要是5G的核心网，由于我的基础比较薄弱，所以我得花花时间了解下发展的历程和了解些工程的问题。</p>
<p><img src="https://s2.loli.net/2022/08/21/8tsFlNCMLvTDSeX.png" alt="img" style="zoom: 25%;" /></p>
<h2 id="核心网"><a href="#核心网" class="headerlink" title="核心网"></a>核心网</h2><h3 id="核心网是什么"><a href="#核心网是什么" class="headerlink" title="核心网是什么"></a>核心网是什么</h3><p>在了解5G核心网之前我们需要先了解核心网到底是什么。核心网，有个洋气的英文名，叫Core Network（核心网络），简称CN。说实话，核心网的概念很难定义！广义上的解释难以界定，而狭义角度的解释又没有对应实体。我们只能通过类比以下来描述一下核心网：试想一个快递分拣中心，上面的传送带装满了双十一的各种包裹，传送带是负责传送包裹的，但是哪一个包裹需要传送到哪个地方，是需要一个智体进行调配的。在这个分拣中心，包裹相当于需要传输的数据，传送带相当于一个承载网络，作为信息的承载介质，而我们的核心网就是负责调配数据应该如何传输的指挥官。在这个基础上，我们可以进一步看看核心网到底是怎么发展的。</p>
<h3 id="核心网的发展"><a href="#核心网的发展" class="headerlink" title="核心网的发展"></a>核心网的发展</h3><p>首先先说说2G，为什么不从1G开始说呢，<del>因为1G的时候根本上不了网啊😂</del>因为1G和2G的核心网架构是非常近似的，这里可以直接等价了。</p>
<p><img src="https://s2.loli.net/2022/08/21/xMaLeEgD5KvWkcp.jpg" alt="640?wx_fmt=png" style="zoom:50%;" /></p>
<center><small>2G核心网架构</small></center>



<p>2G其实前期也是没法上网的，直到2.5G出现才解决了这一问题。从图中可以看出来，2G的核心网组网非常简单，MSC就是核心网的最主要设备。HLR、EIR和用户身份有关，用于鉴权。值得注意的是，之所以图上面写的是“MSC/VLR”，是因为VLR是一个功能实体，但是物理上，VLR和MSC是同一个硬件设备。相当于一个设备实现了两个角色，所以画在一起。HLR/AUC也是如此，HLR和AUC物理合一。下面是2.5G的核心网：</p>
<p><img src = 'https://s2.loli.net/2022/08/21/2qwdskLl6fxXOhQ.jpg' style="zoom:50%;" ></p>
<center><small>2.5G核心网架构，为了图简单没有把2G核心网里面的HLR等模块画上去</small></center>



<p>相比于2G其实就是多了SGSN和GGSN两个模块。这两个模块都是为了实现GPRS数据业务而存在的，GPRS则是2.5G上网的核心服务：</p>
<p><img src="https://s2.loli.net/2022/08/21/vcplj7bGWZX9ymr.png" alt="640?wx_fmt=png" style="zoom:67%;" /></p>
<p>再之后就是3G时代了：</p>
<p><img src="https://s2.loli.net/2022/08/21/9JoVwjSYZR1Gvtb.jpg" alt="640?wx_fmt=png" style="zoom: 50%;" /></p>
<center><small>3G核心网架构，为了图简单没把HLR等模块画上去</small></center>



<p>可以看出3G其实变化较大的部分是基站部分，引入了一个<code>NodeB</code>和一个<code>RNC</code>的东西。这个<code>NodeB</code>其实就是专属于3G时代的基站，只是为了区分和2G时代的基站而另起一个名字而已😂这样的作法会在后面的4G、5G都有体现。尽管在图中3G核心网的变化与2G的相比没啥不同，但其实硬件设备已经经过了一轮新的迭代。我们知道，硬件才是工程里面最能指导性能突破的核心力量，因此3G的成功之处很大一部分要感谢硬件的蓬勃发展。不要小看了硬件平台，实际上，就像最开始<strong>华为的C&amp;C08</strong>、<strong>中兴的ZXJ10</strong>一样，设备商自家的很多不同业务的设备，都是基于同一个硬件平台进行开发的。不可能每个设备都单独开发硬件平台，既浪费时间和精力，又不利于生产和维护。稳定可靠且处理能力强大的硬件平台，是产品的基石。从2G到3G，除了IP化以及硬件平台的升级之外，最大的不同就在于分离。在通信系统里面，说白了，就两个（平）面，用户面和控制面：</p>
<p><img src="https://s2.loli.net/2022/08/21/ZrOvf4BPnq1sxW5.jpg" alt="640?wx_fmt=png" style="zoom:50%;" /></p>
<p><img src="https://s2.loli.net/2022/08/21/mjOSBUkM4HC8YiF.png" alt="640?wx_fmt=png" style="zoom:50%;" /></p>
<p>用户面，就是用户的实际业务数据，就是你的语音数据啊，视频流数据啊之类的。而控制面，是为了管理数据走向的信令、命令。这两个面，在通信设备内部，就相当于两个不同的系统，2G时代，用户面和控制面没有明显分开。3G时代，把两个面进行了分离。接着，SGSN变成MME，GGSN变成SGW/PGW，也就演进成了4G核心网：</p>
<p><img src="https://s2.loli.net/2022/08/21/jIu8dYUeKiX5HDp.jpg" alt="640?wx_fmt=png" style="zoom:50%;" /></p>
<p>值得注意的是基站里面的RNC没有了，为了实现扁平化，功能一部分给了核心网，一部分给了<code>eNodeB</code>（还记得我说的NodeB是什么嘛）。发展到这里的时候，硬件平台也升级了，以中兴为例，开始启用ATCA/ETCA平台（后来MME就用了它），还有xGW T8000平台（后面PGW和SGW用了它，PGW和SGW物理上是一体的）。</p>
<p><img src="https://s2.loli.net/2022/08/21/YTSwIyQ87Z5RUgC.jpg" alt="?wx_fmt=jpeg"></p>
<center><small>中兴ATCA机框</small></center>



<p>在3G到4G的过程中，IMS出现了，取代传统CS（也就是MSC那些），提供更强大的多媒体服务（语音、图片短信、视频电话等）。IMS，使用的也主要是ATCA平台。实际上这些硬件很像一个电脑，有处理器（MP单板），有网卡（以太网接口卡，光纤接口卡）。而ATCA平台，更像一台电脑了，前面你也看到了，名字就叫“先进电信计算平台”，也就是“电信服务器”嘛。确切说，ATCA里面的业务处理单板，本身就是一台单板造型的“小型化电脑”，有处理器、内存、硬盘，俗称“刀片”。</p>
<p><img src="https://s2.loli.net/2022/08/21/VNQCTUz3WoZdKhe.jpg" alt="640?wx_fmt=png"></p>
<center><small>ATCA业务处理板——“刀片”</small></center>



<p>既然都走到这一步，原来的专用硬件，越做越像IT机房里面的x86通用服务器，那么，不如干脆直接用x86服务器吧!于是虚拟化时代就到来了。虚拟化，就是网元功能虚拟化（Network Function Virtualization，<strong>NFV</strong>）。说白了就是在硬件上，直接采用HP、IBM等IT厂家的x86平台通用服务器（目前以刀片服务器为主，节约空间，也够用）。</p>
<p><img src="https://s2.loli.net/2022/08/21/9whUsXVCz2xHRl7.jpg" alt="640?wx_fmt=jpeg"></p>
<p>软件上，设备商基于openstack这样的开源平台，开发自己的虚拟化平台，把以前的核心网网元，“种植”在这个平台之上：</p>
<p><img src="https://s2.loli.net/2022/08/21/NnYyhD6bJQv3em9.jpg" alt="?wx_fmt=jpeg"></p>
<p>虚拟化平台不等于5G核心网。也就是说，并不是只有5G才能用虚拟化平台。也不是用了虚拟化平台，就是5G。按照惯例，设备商先在虚拟化平台部署4G核心网，也就是，在为后面5G做准备，提前实验。硬件平台，永远都会提前准备。上面说了5G核心网的硬件平台，接下来，我们仔细说说5G核心网的架构。到了5G，网络逻辑结构彻底改变了。</p>
<h3 id="5G核心网引入"><a href="#5G核心网引入" class="headerlink" title="5G核心网引入"></a>5G核心网引入</h3><p>5G核心网，采用的是SBA架构（Service Based Architecture，即基于服务的架构）。SBA架构，基于云原生构架设计，借鉴了IT领域的“微服务”理念，把原来具有多个功能的整体，分拆为多个具有独自功能的个体。每个个体，实现自己的微服务。这样的变化，会有一个明显的外部表现，就是网元大量增加了。</p>
<p><img src="https://s2.loli.net/2022/08/21/QTJyAWGHx68INun.jpg" alt="640?wx_fmt=png" style="zoom:50%;" /></p>
<center><small>红色虚线内为5G核心网,除了UPF之外，都是控制面</small></center>



<p>这些网元看上去很多，实际上，硬件都是在虚拟化平台里面虚拟出来的。简而言之，<strong>5G核心网就是模块化、软件化</strong>。5G核心网之所以要模块化，还有一个主要原因，就是为了<strong>“切片”</strong>。5G是一个天下一统的网络，通吃所有用户。设计之初，就需要它应对各种需求。这些需求，被整理为三大应用场景:</p>
<ul>
<li><p><strong>eMBB（增强型移动宽带）</strong></p>
<p>一般日常情况下的上网方式。</p>
</li>
<li><p><strong>mMTC（海量物联网通信）</strong></p>
<p>那堆搞物联网的应该就要搞这个。</p>
</li>
<li><p><strong>uRLLC（低时延、高可靠通信）</strong></p>
<p>用于智能无人驾驶、工业自动化等需要低时延高可靠连接的业务。</p>
</li>
</ul>
<p>既然网络用途不同，当然要见招拆招。以一个死板的固定网络结构去应对，肯定是不行的。只有拆分成模块，灵活组队，才能搞定。所以模块化的设计，有利于网络切片，根据业务需求进行模块的删改增补。</p>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>5G</tag>
        <tag>通信技术</tag>
        <tag>核心网</tag>
      </tags>
  </entry>
  <entry>
    <title>【文献翻译】使用深度强化学习游玩雅达利游戏</title>
    <url>/2022/07/29/Paper%20Translation/</url>
    <content><![CDATA[<center><h1>摘要</center>

<blockquote>
<p>我们提出首个直接通过传感器高维输入并成功地学习到策略控制的深度学习模型。这个模型的是一个卷积神经网络的结构，它使用Q-learning的变种进行训练，使用远摄关的图片作为输入，输出是预测未来收益的价值。我们将我们的方法运用至Arcade Learning环境中的雅达利2600的七个游戏里，在没有调整算法的结构的条件下，我们发现它在其中六个游戏中取得比之前方法更好的表现，它在其中三个游戏中取得了超越人类专家的表现。</p>
</blockquote>
<span id="more"></span>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>直接从像视觉和语音等高维传感输入信息来学习控制智体一直以来都是强化学习(RL)的一个挑战。大部分成功的RL在这些领域的操作应用都需要依赖人工干预的线性价的值函数或策略函数进行特征提取整合。显然，这些RL算法的表现严重依赖于所选取的特征的质量。</p>
<p>最近深度学习的优势是它能够从原生的传感器数据中提取深层次的特征，并在计算机视觉任务[11,22,16]和语言识别任务[6,7]中取得突破性进展。这些方法使用了一系列的神经网络结构，包括卷积神经网络、多层感知机，受限玻尔兹曼机器和循环神经网络，他们都在监督和无监督学习中得到了应用,这样一来似乎很自然地会思考相似的技术是否能对RL的传感器数据有所帮助。</p>
<p>然而强化学习在从深度学习的观点来看有几个挑战。首先大部分成功的深度学习案例都需要大量的人工标注的训练数据。从另一个方面来说，RL的算法必须要能从量化的奖罚信号中进行学习，而这些奖罚信号通常都是充满噪声、延迟和离散的。执行动作后的以及随之而来的奖罚可能需要等待非常久的时间，在这一点上和有监督学习中的输入与输出的直接关联相比就显得艰难非常。另一个问题是绝大多数的深度学习算法都是基于样本数据之间是独立的这一假设的，但是在强化学习中通常就是状态与状态之间紧密相连，关系复杂。并且，在RL中数据概率分布会随着算法学习到新的行为而随之改变，这对深度学习基于固定概率的条件来说无疑是个的问题。</p>
<p>本文提出了一种可以克服这些挑战以学习到成功的控制策略的卷积神经网络，而它仅需要使用复杂的RL环境中的原生视频数据。这个网络使用了一种Q-learning 的变种算法[26]进行训练，使用随机梯度下降法进行网络参数的更新。为了避免相关数据和概率不变的问题，我们使用了一种经验回放机制[13]，他能够随机地从经验池中抽取这些学习的经验，因此这能平缓训练时对先前学习到的动作分布概率。</p>
<p><img src="https://s2.loli.net/2022/05/26/6w8c9R5T7DOHEBm.gif" alt="img" style="zoom:150%;" /></p>
<p><small>图1  雅达利2600五种游戏的屏幕截图：（从左至右）Pong, Breakout, Space Invaders, Seaquest, Beam Rider</small></p>
<p>我们将我们的方法应用于在Arcade Learning Environment(ALE)[3]中一系列的雅达利2600游戏中。雅达利2600是一个充满挑战性的RL测试平台，它为智体提供了高纬度的视觉输入信息(210*160大小的RGB彩色图像，屏幕刷新率为60Hz)以及为人类设计的苦难但多样有趣的挑战任务。我们的目标是去创建一个单独的神经网络作为智体使它能够成功的学习玩尽可能多的游戏。这个网路并不会被提供任何游戏相关的信息或者人为给定的视觉特征，并且也不能直接读取游戏环境的内存数据，它只能从视觉信息、奖罚西欧马屁、游戏结束信号以及一个可选则的动作空间进行学习，就和人类玩家一样。更进一步，网络结构和所有的超参数将在训练过程中保持不变。目前网络已经在7个中的6个游戏中超越之前的所有强化学习算法的性能，并且在三个游戏中甚至还取得了超越人类专家游玩的水平。图片[1]展示了五个用于训练的游戏截图。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我们将问题转换为如下场景：智体在环境$\cal \varepsilon$即雅达利2600模拟器中，在每一个时间步中智体从一系列的合法动作集合$\cal A=\{1,…,K\}$中，选取一个动作$a_t$。这个选择的动作将会被传递至模拟器中并更改了游戏的底层交互数据和游戏得分，一般来说$\sf \varepsilon$是随机的。这些底层的交互数据对智体而言不可见，对它而言它只能从模拟器中观测到一张图片$x_t\in X^d$，这张图片其实一个原生的像素值的向量，描绘了当前的模拟器画面。除此之外，智体还会接收到一个奖罚信号$r_t$，代表了游戏的分的变化。注意一般来说游戏分数取决于之前的动作决策序列和观测；关于动作决策的评价反馈可能在决策做出之后的许多时间步之后才得到。</p>
<p>由于智体只能观测到当前屏幕的图片信息，我们的任务只能观测到片面的信息，比如说对于整个游戏当前的状态而言，仅用一张图片$x_t$是很难表述的。因此我们需要考虑动作序列和观测的序列$s_t=x_1,x_2,…,x_{t-1},x_t$，我们将基于这些序列学习游戏的策略。我们认为所有的在模拟器中迭代的序列将在有限的时间步中终止。这种规范化使得我么你的问题变成一个庞大但有限的马尔可夫决策过程（MDP），在这种情况中每一个序列是一个独立的状态。最终，我们可以在马尔科夫链中使用标准的强化学习算法，仅简单地使用完整的状态序列$s_t$作为每一个时间的表征。</p>
<p>​        智体学习的目标是通过选择一个能在未来带来最大化回报的动作，并将这个动作与模拟器交互。我们基于“未来的回报将会随着每一个时间步以一个折扣因子$\gamma$的倍率进行衰减”这一标准的设想，定义未来$t$时刻未来折扣汇报$R_t = \sum_{t’=t}^{T}\gamma^{t’-t}r_{t’}$，其中$T$是终止时的时间步。我们定义一个最优动作-价值函数$Q^{*}$，它表示为在观测一些序列$s$后根据任一策略采取一些动作$a$所期望得到的最大回报值，定义如下：$Q^{*}(s,a)=\max_\pi\Bbb E[R_t|s_t=s,a_t=a,\pi]$，其中$\pi$是一个策略，表述了一个状态到动作的映射的关系（或者状态到动作的概率分布映射）。</p>
<p>​        最优的动作价值函数遵循一个非常的性质，即贝尔曼方程。它基于如下的设想：如果某一状态的最优的动作价值$Q^{*}$在下一个时间步中成为了所有可行动作的参考，那么最优的策略就是选择动作价值最高的动作$a’$以获取最大化的期望值$r+\gamma Q^{*}(s’,a;)$，</p>
<script type="math/tex; mode=display">
Q^{*}(s,a)=\Bbb E_{s'\sim\varepsilon}[r+\gamma \max_{a'}Q^{\*}(s',a')|_{s,a}]</script><p>​        许多强化学习算法的背后逻辑就是找到用于衡量动作价值函数的方法，通过使用贝尔曼方程作为迭代更新，更新的动作价值为$Q_{i+1}(s,a)=\Bbb E[r+\gamma \max_{a’}Q_i(s’,a’)|s,a]$。这种动作价值的迭代算法会依最优的动作价值函数收敛，即当$i\rightarrow\infty$时$Q_i\rightarrow Q^{*}$[23]。在实践中，这种方法是非常不切实际的，因为动作价值函数是会依据每一个状态序列进行独立的估计且不具备泛化能力。因此，我们很自然地会想到用一个近似的函数取拟合动作价值函数，$Q(s,a;\theta)\approx Q^{*}(s,a)$。在强化学习领域常会使用一个线性的函数进行拟合，但是有时一个非线性的函数也会被用于拟合，比如说神经网络。我们参考使用了一个权重为$\theta$的神经网络作为价值评估网络——Q网络。Q网络可以通过最小化变化的每一迭代$i$过程中损失函数序列值$L_i(\theta_i)$来训练：</p>
<script type="math/tex; mode=display">
L_i(\theta_i)=\Bbb E_{s,a\sim \rho(·)}[(y_i-Q(s,a;\theta_i))^2]</script><p>​        其中$y_i=\Bbb E_{s’\sim\varepsilon}[r+\gamma \max_{a’}Q(s’,a;\theta_{i-1})-Q(s,a;\theta_i)|_{s,a}]$是迭代$i$过程中的目标值，$\rho(s,a)$是一个基于所有状态$s$和动作$a$的概率分布，我们称之为行为分布。在前一步的迭代过程中的网络参数$\theta_{i-1}$会在优化损失函数值$L_i(\theta_i)$时被冻结（即不是所有时候都是冻结的），注意到目标值是基于神经网络的参数权重的，这和我们在有监督学习中的目标值有所区别，在监督学习中这个值在学习时一般都是固定住的。我们使用如下的梯度进行损失函数的微分：</p>
<script type="math/tex; mode=display">
\nabla_{\theta_i}L_i(\theta_i)= \Bbb E_{s,a\sim\rho(·);s'\sim\varepsilon}[(r+\gamma\max_{a'}Q(s',a';\theta_{i-1})-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)]</script><p>​        比起按上述公式的梯度计算完整的期望值，我们通常使用随机梯度下降方法进行计算以优化损失。如果权重在每一个时间步根据行为分布$\rho$、模拟器环境$\varepsilon$的单个样本替换期望继续宁更新，那么我们就得到了熟悉的Q-learning算法了[26]。</p>
<p>​        注意到这是个model-free的算法，即它通过直接使用模拟器的环境$\varepsilon$抽样解决强化学习的问题，但不需要针对这个模拟器的环境$\varepsilon$进行建模和估计。同时它也是off-policy的，也就是说它会学习一个贪心的策略 $a=max_aQ(s,a;\theta)$同时也会以一定恰当的概率分布对状态空间进行探索。在实践中，我们一般使用贪心$\epsilon$策略，即使用$1-\epsilon$的概率使用决策的动作输出，以概率$\epsilon$选择一个随机的动作。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>​        也许强化学习中的最广为人知的成功故事是IBM 的 Gerald Tesauro 开发的一个玩西洋双陆棋戏的程序——TD-gammom ，也即一个通过强化学习自学如何下棋的神经网络，并获得了超越人类水平的成绩[24]。TD-gammom使用了一个model-free的和Q-learning近似的强化学习算法，使用一个多个的隐藏层的网络用于感知特征以拟合价值函数。</p>
<p>​        然而早期在尝试复现TD-gammom的时候，包括尝试在棋类游戏中使用相同的算法时，围棋和跳棋的效果就不如人意了。这就引来了一阵关于TD-gammom的方法是一个只能在双陆棋得到应用的成功特例的说法，也许是因为投骰子的随机性对状态空间的探索提供了帮助并且也使得它的价值函数尤其平缓顺滑[19]。</p>
<p>更进一步说，model-free的强化学习算法比如Q-learning使用了非线性函数进行动作价值函数的拟合[25]，也许off-policy的学习方式确实会导致Q网络的发散的情况[1]。这些争议与讨论带来的就是强化学习的主要理论工作转移到了线性函数的拟合以获取更好的收敛性和收敛的保障[25]。</p>
<p>​        最近，深度学习和强化学习的结合的趋势再次复苏。深度神经网络已经被应用于评估环境$\varepsilon$，受限玻尔兹曼机也被用于拟合价值函数[21]或者行为决策[9]。除此之外，最近梯度时分差分的方法的提出为Q-learning的发散问题带来了部分条件下的解决的方案。这些方法被证明当使用一个固定的用非线性函数拟合的策略可以保证模型的收敛[14]，或者可以使用一个受限的Q-learning的变种方法[15]用线性函数学习拟合一个控制策略，然而这些方法都没扩展至非线性的控制问题上。</p>
<p>​        也许之前和我们方法最接近的方法就是使用神经拟合Q-learning（NFQ）[20]，它使用公式2的损失函数进行优化，并且使用RPROP算法进行Q网络的参数更新。然而，它在每一个迭代计算的基于使用依据数据集大小的代价进行批量更新，因此我们认为随机梯度更新在每一个迭代过程中的大规模数据集中计算的代价将会是一个非常小的常数。NFQ同样被成功应用于简单的真实世界控制任务，而且也是使用简单的树蕨输入，首先通过自动笔爱你吗其去歇息低维度的任务表征，然后将NFQ应用于该表征中进行控制[12]。与之对比，我们的方法是一个端到端的强化学习过程，直接以视觉作为输入，结果也显示这种方法一样能通过动作价值函数的评价直接学习到特征。Q-learning在之前也被应用于结合了经验回放机制和简单的神经网络[13]中，但其输入也是基于低维度的状态而不是原始的视觉信息输入。</p>
<p>​        使用雅达利2600模拟器作为强化学习平台是参考了Marc G Bellemare[3]等的论文，他们将线性函数拟合和普通视觉信息输入应用在强化学习算法中。然后，实验的结果在后续使用更多特征和tug-of-war散列法随机映射至低维空间中的方法的改进后得到提升[2]。HyperNEAT进化架构[8]也被应用于Atari平台，用来（分别针对每款不同的游戏）形成一个表示游戏策略的神经网络。 当用模拟器的重置机制来与确定性序列做反复对抗训练时，我们发现这些策略可以利用几款Atari游戏中的设计缺陷。</p>
<h2 id="深度强化学习"><a href="#深度强化学习" class="headerlink" title="深度强化学习"></a>深度强化学习</h2><p>​        计算机视觉和语音识别领域最近取得的一些突破，靠的就是可以在大型训练集上高效地训练深度神经网络。 其中最成功的方法是通过使用基于随机梯度下降的轻量级更新，直接用原始输入进行训练。向深度神经网络输入足够多的数据，这样常常可以学习到比人工生成的特征更好的表征[11]。 这些成功案例为我们的强化学习方法提供了启发。我们的目标是将强化学习算法与深度神经网络对接起来，这里所说的神经网络可以直接学习RGB图像，并通过使用随机梯度更新来有效地处理训练数据。</p>
<p>​        Tesauro的TD-Gammon架构为这种方法提供了一个起点。该架构利用算法与环境的直接交互（或通过自玩，西洋双陆棋）产生的策略性经验样本$(s_t,a_t,r_t,s_{t+1},a_{t+1})$，对价值函数估计网络的参数进行更新。由于该算法在20年前能超越了水平最高的人类西洋双陆棋玩家，所以我们想知道，二十年的硬件改进以及现代深度神经网络架构和可扩展RL算法是否能让强化学习实现重大进展。</p>
<p>​        不同于TD-Gammon和类似的在线方法，我们使用了一种叫做“经验回放”[13]的方法：将智体在每个时间步长$e_t=(s_t,a_t,r_t,s_{t+1})$上的经验储存在数据集$\cal D=e_1,…,e_N,$中，将许多episode的经验汇集至经验池中。在算法进行内部循环时，我们将Q-learning算法更新或小批量更新应用于经验样本$\cal e\sim\cal D$，这些样本是从经验池中随机抽取的。 执行经验回放后，智体根据$\epsilon$贪心策略选择动作决策。由于用任意长度的历史表征作为神经网络的输入较难实现，所以我们的Q函数使用的是函数$\phi$生成的固定长度的历史表征。 算法1给出了完整的算法流程，我们将之称为深度Q-learning算法。</p>
<p><img src="https://s2.loli.net/2022/05/26/AoOKdq4CNsDGiPu.png" alt="image-20220526094753105"></p>
<p>​        </p>
<p>这种方法和标准的online Q-learning相比有几个优点[23]。受限，每一步的经验都可能会在许多网络权重更新中起作用，数据的有效性大幅增加。其次由于样本之间存在很强的相关性，直接学习连续样本效率很低；随机化样本会破坏这些相关性，减少更新的方差。第三，当学习策略时，当前的参数确定用于训练参数的下一数据样本。举个例子，如果最大化动作是向左移动，训练样本将由左侧的样本主导;如果最大化动作切换到右边，训练数据分布也会切换到右边。很容易看出，这样可能会产生不必要的反馈循环，并且参数也可能会被困在局部最小值，甚至发生严重的发散[25]。通过使用经验回放，在先前状态下的行为分布就会得到变得均匀，这样学习过程就会变得平缓化，并参数也不会出现振荡或发散。需要指出，当使用经验池抽取经验进行学习时off-policy的方法的学习非常有必要（因为我们当前的参数和产生经验使用的参数不同），这就启发了我们使用Q-learning。</p>
<p>​        在实践中，我们的算法仅在经验池中存储最后N个经验元组，并且在执行更新时均匀地从D中随机采样。 这种方法在某些方面有一定局限，因为存储缓冲器并不区分重要的经验；而且由于存储容量N有限，存储缓冲器总是用最新的转移重写记忆。同样，均匀采样使得回放记忆中的所有转移具有相等的重要性。 更复杂的抽样策略可能会强调可以提供最多信息的转换，类似于优先扫除[17] 。</p>
<h3 id="预处理和模型结构"><a href="#预处理和模型结构" class="headerlink" title="预处理和模型结构"></a>预处理和模型结构</h3><p>​    直接使用原始的Atari框架（128色的210×160像素图像）可能在计算上要求很高，所以我们应用了一个基本的预处理步骤来减少输入维数。进行预处理时，首先将原始帧的RGB图像表示转换成灰度图像，并将其下采样成110×84图像。通过从图像上裁剪一个可以大致捕获到游戏区域的的84×84画面，获得最终的输入表征。 然后进行最后的裁剪，因为我们使用的是Alex等提出的的2D卷积GPU实现[11]，需要方形的输入图像。在该论文的实验中，算法1的函数$\phi$将该预处理过程应用于历史记忆的最后4帧，并将它们叠加以生成Q函数的输入。</p>
<p>​    用神经网络参数化Q函数的方法有许多种。 由于Q函数可以将历史动作对映射到其Q值的标量估计上，所以先前的一些的方法可以将历史和动作作为神经网络的输入[20,12]。 这类架构的主要缺点是需要单独进行一次前向传递来计算每个动作的Q值，这样会导致计算成本与动作数呈正比。 在我们使用的架构中，其中每个可能的动作都对应一个单独的输出单元，神经网络的输入只有状态表征。 输出则对应于输入状态的单个动作的预测Q值。 这类架构的主要优点是只需进行一遍前进传递，就可以计算某一给定状态下所有可能动作的Q值。 </p>
<p>​    下面我们将描述七个Atari游戏所使用的架构。 神经网络的输入是由$\phi$产生的84×84×4图像。 第一个隐藏层用16个步长（stride）为4的8×8卷积核与输入图像进行卷积， 并使用非线性的激活函数ReLU函数[10,18]。 第二个隐层用32个步长为2的4×4卷积核进行卷积，应用同样的非线性激活函数ReLU函数。最后一个隐层为完全连接层，由256个激活函数单元组成。输出层是一个线性的完全连接层，每个有效的动作对应一个输出。在我们研究的游戏中，有效动作的数量在4到18之间变化。我们把用该方法训练的卷积网络称为深度Q网络（DQN）。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​        截至目前，我们用7款流行的ATARI游戏进行了试验——Beam Rider、Breakout、Enduro、Pong、Q*bert、Seaquest和Space Invader。在这7款游戏中，我们使用相同的网络架构、学习算法和超参数设置，以证明我们的方法还是能够在不获取特定游戏信息的条件下成功应用于多种游戏中。当在真实且未改动的游戏中对智体进行评估时，我们在训练期间只对游戏的奖励机制作出了一个改变。由于各游戏的得分范围大不相同，我们将所有正奖励都设定为1，将所有负奖励设定为-1，无变化情况设为0奖励。这样的奖励设置可以限制误差范围，便于在多种游戏中使用同一学习率。同时，该奖励机制还会影响智体的表现，因为它无法区分不同大小的奖励。</p>
<p>​        在这些试验中，我们使用的是mini batch大小为32的RMSProp算法。训练中的行为策略为：ϵ-greedy的ϵ在前100万帧从1 线性下降到0.1，然后保持在0.1不变。我们共训练了100万帧，并使用了最近100万帧的回放记忆。</p>
<p>​        在按照前文中的方法玩Atari游戏时，我们还使用了一种简单的跳帧技巧[3]。更确切地说，智体在每隔k帧而不是在每一帧观察并选择动作，在跳过的帧中则重复它的最后一个动作。由于模拟器向前运行一步需要的计算量少于智体选择一个动作的计算量，这种方法可以使智体在不大幅增加运行时间的情况下将游戏次数增加约k倍。除了Space Invader这款游戏，我们在其他游戏中都将k设为4，如果在这款游戏中将k设为4，就会看不见激光，因为跳过的帧与激光闪烁的时长相重叠。将k设定为3就可以看到激光，k值的改变就是不同游戏间的唯一超参数差异。</p>
<h3 id="训练和稳定性"><a href="#训练和稳定性" class="headerlink" title="训练和稳定性"></a>训练和稳定性</h3><p>​        在监督学习中，通过使用训练集和验证集评估模型，我们可以轻易地追踪模型在训练期间的性能。但是在强化学习中，在训练期间准确评估智体的性能可能会十分困难。如Marc G Bellemare所述[3]，我们的评估指标是在若干游戏中智体在某一episode或游戏中得到的总奖励的平均值。而且我们在训练中周期性地计算该指标。总奖励均值指标往往很嘈杂，因为权重的小小改变可能会导致策略访问的状态的分布发生很大的变化。图2中最左侧的两个线图显示了总奖励均值在游戏Seapuest和Breakout的训练期间是如何变化的。这两个总奖励均值线图确实很嘈杂，给人的印象是学习算法的运行不稳定。</p>
<p><img src="https://s2.loli.net/2022/05/26/uqRvEcmjGKDf9h5.png" alt="image-20220526095121441"></p>
<p><small>图2：  左边两图分别展示了训练时Breakout和Seaquest每回合的平均奖励值。数据是使用$\varepsilon=0.05$的$\varepsilon -greedy$策略在运行10000步计算得到的。右图的两图则分别展示了Breakout和Seaquest部分状态下的平均最大的动作价值。大约每三十分钟每一个回合约有50000条抽样数据用于更新</small></p>
<p><img src="https://s2.loli.net/2022/05/26/Qr4Mp86NOykVTxL.png" alt="image-20220526100408551"></p>
<p><small>图3  最左端的图表展示了Seaquest的30帧数据预测的价值函数。三张屏幕截图分别对应图表中的A，B，C三个时刻。</small></p>
<p>​        右侧的两个线图则较为稳定，指标是指策略的预估动作分值函数Q，该函数的作用是预测在任何给定状态下智体遵循其策略所能获得的惩罚后的奖励。我们在训练开始前运行某一随机策略，收集固定数量的状态，并追踪这些状态的最大预测Q值的均值。从图2最右侧的两个线图可以看出，预测平均Q值的增加趋势要比智体获得的总奖励的均值平缓得多，其余5个游戏的平均Q值的增长曲线也很平缓。除了预测Q值在训练期间有较为平缓的增长，我们在试验中未发现任何发散问题。这表明，除了缺乏理论上的收敛保证，我们的方法能够使用强化学习信合和随机梯度下滑以稳定的方式训练大型神经网络。</p>
<h3 id="可视化和价值函数"><a href="#可视化和价值函数" class="headerlink" title="可视化和价值函数"></a>可视化和价值函数</h3><p>​        图3给出了游戏Seaquest中学到的价值函数的可视化形式。从图中可以看出，当屏幕左侧出现敌人后预测值出现跳跃（点A）。然后代理想敌人发射鱼雷，当鱼雷快要集中敌人时预测值达到最高点（点B）。最后当敌人消失后预测值差不多恢复到原始值（点C）。图3表明我们的方法能够在较为复杂一系列的事件中学习价值函数的变化方式。</p>
<h3 id="主要的评估"><a href="#主要的评估" class="headerlink" title="主要的评估"></a>主要的评估</h3><p>​        我们将我们的结果与Marc中提出的方法进行了比较。该方法被称为“Sarsa”，Sarsa算法借助为Atari任务人工设计的多个特征集来学习线性策略，我们在[3,,4]中给出了表现最佳的特征集的得分[3]。Contingency算法的基本思路和Sarsa法相同，但是该方法可以通过学习智体控制范围内的屏幕区域的表征，来增强特征集[4]。需要指出，这两种方法都通过背景差分法吸纳了大量关于计算机视觉问题的知识，并将128种颜色中的每种颜色都作为一个单独的通道。由于许多Atari游戏中每种类型的目标所用的颜色通常都各不相同，将每种颜色作为一个单独的通道，这种做法类似于生成一个单独的二元映射，对每种目标类型进行编码。相比之下，我们的代理只接收原始RGB屏幕截图输入，并且必须学习自行检测目标。</p>
<p>​        除了给出学习代理（learned agents）的得分，我们还给出了人类专业游戏玩家的得分，以及一种均匀地随机选择动作的策略。人类玩家的表现将表示为玩了两小时游戏后得到的奖励中值。需要指出，我们给出的人类玩家的得分要比Bellemare等人[3]论文中给出的得分高得多。至于学习方法，我们遵循的是Bellemare等人的论文==[3,5]==中使用的评估策略，并且我们还通过将$\epsilon$设定为0.05运行 ε 贪心策略来获得固定步数的平均得分。表1的前5行给出了所有游戏的各游戏平均得分。在这7款游戏中，我们的方法（标记为DQN）虽然没有吸纳任何关于输入的先验知识，结果还是大幅超越了其他学习方法.。</p>
<p>​        我们同样将文献[8]中的策略搜索方法囊括进表格的后三行进行比较，并做出了两组使用该方法的结果。HNeat Best 的得分反映了通过使用人工标注的目标检测法现实游戏物体位置和类型.HNeat Pixel的得分反映的是使用特定的8种颜色通道代表雅达利游戏模拟器的特定物体类型。这两种方法依赖于确定的状态序列且不存在随机的扰动，因此与其他方法对比时对比的是单个会和下的最佳表现。相反的，我们的算法是在贪心$\epsilon$策略下进行评估的，因此必须对所有可能的情况进行归一化。然而，我们发现在所有展示的游戏中，除了Space Invaders这款游戏不单是我们算法的最优表现还是我们的平均表现都能达到一个相对较好的效果。</p>
<p><img src="https://s2.loli.net/2022/05/26/aZeD57p8KGwxCWk.png" alt="image-20220526100647570"></p>
<p><small>表1  上端的表格对比了使用贪心值为0.0.5的$\varepsilon-greedy$的不同学习方式平均总奖励。下端的表格展示了HNeat和DQN在单个回合中最佳表现分数。HNeat做出确定性策略与DQN使用$\varepsilon-greedy$的效果相同。</small></p>
<p>​        最终我们发现我们的方法在Breakout，Enduro和Pong这三款游戏中达到了超越人类专家水平的水准，并且在Beam Rider种取得了与人类水平相近的成绩。Q*bert，Seaquest，Space Invaders这三款游戏我们依然与人类专家水平相比有较大的差距，但它充满了挑战性，因为他们需要网络找到一个从更长远的时间层面上考察的策略。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>​        本文介绍了一种新的强化学习的深度学习模型，并通过在雅达利2600游戏中证明了其对困难游戏的控制能力，而它仅需要原始的像素信息作为输入。我们同时也展示了一种结合了随机批量更新和经验回放机制Q-learning的变种算法来降低训练强化学习中的深度神经网络难度。我们的方法在七个测试的游戏中的六个取得SOTA成果，并且没有调整过超参数和网络模型结构。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In <em>Proceedings of the 12th International Conference on Machine Learning (ICML 1995)</em>, pages 30–37. Morgan Kaufmann, 1995.</p>
<p>[2] Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap- proximation. In <em>Advances in Neural Information Processing Systems 25</em>, pages 2222–2230, 2012.</p>
<p>[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. <em>Journal of Artificial Intelligence</em> <em>Research</em>, 47:253–279, 2013.</p>
<p>[4] Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness using atari 2600 games. In <em>AAAI</em>, 2012.</p>
<p>[5] Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac- tored environments. In <em>Proceedings of the Thirtieth International Conference on Machine</em> <em>Learning</em> <em>(ICML</em> <em>2013)</em>, pages 1211–1219, 2013.</p>
<p>[6] George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. <em>Audio, Speech, and Language Pro-</em> <em>cessing,</em> <em>IEEE</em> <em>Transactions</em> <em>on</em>, 20(1):30 –42, January 2012.</p>
<p>[7] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep recurrent neural networks. In <em>Proc.</em> <em>ICASSP</em>, 2013.</p>
<p>[8] Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to general atari game playing. 2013.</p>
<p>[9] Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with energy-based policies. In <em>European</em> <em>Workshop</em> <em>on</em> <em>Reinforcement</em> <em>Learning</em>, page 43, 2012.</p>
<p>[10] Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In <em>Proc. International Conference on Com-</em> <em>puter</em> <em>Vision</em> <em>and</em> <em>Pattern</em> <em>Recognition</em> <em>(CVPR</em> <em>2009)</em>, pages 2146–2153. IEEE, 2009.</p>
<p>[11] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep con- volutional neural networks. In <em>Advances in Neural Information Processing Systems 25</em>, pages 1106–1114, 2012.</p>
<p>[12] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In <em>Neural Networks (IJCNN), The 2010 International Joint Conference on</em>, pages 1–8. IEEE, 2010.</p>
<p>[13] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.</p>
<p>[14] Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich Sutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi- mation. In <em>Advances</em> <em>in</em> <em>Neural</em> <em>Information</em> <em>Processing</em> <em>Systems</em> <em>22</em>, pages 1204–1212, 2009.</p>
<p>[15] Hamid Maei, Csaba Szepesva´ri, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy learning control with function approximation. In <em>Proceedings of the 27th International Con-</em> <em>ference</em> <em>on</em> <em>Machine</em> <em>Learning</em> <em>(ICML</em> <em>2010)</em>, pages 719–726, 2010.</p>
<p>[16] Volodymyr Mnih. <em>Machine Learning for Aerial Image Labeling</em>. PhD thesis, University of Toronto, 2013.</p>
<p>[17] Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less data and less real time. <em>Machine</em> <em>Learning</em>, 13:103–130, 1993.</p>
<p>[18] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann ma- chines. In <em>Proceedings of the 27th International Conference on Machine Learning (ICML</em> <em>2010)</em>, pages 807–814, 2010.</p>
<p>[19] Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In <em>Advances in Neural</em> <em>Information</em> <em>Processing</em> <em>Systems</em> <em>9</em>, pages 10–16, 1996.</p>
<p>[20] Martin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural re- inforcement learning method. In <em>Machine Learning: ECML 2005</em>, pages 317–328. Springer, 2005.</p>
<p>[21] Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.</p>
<p><em>Journal</em> <em>of</em> <em>Machine</em> <em>Learning</em> <em>Research</em>, 5:1063–1088, 2004.</p>
<p>[22] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de- tection with unsupervised multi-stage feature learning. In <em>Proc. International Conference on</em> <em>Computer</em> <em>Vision</em> <em>and</em> <em>Pattern</em> <em>Recognition</em> <em>(CVPR</em> <em>2013)</em>. IEEE, 2013.</p>
<p>[23] Richard Sutton and Andrew Barto. <em>Reinforcement Learning:</em> <em>An Introduction</em>. MIT Press, 1998.</p>
<p>[24] Gerald Tesauro. Temporal difference learning and td-gammon. <em>Communications of the ACM</em>, 38(3):58–68, 1995.</p>
<p>[25] John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. <em>Automatic</em> <em>Control,</em> <em>IEEE</em> <em>Transactions</em> <em>on</em>, 42(5):674–690, 1997.</p>
<p>[26] Christopher JCH Watkins and Peter Dayan. Q-learning. <em>Machine learning</em>, 8(3-4):279–292, 1992.</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>文献翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>关于SMMS更换国内域名导致Picgo不能上传图片的解决方法</title>
    <url>/2022/08/21/%E8%A7%A3%E5%86%B3Picgo%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p>最近（2022-8-20）在写文档，突然发现用<code>PicGo</code>上传图片一直报错，主要是<code>Error: connect ETIMEDOUT</code>和<code>Error: read ECONNRESET</code>这俩，但是我挂了梯子后用SMMS是可以直接上传图片的，所以问题出在<code>PicGo</code>上面了。查了下，估计是和SMMS更换了国内域名为<code>.app</code>的原因，这里给出我的解决方法:……</p>
<span id="more"></span>
<p>解决方法也很简单，打开<code>PicGo</code>然后点<code>PicGo设置</code>，设置下代理和镜像源就好：</p>
<p><img src="https://s2.loli.net/2022/08/21/YOnJ9T2FG3A8sEb.png" alt="image-20220821001419713" style="zoom:80%;" /></p>
<center><small>我的PicGo设置</small></center>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">上传代理:</span> <span class="string">http://127.0.0.1:7890</span></span><br><span class="line"><span class="string">插件安装代理:</span> <span class="string">http://127.0.0.1:7890</span></span><br><span class="line"><span class="string">插件镜像地址:</span> <span class="string">https://registry.npm.taobao.org/</span></span><br></pre></td></tr></table></figure>
<p>然后就点确定重启一下<code>PicGo</code>就好了~</p>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>PicGo</tag>
      </tags>
  </entry>
  <entry>
    <title>【学习记录】2022-7-22-2022-8-5</title>
    <url>/2022/08/03/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952022-7-22/</url>
    <content><![CDATA[<p>要写一个yolov5+faceNet的人脸识别，没给定数据集和模型要求，所以我按<del>最高性价比</del>最摸鱼的原则进行构建。这个任务的由三部分组成的，首先是使用<code>Yolov5</code>完成人脸的识别并裁剪，随后用<code>FaceNet</code>进行特征的提取，最后使用提取的特征通过一个<code>classifier</code>完成人脸所属对象的识别。</p>
<p>测试用的demo使用了B站Up主<a href="https://space.bilibili.com/298054634">马哥巨离谱</a>的作品：<a href="https://www.bilibili.com/video/BV1z34y167KD?spm_id_from=333.999.0.0&amp;vd_source=ab34db443b112b108b42c31ac575fd1f">穿山甲之【广东分甲】</a>，人物共有五个，人脸信息分布也很足；FaceNet作为局部的信息提取，我是使用MobileNet作为主干网络，并使用了一些预训练的权重进行训练。至于最后的分类器，我觉就只用了一个FC就完成了，效果还算不错。</p>
<span id="more"></span>
<h1 id="Yolov5"><a href="#Yolov5" class="headerlink" title="Yolov5"></a>Yolov5</h1><p><img src="https://s2.loli.net/2022/07/25/AtufwTK238Z1sPR.png" alt="yolov5s_1"></p>
<center><small>Yolov5s结构，图源知乎@江大白</small></center>

<p>Yolov5其实就是一个大锅炉，整个网络就分为Backbone、Neck、head和prediction四个部分。可以看到Yolov5用到的模块还挺多的，下面简单的分析一下每个模块。</p>
<h2 id="Backbone部分"><a href="#Backbone部分" class="headerlink" title="Backbone部分"></a>Backbone部分</h2><p>Backbone就是用来提取主要的特征的。Backbone网络一般都很大，且有人训练好了一些权重，不用我们自己从头再训练，直接到官网下载模型权重就可以了。在训练时只需Finetune一下就可以了。</p>
<h3 id="CBL模块"><a href="#CBL模块" class="headerlink" title="CBL模块"></a>CBL模块</h3><p><code>CBL</code>其实就是卷积层BN层再加一个激活函数LeakyReLU。卷积层和BN层没啥好说的了，现在成功的网络结构的基本模块都是这样搭配，值得一提的是这个LeakyReLU，区别于传统ReLU的，它增强了非线性能力。</p>
<h3 id="Res模块"><a href="#Res模块" class="headerlink" title="Res模块"></a>Res模块</h3><p>就是改进过后的残差块。Kaiming的<code>ResNet</code>出来之后<code>ResBlock</code>的使用就广泛起来了，当今的网络发展趋势就是不断的加深、加参数，ResBlock的出现就是为了解决深度加大后的梯度弥散问题，跳跃连接可以将梯度信息不那么快的消失掉。</p>
<h3 id="Focus模块"><a href="#Focus模块" class="headerlink" title="Focus模块"></a>Focus模块</h3><p>这个其实我觉得挺关键的，作者也在<a href="https://github.com/ultralytics/yolov5/discussions/3181">Issue</a>有给出设计的目的。结论就是：减少参数和计算量。</p>
<p><img src="https://s2.loli.net/2022/07/25/kBKaHcQCPdO3RmV.png" alt="image-20220725125709651"></p>
<p>首先<code>Focus</code>模块是放在图像输入后的第一个位置，它使用了4个slice对图像进行分割，具体的操作是这样子的：</p>
<p><img src="https://s2.loli.net/2022/07/25/9scCXZ3rOhavUBQ.png" alt="slice操作"></p>
<center><small>Focus模块的Slice操作</small></center>

<p>具体就是在一张图片中每隔一个像素拿到一个值，按照图中这样的Slice方法可以得到4张图片（原始图像是<code>4*4*3</code>，新图像是<code>2*2*12</code>）。这样做的目的我的理解是将图像的宽、高信息转换到了通道channel中，然后这时图像大小类似下采样虽然缩小了一倍，但它蕴含的信息量并未减少，只是信息转换到了通道中，为后续的处理预留了方便吧。为什么这样说呢，首先要明确网络的参数和计算量的概念。</p>
<p><strong>网络的参数量（parmas）决定了网络的大小</strong>，由于一般使用<code>float32</code>进行保存，因此模型大小是参数量的4倍；<strong>计算量（FLOPs）即浮点运算数</strong>，可以用来衡量算法/模型的复杂度，这关系到算法速度，大模型的单位通常为G，小模型单位通常为M；通常只考虑乘加操作的数量，而且只考虑Conv和FC等参数层的计算量，忽略BN和PReLU等，一般情况下，Conv和FC层也会忽略仅纯加操作的计算量，如bias偏置加和shoutcut残差加等，目前技术有BN和CNN可以不加bias。</p>
<p>经过<code>Focus</code>模块后的图像类似经过了下采样的操作，但是和卷积实现的下采样不同，Slice不但减少了计算量也减少了参数。作者做了一个和Yolov3对比的实验：</p>
<p><img src="https://s2.loli.net/2022/07/25/mhyW6EGT37RCYuA.png" alt="image-20220725130745790"></p>
<h3 id="CSP模块"><a href="#CSP模块" class="headerlink" title="CSP模块"></a>CSP模块</h3><p>和Yolov4不同，CSP的结构在Yolov5中有两种，分别运用在Backbone和Neck部分，而在Yolov4中只有Backbone使用了CSP结构。CSP结构就是<strong>跨阶段局部网络（Cross Stage Paritial Network）</strong>，用于缓解以往工作需要从网络架构角度进行大量推理计算的问题，CSPNet作者认为这个问题的<strong>出现原因是网络优化中的重复梯度信息。</strong></p>
<p>简单总结一下CSP结构的有点：</p>
<ul>
<li>增强CNN的学习能力，在轻量化和准确性这两方面两手抓。</li>
<li>降低计算瓶颈。</li>
<li>降低内存成本。</li>
</ul>
<h2 id="Neck部分"><a href="#Neck部分" class="headerlink" title="Neck部分"></a>Neck部分</h2><p>Neck的作用就是更好的利用Backbone提取的特征进行更精细的划分。Yolov5最新的模型是使用了特征金字塔网络(FPN,Feature Pyramid Network)+路径聚合网络(PAN，Path Aggregation Network)结构。</p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="制作数据集和配置文件"><a href="#制作数据集和配置文件" class="headerlink" title="制作数据集和配置文件"></a>制作数据集和配置文件</h3><p>首先使用<code>git clone</code>把Yolov5的代码给下载下来：</p>
<p><img src="https://s2.loli.net/2022/08/03/B9C1MgU4N7XrHdW.png" alt="image-20220727114737933"></p>
<p>然后再这个目录下新建一个文件夹，命名随意，但不用用已有的<code>data</code>。我这里用的是<code>Facedata</code>，然后在<code>Facedata</code>下面再新建两个文件夹<code>Annotations</code>和<code>images</code>，分别用于存储用<code>labelimg</code>标注的数据(<code>.xml</code>)和图片(<code>.jpg</code>)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- yolov5</span><br><span class="line">	- Facedata</span><br><span class="line">		- Annotations</span><br><span class="line">			- 1.xml</span><br><span class="line">			- 2.xml</span><br><span class="line">			- ...</span><br><span class="line">		- images</span><br><span class="line">			- 1.jpg</span><br><span class="line">			- 2.jpg</span><br><span class="line">			- ...</span><br></pre></td></tr></table></figure>
<p>然后在自己数据集的文件夹下新建一个用于分离训练集和测试集的脚本<code>split_train_test.py</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"><span class="comment">#xml文件的地址，根据自己的数据进行修改 xml一般存放在Annotations下</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--xml_path&#x27;</span>, default=<span class="string">&#x27;Annotations&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&#x27;input xml label path&#x27;</span>)</span><br><span class="line"><span class="comment">#数据集的划分，地址选择自己数据下的ImageSets/Main</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--txt_path&#x27;</span>, default=<span class="string">&#x27;ImageSets/Main&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&#x27;output txt label path&#x27;</span>)</span><br><span class="line">opt = parser.parse_args()</span><br><span class="line"></span><br><span class="line">trainval_percent = <span class="number">1.0</span>  <span class="comment"># 训练集和验证集所占比例。 这里没有划分测试集</span></span><br><span class="line">train_percent = <span class="number">0.9</span>     <span class="comment"># 训练集所占比例，可自己进行调整</span></span><br><span class="line">xmlfilepath = opt.xml_path</span><br><span class="line">txtsavepath = opt.txt_path</span><br><span class="line">total_xml = os.listdir(xmlfilepath)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(txtsavepath):</span><br><span class="line">    os.makedirs(txtsavepath)</span><br><span class="line"></span><br><span class="line">num = <span class="built_in">len</span>(total_xml)</span><br><span class="line">list_index = <span class="built_in">range</span>(num)</span><br><span class="line">tv = <span class="built_in">int</span>(num * trainval_percent)</span><br><span class="line">tr = <span class="built_in">int</span>(tv * train_percent)</span><br><span class="line">trainval = random.sample(list_index, tv)</span><br><span class="line">train = random.sample(trainval, tr)</span><br><span class="line"></span><br><span class="line">file_trainval = <span class="built_in">open</span>(txtsavepath + <span class="string">&#x27;/trainval.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">file_test = <span class="built_in">open</span>(txtsavepath + <span class="string">&#x27;/test.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">file_train = <span class="built_in">open</span>(txtsavepath + <span class="string">&#x27;/train.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">file_val = <span class="built_in">open</span>(txtsavepath + <span class="string">&#x27;/val.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list_index:</span><br><span class="line">    name = total_xml[i][:-<span class="number">4</span>] + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">in</span> trainval:</span><br><span class="line">        file_trainval.write(name)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> train:</span><br><span class="line">            file_train.write(name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            file_val.write(name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        file_test.write(name)</span><br><span class="line"></span><br><span class="line">file_trainval.close()</span><br><span class="line">file_train.close()</span><br><span class="line">file_val.close()</span><br><span class="line">file_test.close()</span><br></pre></td></tr></table></figure>
<p>然后同样是在这个目录下新建<code>xml_to_yolo.py</code>文件，这个主要是用来转换yolov5训练的格式(<code>.txt</code>)，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"></span><br><span class="line">sets = [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">classes = [<span class="string">&quot;face&quot;</span>]   <span class="comment"># 改成自己的类别</span></span><br><span class="line">abs_path = os.getcwd()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">size, box</span>):</span><br><span class="line">    dw = <span class="number">1.</span> / (size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span> / (size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>]) / <span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>]) / <span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x * dw</span><br><span class="line">    w = w * dw</span><br><span class="line">    y = y * dh</span><br><span class="line">    h = h * dh</span><br><span class="line">    <span class="keyword">return</span> x, y, w, h</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_annotation</span>(<span class="params">image_id</span>):</span><br><span class="line">    in_file = <span class="built_in">open</span>(<span class="string">&#x27;yolov5\\Facedata\\Annotations\\%s.xml&#x27;</span> % (image_id), encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    out_file = <span class="built_in">open</span>(<span class="string">&#x27;yolov5\\Facedata\\labels\\%s.txt&#x27;</span> % (image_id), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">    w = <span class="built_in">int</span>(size.find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">    h = <span class="built_in">int</span>(size.find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.<span class="built_in">iter</span>(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">        difficult = obj.find(<span class="string">&#x27;difficult&#x27;</span>).text</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#difficult = obj.find(&#x27;Difficult&#x27;).text</span></span><br><span class="line">        cls = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> <span class="built_in">int</span>(difficult) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">        b = (<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmax&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymin&#x27;</span>).text),</span><br><span class="line">             <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))</span><br><span class="line">        b1, b2, b3, b4 = b</span><br><span class="line">        <span class="comment"># 标注越界修正</span></span><br><span class="line">        <span class="keyword">if</span> b2 &gt; w:</span><br><span class="line">            b2 = w</span><br><span class="line">        <span class="keyword">if</span> b4 &gt; h:</span><br><span class="line">            b4 = h</span><br><span class="line">        b = (b1, b2, b3, b4)</span><br><span class="line">        bb = convert((w, h), b)</span><br><span class="line">        </span><br><span class="line">        out_file.write(<span class="built_in">str</span>(cls_id) + <span class="string">&quot; &quot;</span> + <span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">wd = getcwd()</span><br><span class="line"><span class="keyword">for</span> image_set <span class="keyword">in</span> sets:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;yolov5\\Facedata\\labels&#x27;</span>):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;yolov5\\Facedata\\labels&#x27;</span>)</span><br><span class="line">    image_ids = <span class="built_in">open</span>(<span class="string">&#x27;yolov5\\Facedata\\ImageSets\\Main\\%s.txt&#x27;</span> % (image_set)).read().strip().split()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;yolov5\\Facedata\\dataSet_path&#x27;</span>):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;yolov5\\Facedata\\dataSet_path&#x27;</span>)</span><br><span class="line">     </span><br><span class="line">    list_file = <span class="built_in">open</span>(<span class="string">&#x27;yolov5\\Facedata\\dataSet_path\\%s.txt&#x27;</span> % (image_set), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">        list_file.write(<span class="string">&#x27;yolov5\\Facedata\\images\\%s.jpg\n&#x27;</span> % (image_id))</span><br><span class="line">        convert_annotation(image_id)</span><br><span class="line">    list_file.close()</span><br></pre></td></tr></table></figure>
<p>然后到Yolov5文件夹下的data文件夹新建自己的数据库索引文件<code>Facedata.yaml</code>:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">train:</span> <span class="string">F:/Programme/Demo/FaceDetection/yolov5/Facedata/dataSet_path/train.txt</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">F:/Programme/Demo/FaceDetection/yolov5/Facedata/dataSet_path/val.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># number of classes</span></span><br><span class="line"><span class="attr">nc:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># class names</span></span><br><span class="line"><span class="attr">names:</span> [<span class="string">&quot;face&quot;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后在<code>model</code>文件夹中选择使用的模型，我选的是<code>Yolov5s.yaml</code>，然后修改一下nc对应的数量就可以了。</p>
<h3 id="正式训练"><a href="#正式训练" class="headerlink" title="正式训练"></a>正式训练</h3><p>正式训练是在这个<code>train.py</code>的文件里。打开看一下可以看到很多的参数：</p>
<p><img src="https://s2.loli.net/2022/07/27/jIfv8VZgLep4OHt.png" alt="image-20220727170725337"></p>
<p>主要是修改以下几个东西：</p>
<ul>
<li><code>--weights</code>：初始权重，要和模型对应，脚本会从官网下载预训练好的权重。</li>
<li><code>--data</code>：训练数据配置文件<code>xxx.yaml</code>，存放在Yolov5文件夹下的<code>data</code>文件夹中</li>
<li><code>--epochs</code>：训练轮数，一般来说小数据（不到一千张）100轮左右就有结果了。</li>
<li><code>--batch-size</code>：取决于设备内存大小，我设置为16。</li>
<li><code>device</code>：用于训练的设备，一般个人单显卡且有CUDA环境支持的话就设成0就好。</li>
<li><code>workers</code>：似乎是CPU核心控制的线程数量，电脑配置低一点的就设成0或者1就好。</li>
</ul>
<h3 id="使用训练后的模型"><a href="#使用训练后的模型" class="headerlink" title="使用训练后的模型"></a>使用训练后的模型</h3><p>测试模型的话，在yolov5的文件夹下方有一个<code>detect.py</code>的文件，按照代码的说明修改程序就可以测试了。测试的结果是放在<code>runs/detect/expx/</code>的文件夹中。</p>
<h3 id="如何部署自定义模型"><a href="#如何部署自定义模型" class="headerlink" title="如何部署自定义模型"></a>如何部署自定义模型</h3><p>如果图省事的话，可以用<code>pytorchhub</code>直接加载集成好的环境，具体的操作可以见<a href="https://github.com/ultralytics/yolov5/issues/36">作者写的Documentation</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">model = torch.hub.load(<span class="string">&#x27;yolov5&#x27;</span>, <span class="string">&#x27;custom&#x27;</span>, path=<span class="string">&#x27;best&#x27;</span>, source=<span class="string">&#x27;local&#x27;</span>) </span><br><span class="line"><span class="comment"># Image</span></span><br><span class="line">img = <span class="string">r&#x27;dataset\video\VideoCapture\9.jpg&#x27;</span></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">results = model(img)</span><br><span class="line"><span class="built_in">print</span>(results.pandas().xyxy[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#         xmin        ymin        xmax        ymax  confidence  class  name</span></span><br><span class="line"><span class="comment">#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face</span></span><br></pre></td></tr></table></figure>
<p>甚至可以使用<code>ndarrays</code>作为输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">model = torch.hub.load(<span class="string">&#x27;yolov5&#x27;</span>, <span class="string">&#x27;custom&#x27;</span>, path=<span class="string">&#x27;best&#x27;</span>, source=<span class="string">&#x27;local&#x27;</span>) </span><br><span class="line"><span class="comment"># Image</span></span><br><span class="line">img = <span class="string">r&#x27;F:\Programme\Demo\FaceDetection\dataset\video\VideoCapture\9.jpg&#x27;</span></span><br><span class="line">img = cv2.imread(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))</span><br><span class="line"><span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">results = model(img)</span><br><span class="line"><span class="built_in">print</span>(results.pandas().xyxy[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#         xmin        ymin        xmax        ymax  confidence  class  name</span></span><br><span class="line"><span class="comment">#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face</span></span><br></pre></td></tr></table></figure>
<p>但如果想进一步拓展Yolov5的输入端性能的话，<code>Yolov5</code>的作者把跟<code>Pytorchhub</code>加载的模型相关的内容放在了<code>models/common.py</code>里面，它的核心功能靠两个类<code>AutoShape</code>和<code>Detections</code>完成，其中<code>AutoShape</code>完成了模型的核心，<code>Detections</code>完成结果的展示以及渲染。用好这两个类足够完成大部分的输入任务了。</p>
<h2 id="用Yolov5的预测结果获取人脸数据的训练集"><a href="#用Yolov5的预测结果获取人脸数据的训练集" class="headerlink" title="用Yolov5的预测结果获取人脸数据的训练集"></a>用Yolov5的预测结果获取人脸数据的训练集</h2><p>观察到模型输出的信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#         xmin        ymin        xmax        ymax  confidence  class  name</span></span><br><span class="line"><span class="comment">#0  714.960083  141.053375  932.943726  412.439209    0.924156      0  face</span></span><br></pre></td></tr></table></figure>
<p>是使用<code>pandas.DataFrame</code>类表示的，且内取值为浮点数。因此需要对数据进行抽取排序以及取整处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">crop_images</span>(<span class="params">file_path, file_name, save_dir</span>):</span><br><span class="line">    <span class="comment"># file_path: 文件路径</span></span><br><span class="line">    <span class="comment"># file_name: 文件名称</span></span><br><span class="line">    <span class="comment"># save_dir:  保存路径</span></span><br><span class="line">    img = cv2.imread(file_path)</span><br><span class="line">    results = model(img)</span><br><span class="line">    df = results.pandas().xyxy[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">        xmin, ymin, xmax, ymax = <span class="built_in">int</span>(row[<span class="string">&#x27;xmin&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;ymin&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;xmax&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;ymax&#x27;</span>])</span><br><span class="line">        crop = img[ymin : ymax, xmin : xmax]</span><br><span class="line">        cv2.imwrite(<span class="string">r&#x27;&#123;&#125;\&#123;&#125;-&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(save_dir, file_name[:-<span class="number">4</span>], index), crop)   <span class="comment"># index是图像识别中的第x个对象</span></span><br></pre></td></tr></table></figure>
<p>随后使用模型裁剪人脸数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Easy way to use customed yolov5 model to predict</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">model = torch.hub.load(<span class="string">&#x27;yolov5&#x27;</span>, <span class="string">&#x27;custom&#x27;</span>, path=<span class="string">&#x27;best&#x27;</span>, source=<span class="string">&#x27;local&#x27;</span>) </span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crop_images</span>(<span class="params">file_path, file_name, save_dir</span>):</span><br><span class="line">    <span class="comment"># file_path: 文件路径</span></span><br><span class="line">    <span class="comment"># file_name: 文件名称</span></span><br><span class="line">    <span class="comment"># save_dir:  保存路径</span></span><br><span class="line">    img = cv2.imread(file_path)</span><br><span class="line">    results = model(img)</span><br><span class="line">    df = results.pandas().xyxy[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">        xmin, ymin, xmax, ymax = <span class="built_in">int</span>(row[<span class="string">&#x27;xmin&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;ymin&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;xmax&#x27;</span>]), <span class="built_in">int</span>(row[<span class="string">&#x27;ymax&#x27;</span>])</span><br><span class="line">        crop = img[ymin : ymax, xmin : xmax]</span><br><span class="line">        cv2.imwrite(<span class="string">r&#x27;&#123;&#125;\&#123;&#125;-&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(save_dir, file_name[:-<span class="number">4</span>], index), crop)   <span class="comment"># index是图像识别中的第x个对象</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">path = <span class="string">r&#x27;dataset\raw&#x27;</span></span><br><span class="line">files= os.listdir(path) <span class="comment">#得到文件夹下的所有文件名称</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> files:</span><br><span class="line">    crop_images(<span class="string">r&#x27;dataset\raw\&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(each), each, save_dir=<span class="string">r&#x27;dataset\train&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;file &#123;&#125; has been crop.&#x27;</span>.<span class="built_in">format</span>(each))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>得到后人工对人脸进行分类放入不同类别的文件夹中。</p>
<h1 id="FaceNet"><a href="#FaceNet" class="headerlink" title="FaceNet+"></a>FaceNet+</h1><p>论文链接：<a href="https://arxiv.org/abs/1503.03832">[1503.03832] FaceNet: A Unified Embedding for Face Recognition and Clustering (arxiv.org)</a>。FaceNet是谷歌提出的，特点是CNN的端到端训练、Triplet loss 以及 特征欧几里得距离，模型大小仅有128Bytes，达到了当时的SOTA水平。论文使用的模型如下：</p>
<p><img src="https://s2.loli.net/2022/08/02/bWgmUj4fKGweoMr.png" alt="image-20220802190434704" style="zoom: 80%;" /></p>
<h2 id="网络部分"><a href="#网络部分" class="headerlink" title="网络部分"></a>网络部分</h2><p>论文当时的Backbone使用的是<code>Inception-net</code>的结构，然后后续为了更加轻便的部署我又更换成了<code>MobileNet-v1</code>的结构。下面的内容主要针对的是<code>MobileNet-V1</code>。</p>
<h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><p>首先<code>MobileNet</code>也是谷歌提出来的，它的核心组件是一种叫做<code>“深度可分离卷积块”</code>的结构组成的，其结构如下：</p>
<p><img src="https://s2.loli.net/2022/08/02/WAr5wIh4ZbStVuF.jpg" alt="img"></p>
<p>其具体的解析在<a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">这篇文章</a>有。深度可分离卷积块由两部分组成，分别为<code>Depthwise Convolutional Filters</code>和<code>Pointwise Convolutional Filters</code>。深度可分离卷积块的目的是使用更少的参数来代替普通的卷积。下面给出普通卷积和可分离卷积的对比：</p>
<p><img src="https://s2.loli.net/2022/08/02/ChxozZ3PRFp2nyO.png" alt="img" style="zoom: 33%;" /></p>
<center><small>普通卷积，对于12*12*3->8*8*256的映射需要256个[5,5,3]的卷积核，<br>参数量为256 x 5 x 5 x 3 = 19200</small></center>

<p><img src="https://s2.loli.net/2022/08/02/LKJy9svTaERXUZS.png" alt="img" style="zoom: 33%;" /></p>
<p><img src="https://s2.loli.net/2022/08/02/quI9WnQZ4kzRDKo.png" alt="img" style="zoom: 33%;" /></p>
<center><small>可分离卷积，对于12*12*3->8*8*256的映射需要3个[5,5,1]的卷积核(Depthwise)和256个[1,1,3]的卷积核(Pointwise)<br>参数量为3 x 5 x 5 x 1 + 256 x 1 x 1 x 3 = 843</small></center>

<p>可以看出来深度可分离卷积结构块可以减少模型的参数。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>网络结构由Backbone+Classifier两个部分组成。Backbone前文也提及到使用的是<code>MobileNet</code>，而<code>Classifier</code>则是一个多层感知机。师兄给了一篇参考的博客给我看，他的<code>Classifier</code>用的是<code>SVM</code>，我能理解这样做的目的，舍弃了端到端的直观过程，让神经网络的黑盒少了最后那部分的可解释性差的问题。但是这样就很麻烦，部署起来很费时间，所以我最终还是选择了用MLP作为分类器，还有一个原因我会放在后面的训练技巧部分那里说。</p>
<p>Backbone部分的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_bn</span>(<span class="params">inp, oup, stride = <span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(inp, oup, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(oup),</span><br><span class="line">        nn.ReLU6()</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_dw</span>(<span class="params">inp, oup, stride = <span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(inp, inp, <span class="number">3</span>, stride, <span class="number">1</span>, groups=inp, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(inp),</span><br><span class="line">        nn.ReLU6(),</span><br><span class="line"></span><br><span class="line">        nn.Conv2d(inp, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(oup),</span><br><span class="line">        nn.ReLU6(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV1, self).__init__()</span><br><span class="line">        self.stage1 = nn.Sequential(</span><br><span class="line">            <span class="comment"># 160,160,3 -&gt; 80,80,32</span></span><br><span class="line">            conv_bn(<span class="number">3</span>, <span class="number">32</span>, <span class="number">2</span>), </span><br><span class="line">            <span class="comment"># 80,80,32 -&gt; 80,80,64</span></span><br><span class="line">            conv_dw(<span class="number">32</span>, <span class="number">64</span>, <span class="number">1</span>), </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 80,80,64 -&gt; 40,40,128</span></span><br><span class="line">            conv_dw(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">128</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 40,40,128 -&gt; 20,20,256</span></span><br><span class="line">            conv_dw(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">256</span>, <span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line">        self.stage2 = nn.Sequential(</span><br><span class="line">            <span class="comment"># 20,20,256 -&gt; 10,10,512</span></span><br><span class="line">            conv_dw(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line">        self.stage3 = nn.Sequential(</span><br><span class="line">            <span class="comment"># 10,10,512 -&gt; 5,5,1024</span></span><br><span class="line">            conv_dw(<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2</span>),</span><br><span class="line">            conv_dw(<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.avg = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.stage1(x)</span><br><span class="line">        x = self.stage2(x)</span><br><span class="line">        x = self.stage3(x)</span><br><span class="line">        x = self.avg(x)</span><br><span class="line">        <span class="comment"># x = self.model(x)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">1024</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>FaceNet的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Facenet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone=<span class="string">&quot;mobilenet&quot;</span>, dropout_keep_prob=<span class="number">0.5</span>, embedding_size=<span class="number">128</span>, num_classes=<span class="literal">None</span>, mode=<span class="string">&quot;train&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Facenet, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> backbone == <span class="string">&quot;mobilenet&quot;</span>:</span><br><span class="line">            self.backbone = mobilenet()</span><br><span class="line">            flat_shape = <span class="number">1024</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Unsupported backbone - `&#123;&#125;`, Use mobilenet.&#x27;</span>.<span class="built_in">format</span>(backbone))</span><br><span class="line">        self.avg = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.Dropout = nn.Dropout(<span class="number">1</span> - dropout_keep_prob)</span><br><span class="line">        self.Bottleneck = nn.Linear(flat_shape, embedding_size,bias=<span class="literal">False</span>)</span><br><span class="line">        self.last_bn = nn.BatchNorm1d(embedding_size, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.classifier = nn.Linear(embedding_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.backbone(x)</span><br><span class="line">        x = self.avg(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.Dropout(x)</span><br><span class="line">        x = self.Bottleneck(x)</span><br><span class="line">        x = self.last_bn(x)</span><br><span class="line">        x = F.normalize(x, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_feature</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.backbone(x)</span><br><span class="line">        x = self.avg(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.Dropout(x)</span><br><span class="line">        x = self.Bottleneck(x)</span><br><span class="line">        before_normalize = self.last_bn(x)</span><br><span class="line">        x = F.normalize(before_normalize, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> before_normalize, x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_classifier</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Loss部分"><a href="#Loss部分" class="headerlink" title="Loss部分"></a>Loss部分</h2><p>FaceNet使用了<code>Triplet Loss</code>。这个Loss的目的是：对于一张人脸图片<code>Anchor</code>输入$x_i^a$，通过神经网络$f(x)$的映射到$d$维特征空间$\Bbb R^d$的结果$f(x_i^a)$和所有的正样本<code>Positive</code>的结果(和Anchor输入的类别相同的所有照片的$d$维特征空间结果$f(x_i^p)$)的欧式距离最短，而与所有的负样本<code>Negative</code>（和Anchor输入的类别不同的所有照片的$d$维特征空间结果$f(x_i^n)$）的欧氏距离最长，如图所示：</p>
<p><img src="https://s2.loli.net/2022/08/02/OYi8dzawNkR26Vf.png" alt="image-20220802195743710" style="zoom:80%;" /></p>
<p>用公式表示一下这个<strong>约束条件</strong>就是：</p>
<script type="math/tex; mode=display">
||f(x_i^a)-f(x_i^p)||_2^2+\alpha<||f(x_i^a)-f(x_i^n)||_2^2,\\
\forall (f(x_i^a),f(x_i^p),f(x_i^n))\in\tau</script><p>其中，$\alpha$是规定的正负样本的边界距离，$\tau$代表的是三元组的每一组基组成的$N$维空间。对于这种情况，网络的优化目标就是最小化这个式子：</p>
<script type="math/tex; mode=display">
\sum_i^N{[||f(x_i^a)-f(x_i^p)||_2^2+\alpha-||f(x_i^a)-f(x_i^n)||_2^2]}_+</script><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="三元组Triplet的选择"><a href="#三元组Triplet的选择" class="headerlink" title="三元组Triplet的选择"></a>三元组Triplet的选择</h3><p>上文提到，把所有可能的三元组构建出来是非常耗时的，而且大部分的式子都能满足<strong>约束条件</strong>，而神经网络是一个基于差别进行反馈修正的系统，因此较小的Loss其实对训练的结果修改并没有很大的意义。举个例子，备战高考，在做往年题的时候发现自己的立体几何做得很强，但是导数题做得一塌糊涂漏洞百出。这个时候就需要专攻薄弱环节导数题进行训练，因为薄弱环节的提分可以最大程度地拉升和满分之间的差距（减小Loss，提高Accuracy）。基于这种想法，我们直接选择那些违背了约束条件的三元组进行训练即可。（但在实际的训练中，我将直接选择更改为了随机选择，因为考虑到模型的泛化性能我对违背约束条件的三元组的选择是以0.2的概率进行抽样送往网络训练）</p>
<h3 id="分类器的辅助训练"><a href="#分类器的辅助训练" class="headerlink" title="分类器的辅助训练"></a>分类器的辅助训练</h3><p>FaceNet输出的其实是要给长度为128的特征向量，下一步的分类器根据特征向量分类才输出最终的分类结果。仅仅使用Triplet Loss会难以让网络收敛，因此在训练过程中可以再加上一个MLP作为训练器，使用Cross-Entropy作为辅助Triplet-Loss收敛。这样做的话其实就已经自带了一个<code>Classifier</code>了，不再需要额外的训练。这就是为什么之前提到不使用SVM等等一类的分类算法进行最后的分类器。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><ul>
<li><p><strong>随机水平翻转。</strong>给定正态分布$X\sim \delta(0,1)$，对于随机变量$x$，当其取值小于0.5时，将输入的图像进行左右的翻转。在<code>opencv</code>中仅需要<code>cv2.flip(image,1)</code>即可完成操作。</p>
</li>
<li><p><strong>灰度边框+等比缩放。</strong>首先获取输入图片的原始尺寸，计算其长宽比$r_1$；然后根据长宽中的最大值和MobileNet的输入尺寸<code>160*160</code>进行比较，得到缩放倍率$d_{wh}=\frac{160}{\max(w,h)}$，随后根据长宽比和缩放倍率对原始图像进行缩放，使得长宽中的一边达到160；随后，针对不足160的部分使用灰色<code>RGB:(128,128,128)</code>矩形从两端均匀padding。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">letterbox</span>(<span class="params">im, new_shape=(<span class="params"><span class="number">160</span>, <span class="number">160</span></span>), color=(<span class="params"><span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span></span>), auto=<span class="literal">False</span>, scaleFill=<span class="literal">False</span>, scaleup=<span class="literal">True</span>, stride=<span class="number">32</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Resize and pad image while meeting stride-multiple constraints</span></span><br><span class="line">    shape = im.shape[:<span class="number">2</span>]  <span class="comment"># current shape [height, width]</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(new_shape, <span class="built_in">int</span>):</span><br><span class="line">        new_shape = (new_shape, new_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scale ratio (new / old)</span></span><br><span class="line">    r = <span class="built_in">min</span>(new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>], new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> scaleup:  <span class="comment"># only scale down, do not scale up (for better val mAP)</span></span><br><span class="line">        r = <span class="built_in">min</span>(r, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute padding</span></span><br><span class="line">    ratio = r, r  <span class="comment"># width, height ratios</span></span><br><span class="line">    new_unpad = <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">1</span>] * r)), <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">0</span>] * r))</span><br><span class="line">    dw, dh = new_shape[<span class="number">1</span>] - new_unpad[<span class="number">0</span>], new_shape[<span class="number">0</span>] - new_unpad[<span class="number">1</span>]  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">if</span> auto:  <span class="comment"># minimum rectangle</span></span><br><span class="line">        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  <span class="comment"># wh padding</span></span><br><span class="line">    <span class="keyword">elif</span> scaleFill:  <span class="comment"># stretch</span></span><br><span class="line">        dw, dh = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        new_unpad = (new_shape[<span class="number">1</span>], new_shape[<span class="number">0</span>])</span><br><span class="line">        ratio = new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>], new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>]  <span class="comment"># width, height ratios</span></span><br><span class="line"></span><br><span class="line">    dw /= <span class="number">2</span>  <span class="comment"># divide padding into 2 sides</span></span><br><span class="line">    dh /= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shape[::-<span class="number">1</span>] != new_unpad:  <span class="comment"># resize</span></span><br><span class="line">        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)</span><br><span class="line">    top, bottom = <span class="built_in">int</span>(<span class="built_in">round</span>(dh - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dh + <span class="number">0.1</span>))</span><br><span class="line">    left, right = <span class="built_in">int</span>(<span class="built_in">round</span>(dw - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dw + <span class="number">0.1</span>))</span><br><span class="line">    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  <span class="comment"># add border</span></span><br><span class="line">    <span class="keyword">return</span> im<span class="comment"># , ratio, (dw, dh)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="整合收尾"><a href="#整合收尾" class="headerlink" title="整合收尾"></a>整合收尾</h1><p>终于到这里了！到了我第一喜欢的UI设计环节！！！</p>
<p><img src="https://s2.loli.net/2022/08/05/ldiUFYzgjBrqyQN.png" alt="image-20220805181658926"  /></p>
<center><small>其实也没啥设计，就是选了几个颜色而已</small></center>

<h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>Yolo推理速度直接搬运官网的0.01s左右，<code>FaceNet</code>推理时间为0.003秒左右。这里其实有个坑，就是一开始测试的时候发现时间差异太大了，然后查了一下发现好像是在部署到设备上进行推理的时候，需要使用一些数据进行<code>Warmup</code>的操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">dummy_input = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">160</span>,<span class="number">160</span>).to(device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Warming Up&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        _ = model(dummy_input)</span><br></pre></td></tr></table></figure>
<p>然后进行性能的测试。首先是推理时间：</p>
<p><img src="https://s2.loli.net/2022/08/07/JsfkZiAtdFRXL3P.png" alt="image-20220807092856305"></p>
<p>基本上可以做到100FPS的水平，也就是说，基本上实时检测是没问题的了。那么准确度如何呢？我使用了两个验证集进行测试，其中一个验证集是训练集数据来源的视频的下半部分（这部分没有任何数据用于训练、测试），另一个则是完全独立的新视频。先说几个存在的问题吧，首先无论是在哪个验证集上面进行效果测试，脸部画面占据画面20-50%的情况下，推理准确度为100%。然而在人脸屏占比为10~15%左右的时候，会出现分类抖动的情况，就是说分类结果会很不稳定，出现的原因可能是<code>FaceNet</code>要求的输入图像大小为<code>160*160</code>导致的，加上Resize函数和LetterBox的操作会导致一些特征的变换有所丢失。解决方法则是对对所有的小尺寸图片划分为子类，并固定Backbone参数重新训练FaceNet以及分类器，最后导出结果的时候再把子类合并到大类里面。</p>
<h2 id="探索实验"><a href="#探索实验" class="headerlink" title="探索实验"></a>探索实验</h2><p>然后这里我还额外做了个实验，研究模型的卷积层的关注点，对重点区域生成热力图，观察到底是什么特征区分了人脸：</p>
<p><img src="https://s2.loli.net/2022/08/05/qnpZ3vXkxS9uG78.png" alt="image-20220805183241174" style="zoom: 50%;" /></p>
<center><small>Grad-CAM钩在Backbone最后的卷积层上生成的热力图</small></center>

<p>可以看出网络主要是重点关注鼻子部分的信息进行分类决策的。就这个简单的分类任务而言基本上准确率处于一个可以接受的范围，但是就更广泛一点的人脸识别任务来说，这个网络的设计其实还是有缺陷的，因为我们希望的是网络<strong>更加关注五官的信息而不是针对数据集的差异关注某一局部的信息。</strong>所以如果要构建更通用的人脸识别系统的话，可以参考目标识别设定中的预设描框对人脸的五官局部进行裁剪或者滑动窗口+注意力机制综合运用，对局部信息进行特征向量的提取与embedding，甚至还可以降低特征向量的维度（FaceNet的输出特征向量维度是128），在我调参的过程中我也研究过<code>Embedding Size</code>的大小设置，针对这次使用的数据集而言，128的长度其实对5个类别来说其实有些冗余了，可以进行一定的缩减；其次，五官信息的特征可以以组合的形式输出一个新的特征向量，最后只需要对这个新的特征向量做分类工作即可。我更改了网络的主干部分，使用并修改了Swim Transformer的结构，但碍于设备性能未能进行实验探索，日后有机会的话可能会从这方面进行进一步的探索工作。</p>
]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Yolov5</tag>
        <tag>FaceNet</tag>
      </tags>
  </entry>
  <entry>
    <title>通信名词解释</title>
    <url>/2020/01/01/%E9%80%9A%E4%BF%A1%20%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</url>
    <content><![CDATA[<p>本文主要用于记录常见的通信专业的专业术语，并尝试记录其简易的意义与功能，配合博客的本地搜索功能以达到查阅的目的。<span id="more"></span></p>
<h2 id="LTE"><a href="#LTE" class="headerlink" title="LTE"></a>LTE</h2><p>LTE即(Long Term Evolution)长期演化，它其实是一种网络制式（如CDMA、GSM这些）它是3G的演进，但<strong>并非人们普遍误解的4G技术</strong>，而是3G与4G技术之间的一个<strong>过渡</strong>，是3.9G的全球标准,它改进并增强了3G的空中接入技术，采用OFDM和MIMO作为其无线网络演进的唯一标准。在20MHz频谱带宽下能够提供下行326Mbit/s与上行86Mbit/s的峰值速率。改善了小区边缘用户的性能，提高小区容量和降低系统延迟。严格意义上，其实LTE一开始的定位仅仅是3G的加强版，但是随着后期的发展，大大超出当初设计者的预期。本来只能称做3.9G，但是由于不断的在继续改善升级，所以后续版本已经成为了真正的4G。</p>
<h2 id="OFDM"><a href="#OFDM" class="headerlink" title="OFDM"></a>OFDM</h2><p>OFDM即Orthogonal Frequency Division Multiplexing，正交频分复用，属于多载波调制的一种。OFDM技术是多载波传输方案的实现方式之一，通过<a href="https://baike.baidu.com/item/频分复用/7626706">频分复用</a>实现高速串行数据的<a href="https://baike.baidu.com/item/并行传输/5431953">并行传输</a>, 它具有较好的抗多径衰落的能力，能够支持多用户接入，它的调制和解调是分别基于IFFT和FFT来实现的，是实现复杂度最低、应用最广的一种多载波传输方案。</p>
<h2 id="MIMO"><a href="#MIMO" class="headerlink" title="MIMO"></a>MIMO</h2><p>Multiple-In Multiple-Out多进多出，使用多根天线在发送端和接受端构建多个像互不干扰的物理无线信道，同时在收发双方的物理层之间，构建多个并行的<strong>逻辑</strong>无线信道。MIMO系统的一个明显特点就是具有极高的频谱利用效率，在对现有频谱资源充分利用的基础上通过利用空间资源来获取可靠性与有效性两方面增益，<strong>其代价是增加了发送端与接收端的处理复杂度。</strong>详细介绍可见<a href="https://blog.csdn.net/HiWangWenBing/article/details/110871535">这篇文章</a>。</p>
<h2 id="NR"><a href="#NR" class="headerlink" title="NR"></a>NR</h2><p>手机和基站之间的接口，因为是基站和手机之间通过电磁波在空气中传播的，因此就叫做“空口”。由此可见，空口这套东西专用于基站子系统内部，跟核心网的没有直接联系。</p>
<p><img src="https://s2.loli.net/2022/07/11/IYReFBAspcuwLdU.jpg" alt=""></p>
<p>5G使用了新空口，即基站跟核心网这两个子系统的独立性增强了。因此，5G是非常灵活的，可以独立组网，也可以跟4G一起非独立组网。在独立组网时，5G基站连接的是5G核心网；非独立组网时，5G基站和4G基站可以连接4G核心网，也可以连接5G核心网。下图是一张非独立组网选项3和选项7的架构图。其中，4G基站和5G基站都连接到了同一套核心网，共同组成了5G非独立组网体系。</p>
<p><img src="https://s2.loli.net/2022/07/11/nxOfzviYrGQlWD6.jpg" alt=""></p>
<p>这样一来，不论连接的核心网是4G的EPC还是5G的5GC，同一部手机拥有两路空口连接，跟4G基站之间的链路就是“旧空口”，跟5G基站之间的连接就是所谓的“新空口”了。5G新空口是基于4G空口技术设计的，也使用了OFDM的调制方式，在帧结构上修正了4G的一些不合理之处，增加了对大连接和低时延的支持，因此更加灵活，频谱效率也更高了。</p>
<h2 id="NG"><a href="#NG" class="headerlink" title="NG"></a>NG</h2><p><strong>简单的解释就是无线接入网和5G核心网之间的接口。</strong>NG接口是一个逻辑接口，规范了NG接口，NG-RAN节点与不同制造商提供的AMF的互连；同时，分离NG接口无线网络功能和传输网络功能，以便于引入未来的技术。从任何一个NG-RAN节点向5GC可能存在多个NG-C逻辑接口。然后，通过NAS节点选择功能确定NG-C接口的选择。从任何一个NG-RAN节点向5GC可能存在多个NG-U逻辑接口。NG-U接口的选择在5GC内完成，并由AMF发信号通知NG-RAN节点。NG接口分为NG-C接口（NG-RAN和5GC之间的控制面接口）和NG-U接口（NG-RAN和5GC之间的用户面接口）。</p>
<h2 id="BTS"><a href="#BTS" class="headerlink" title="BTS"></a>BTS</h2><p>Base Transceiver Station，可以简单粗暴理解为专属于2G技术的基站。</p>
<h2 id="BSC"><a href="#BSC" class="headerlink" title="BSC"></a>BSC</h2><p>Base Station Controller：基站控制器，是基站收发台和移动交换中心之间的连接点，也为基站收发台（BTS）和移动交换中心（MSC）之间交换信息提供接口。</p>
<h2 id="MSC"><a href="#MSC" class="headerlink" title="MSC"></a>MSC</h2><p>Mobile Switch Center，移动交换中心，是在电话和数据系统之间提供呼叫转换服务和呼叫控制的设备。</p>
<h2 id="VLR"><a href="#VLR" class="headerlink" title="VLR"></a>VLR</h2><p>Visitor Location Register，访问地址寄存器。是一个动态的数据库，在网络中VLR都是与MSCS合设，协助MSCS记录当前覆盖区域内的所有移动用户的相关信息。</p>
<h2 id="HLR"><a href="#HLR" class="headerlink" title="HLR"></a>HLR</h2><p>Home Location Register，归属地址寄存器。是一个静态数据库，是移动网络中存储永久用户信息的主数据库，包括地址、帐户状态和偏好，是GSM系统的一个集成构件，由用户的网络运营商维护。</p>
<h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>Authentication Center，鉴权中心，是用于产生为确定移动用户身份和对呼叫身份保密所需鉴权、加密的三参数（随机号码 RAND 、符合响应 SRES 和密钥 Kc ）的功能实体。</p>
<h2 id="EIR"><a href="#EIR" class="headerlink" title="EIR"></a>EIR</h2><p>Equipment Identity Register，设备标识寄存器。简单理解为存储终端设备的标识符。</p>
<h2 id="ISDN"><a href="#ISDN" class="headerlink" title="ISDN"></a>ISDN</h2><p>Integrated Services Digital Network，综合业务数字网。是一个数字电话网络国际标准，是一种典型的电路交换网络系统。在ITU的建议中，ISDN是一种在数字电话网IDN的基础上发展起来的通信网络，ISDN能够支持多种业务，包括电话业务和非电话业务。</p>
<h2 id="PSTN"><a href="#PSTN" class="headerlink" title="PSTN"></a>PSTN</h2><p> Public Switched Telephone Network，公共交换电话网络，一种常用旧式电话系统。即我们日常生活中常用的电话网。</p>
<h2 id="PLMN"><a href="#PLMN" class="headerlink" title="PLMN"></a>PLMN</h2><p>Public Land Mobile Network，公共陆地移动网。由政府或它所批准的经营者，为公众提供陆地移动通信业务目的而建立和经营的网络。该网路通常与公众交换电话网（PSTN）互连，形成整个地区或国家规模的通信网。</p>
<h2 id="MME"><a href="#MME" class="headerlink" title="MME"></a>MME</h2><p>Mobility Management Entity，移动管理实体。它负责空闲模式的UE (User Equipment)的定位，传呼过程，包括中继，简单的说MME是负责信令处理部分。</p>
<h2 id="SGW"><a href="#SGW" class="headerlink" title="SGW"></a>SGW</h2><p>Serving Gateway，服务网关。SGW（Serving GateWay，服务网关）是<a href="https://baike.baidu.com/item/移动通信网络/22287927">移动通信网络</a>EPC中的重要网元。EPC网络实际上是原3G核心网PS域的演进版本，而SGW的功能和作用与原3G核心网SGSN网元的用户面相当，即在新的EPC网络中，控制面功能和媒体面功能分离更加彻底。</p>
<h2 id="PGW"><a href="#PGW" class="headerlink" title="PGW"></a>PGW</h2><p>PDN Gateway，PDN网关。类似于GGSN网元的功能，为EPC网络的边界网关，提供用户的会话管理和承载控制、数据转发、IP地址分配以及非3GPP用户接入等功能。它是3GPP接入和非3GPP接入公用数据网络PDN的锚点。所谓3GPP接入，是指3GPP标准家族出来的无线接入技术，比如我国中国移动和中国联通的手机，就是3GPP接入技术；所谓非3GPP接入，就是3GPP标准家族以外的无线接入技术，典型的比如中国电信的CDMA接入技术以及流行的WiFi接入技术等。就是说，在EPC网络中，移动终端如果是非3GPP接入，它可以不经过MME网元和SGW网元，但一定会经过PGW网元，才能接入到PDN。</p>
<h2 id="SGSN"><a href="#SGSN" class="headerlink" title="SGSN"></a>SGSN</h2><p>Serving GPRS Support Node，服务GPRS支持节点。SGSN与GGSN 配合，共同完成移动通信网络分组业务（PS，Packet Service）功能。其地位和作用类似于移动通信网络电路域（CS，Circuit Switch）中的MSC/VLR。当用户处于GPRS Attach（GPRS附着）状态时，SGSN 中存储了同分组相关的用户信息和位置信息。</p>
<h2 id="GGSN"><a href="#GGSN" class="headerlink" title="GGSN"></a>GGSN</h2><p>Gateway GPRS Support Node，网关GPRS支持节点。SGSN与GGSN 配合，共同完成移动通信网络分组业务（PS，Packet Service）功能。主要是起网关作用，它可以和多种不同的数据网络连接，如ISDN、PSPDN和LAN等，可以把GSM网中的GPRS分组数据包进行协议转换，从而可以把这些分组数据包传送到远端的TCP/IP或X.25网络。</p>
<h2 id="ATCA"><a href="#ATCA" class="headerlink" title="ATCA"></a>ATCA</h2><p>Advanced Telecom Computing Architecture，先进电信计算架构，可以理解为是为下一代融合通信及数据网络应用提供的一个高性价比的，基于模块化结构的、兼容的、并可扩展的硬件构架。</p>
<h2 id="ETCA"><a href="#ETCA" class="headerlink" title="ETCA"></a>ETCA</h2><p>Enhanced ATCA，增强型ATCA，可以理解为ATCA的强化版。</p>
<h2 id="SBA"><a href="#SBA" class="headerlink" title="SBA"></a>SBA</h2><p>Service Based Architecture，即基于服务的架构。SBA架构，基于云原生构架设计，借鉴了IT领域的“微服务”理念。把原来具有多个功能的整体，分拆为多个具有独自功能的个体。每个个体，实现自己的微服务。</p>
<h2 id="NFV"><a href="#NFV" class="headerlink" title="NFV"></a>NFV</h2><p>Network Functions Virtualization，利用<a href="https://baike.baidu.com/item/虚拟化技术/276750">虚拟化技术</a>，将<a href="https://baike.baidu.com/item/网络节点/9338583">网络节点</a>阶层的功能，分割成几个功能区块，分别以软件方式实现，不再局限于<a href="https://baike.baidu.com/item/硬件/479446">硬件</a>架构。</p>
<h2 id="IMS"><a href="#IMS" class="headerlink" title="IMS"></a>IMS</h2><p>IP Multimedia Subsystem，是一种全新的多媒体业务形式，它能够满足的<a href="https://baike.baidu.com/item/终端客户">终端客户</a>更新颖、更多样化多媒体业务的需求。IMS被认为是下一代网络的核心技术，也是解决移动与固网融合，引入语音、数据、视频三重融合等差异化业务的重要方式。但是，全球IMS网络多数处于初级阶段，应用方式也处于业界探讨当中。</p>
<h2 id="EPC"><a href="#EPC" class="headerlink" title="EPC"></a>EPC</h2><p>Evolved Packet Core，特点为仅有分组域而无电路域、基于全IP结构、控制与承载分离且网络结构扁平化，其中主要包含MME、SGW、PGW、PCRF等网元。其中SGW和PGW常常合设并被称为SAE-GW。</p>
<h2 id="HSS"><a href="#HSS" class="headerlink" title="HSS"></a>HSS</h2><p>Home Subscriber Server，是IMS（IP Multimedia Subsystem，<a href="https://baike.baidu.com/item/IP多媒体子系统/2841593">IP多媒体子系统</a>）中控制层的重要组成部分，支持用于处理调用/会话的IMS网络实体的主要用户数据库。它包含用户配置文件，执行用户的身份验证和授权，并可提供有关用户物理位置的信息。</p>
<h2 id="NRF"><a href="#NRF" class="headerlink" title="NRF"></a>NRF</h2><p>Network Repository Function，网络存储库功能，负责对网络功能服务注册登记、状态监测等，实现网络功能服务自动化管理、选择和可扩展，并允许每个网络功能发现其它网络功能提供的服务。</p>
<h2 id="PCF"><a href="#PCF" class="headerlink" title="PCF"></a>PCF</h2><p>Policy Control function，策略控制功能，支持统一的策略框架去管理网络行为，提供策略规则给网络实体去实施执行，访问统一数据仓库（UDR）的订阅信息。</p>
<h2 id="UDM"><a href="#UDM" class="headerlink" title="UDM"></a>UDM</h2><p>Unified Data Management，统一数据管理，负责用户标识、签约数据、鉴权数据的管理、用户的服务网元注册管理（比如当前为终端提供业务的AMF、SMF等，如当用户切换了访问的AMF时，UDM还会向旧的AMF发起注销消息，要求旧的AMF删除用户相关信息）。</p>
<h2 id="UDR"><a href="#UDR" class="headerlink" title="UDR"></a>UDR</h2><p>Unified Data Repository，统一数据仓库功能，用于UDM存储订阅数据或读取订阅数据以及PCF存储策略数据或者读取策略数据。</p>
<h2 id="SMF"><a href="#SMF" class="headerlink" title="SMF"></a>SMF</h2><p>Service Management Function，业务管理功能。</p>
<h2 id="AF"><a href="#AF" class="headerlink" title="AF"></a>AF</h2><p>Application Function， 应用功能。AF类似于一个应用服务器，其与其他5G核心网控制面NF交互，并提供业务服务。AF可以针对不同的应用服务而存在，可以由运营商或可信的第三方拥有。</p>
<h2 id="AUSF"><a href="#AUSF" class="headerlink" title="AUSF"></a>AUSF</h2><p>Authentication Server Function，鉴权服务功能 ，AUSF用于接收AMF（access and mobility management function，AMF）对UE进行身份验证的请求，通过向UDM请求密钥，再将UDM下发的密钥转发给AMF进行鉴权处理。</p>
<h2 id="AMF"><a href="#AMF" class="headerlink" title="AMF"></a>AMF</h2><p>Access and Mobility management Function，接入和移动管理功能。AMF负责UE身份验证、鉴权、注册、移动性管理和连接管理等功能。与4G EPC相比，AMF的功能类似于MME。</p>
<h2 id="R-AN"><a href="#R-AN" class="headerlink" title="(R)AN"></a>(R)AN</h2><p>Radio Access Network，无线接入网是指固定用户全部或部分以无线的方式接入到交换机,通常无线接入网所接入的交换机是指PSTN的交换机,但也可以是ISDN的交换机。</p>
<h2 id="UPF"><a href="#UPF" class="headerlink" title="UPF"></a>UPF</h2><p>User Plane Function，用户平面功能，数据从基站到网络的路由转发是它的主要功能，是核心网里唯一的处理数据的模块，剩下的模块都是处理信令的，也就是做网络控制的。5G核心网是彻底的控制面与用户面分离，就是用户面模块仅仅处理数据，控制面模块仅仅负责实现网络管控。</p>
<h2 id="UE"><a href="#UE" class="headerlink" title="UE"></a>UE</h2><p>User Equipment，用户设备（UE）是指在一台无线网络中的设备，其能够使用户通过无线通信网络通信。</p>
]]></content>
      <tags>
        <tag>通信技术</tag>
      </tags>
  </entry>
  <entry>
    <title>帝国时代2决定版数据分析</title>
    <url>/2022/08/23/Aoe/</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2022/08/23/XR4r9qmpelQZ3gW.png" alt=""></p>
<center><big>伟大的帝国正在召唤！</big></center>

<span id="more"></span>
<p>我们从<a href="https://www.kaggle.com/datasets/jerkeeler/age-of-empires-ii-de-match-data">Kaggle</a>下载数据集，解压之后可以发现一共有两个<code>.csv</code>文件。我们通过<code>pandas</code>分别对其读取，观察一下数据的大致摸样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">match_data = pd.read_csv(<span class="string">&#x27;matches.csv&#x27;</span>)</span><br><span class="line">player_data = pd.read_csv(<span class="string">&#x27;match_players.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(match_data.head())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">              token             match  rating   color      civ  team  winner</span></span><br><span class="line"><span class="string">0  rFWxLAdY6TF78xlo  axps4XstiBOmrDeG  1565.0     Red  Chinese     1   False</span></span><br><span class="line"><span class="string">1  zsyvxRyzLh85YIba  axps4XstiBOmrDeG  1600.0    Blue    Goths     2    True</span></span><br><span class="line"><span class="string">2  CHrJISNtjKDKM114  uQdosqwC7uiQ78ya  2145.0  Orange    Incas     1   False</span></span><br><span class="line"><span class="string">3  X147inwVdQuaegxT  uQdosqwC7uiQ78ya  2124.0   Green   Tatars     2    True</span></span><br><span class="line"><span class="string">4  kNckdaCe6pjKm6Au  uQdosqwC7uiQ78ya  2105.0     Red  Magyars     1   False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(player_data.head())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">              token             match  rating   color      civ  team  winner</span></span><br><span class="line"><span class="string">0  rFWxLAdY6TF78xlo  axps4XstiBOmrDeG  1565.0     Red  Chinese     1   False</span></span><br><span class="line"><span class="string">1  zsyvxRyzLh85YIba  axps4XstiBOmrDeG  1600.0    Blue    Goths     2    True</span></span><br><span class="line"><span class="string">2  CHrJISNtjKDKM114  uQdosqwC7uiQ78ya  2145.0  Orange    Incas     1   False</span></span><br><span class="line"><span class="string">3  X147inwVdQuaegxT  uQdosqwC7uiQ78ya  2124.0   Green   Tatars     2    True</span></span><br><span class="line"><span class="string">4  kNckdaCe6pjKm6Au  uQdosqwC7uiQ78ya  2105.0     Red  Magyars     1   False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从中可以看到这份数据其实是有些瑕疵的，比如说NAN的缺失值在开头就能看到了。所以第一步要先对数据进行清洗操作，除了需要关注缺失值处理，对于竞技类游戏我们还需要清洗掉挂机和开挂等异常大的游戏数据，这种异常游戏数据的判断方法可以通过游戏进行的时间长度进行筛选。这里我过滤掉那些游戏时间小于5分钟或游戏时间超出两个小时的（这好像有点误伤，因为我第一喜欢看的八方混战的黑铁局都要酣畅淋漓地打上个三小时，但是那些数据的参考意义不大所以还是忍痛筛掉了）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tidy_matches = match_data.copy()</span><br><span class="line">tidy_matches = tidy_matches.dropna(subset=[<span class="string">&quot;average_rating&quot;</span>])</span><br><span class="line">tidy_matches[<span class="string">&quot;patch&quot;</span>] = tidy_matches[<span class="string">&quot;patch&quot;</span>].replace(<span class="number">37650</span>, <span class="number">37906</span>)</span><br><span class="line">tidy_matches[<span class="string">&quot;duration&quot;</span>] = pd.to_timedelta(tidy_matches[<span class="string">&quot;duration&quot;</span>])</span><br><span class="line">tidy_matches[<span class="string">&quot;duration_s&quot;</span>] = tidy_matches[<span class="string">&quot;duration&quot;</span>].dt.total_seconds()</span><br><span class="line">tidy_matches = tidy_matches[(tidy_matches[<span class="string">&quot;duration_s&quot;</span>] &gt; <span class="number">60</span> * <span class="number">5</span>) &amp; (tidy_matches[<span class="string">&quot;duration_s&quot;</span>] &lt; <span class="number">60</span> * <span class="number">60</span> * <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
<p>随后就是对数据类型进行整理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tidy_matches[<span class="string">&quot;map&quot;</span>] = tidy_matches[<span class="string">&quot;map&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">tidy_matches[<span class="string">&quot;map_size&quot;</span>] = tidy_matches[<span class="string">&quot;map_size&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">tidy_matches[<span class="string">&quot;ladder&quot;</span>] = tidy_matches[<span class="string">&quot;ladder&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">tidy_matches[<span class="string">&quot;patch&quot;</span>] = tidy_matches[<span class="string">&quot;patch&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">tidy_matches[<span class="string">&quot;server&quot;</span>] = tidy_matches[<span class="string">&quot;server&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br><span class="line">player_data[<span class="string">&quot;civ&quot;</span>] = player_data[<span class="string">&quot;civ&quot;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>然后整理一下，得到一个干净的整合体：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">joined_df = pd.merge(player_data, tidy_matches, left_on=<span class="string">&quot;match&quot;</span>, right_on=<span class="string">&quot;token&quot;</span>, suffixes=[<span class="string">&quot;_player&quot;</span>, <span class="string">&quot;_match&quot;</span>])</span><br><span class="line"><span class="comment">#joined_df.to_csv(&#x27;merge.csv&#x27;)</span></span><br></pre></td></tr></table></figure>
<p>根据这份数据，我们可以做出一些有意思的分析，我们开始吧！</p>
<div id="echarts954" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts954'));

        // 指定图表的配置项和数据
        var option = {
    animation: true,
    animationThreshold: 2000,
    animationDuration: 1000,
    animationEasing: "cubicOut",
    animationDelay: 0,
    animationDurationUpdate: 300,
    animationEasingUpdate: "cubicOut",
    animationDelayUpdate: 0,
    color: [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    series: [
        {
            "type": "bar",
            "name": "number",
            "data": [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                1,
                1,
                2,
                1,
                1,
                2,
                2,
                1,
                2,
                2,
                2,
                1,
                1,
                3,
                1,
                2,
                1,
                1,
                2,
                3,
                1,
                1,
                2,
                2,
                1,
                3,
                3,
                2,
                6,
                2,
                1,
                2,
                3,
                2,
                6,
                5,
                7,
                1,
                2,
                1,
                2,
                4,
                2,
                5,
                1,
                6,
                3,
                3,
                7,
                3,
                4,
                4,
                7,
                1,
                2,
                5,
                1,
                3,
                1,
                6,
                5,
                2,
                4,
                4,
                4,
                5,
                4,
                6,
                2,
                6,
                4,
                6,
                7,
                5,
                3,
                9,
                6,
                5,
                5,
                7,
                5,
                9,
                7,
                5,
                3,
                5,
                5,
                6,
                6,
                13,
                4,
                10,
                10,
                3,
                2,
                7,
                3,
                6,
                10,
                7,
                13,
                13,
                6,
                3,
                3,
                4,
                7,
                12,
                9,
                7,
                2,
                11,
                11,
                13,
                7,
                11,
                10,
                8,
                4,
                11,
                3,
                8,
                9,
                14,
                10,
                10,
                9,
                13,
                12,
                14,
                4,
                15,
                12,
                11,
                9,
                14,
                11,
                16,
                12,
                16,
                17,
                12,
                8,
                16,
                12,
                11,
                12,
                12,
                14,
                17,
                14,
                11,
                11,
                13,
                8,
                12,
                15,
                16,
                19,
                22,
                19,
                10,
                12,
                11,
                11,
                16,
                18,
                10,
                23,
                12,
                18,
                10,
                20,
                17,
                32,
                17,
                16,
                13,
                13,
                23,
                10,
                20,
                16,
                23,
                23,
                17,
                28,
                12,
                26,
                29,
                17,
                20,
                15,
                25,
                25,
                28,
                18,
                9,
                23,
                24,
                23,
                18,
                23,
                13,
                28,
                30,
                25,
                30,
                23,
                22,
                36,
                29,
                38,
                30,
                19,
                24,
                28,
                28,
                33,
                26,
                24,
                30,
                23,
                29,
                25,
                24,
                26,
                29,
                29,
                29,
                32,
                35,
                31,
                38,
                33,
                39,
                36,
                31,
                27,
                26,
                33,
                43,
                41,
                36,
                31,
                44,
                44,
                33,
                30,
                37,
                34,
                46,
                44,
                44,
                38,
                52,
                43,
                53,
                47,
                47,
                47,
                33,
                36,
                46,
                42,
                36,
                52,
                41,
                43,
                59,
                53,
                47,
                53,
                52,
                46,
                49,
                48,
                49,
                54,
                48,
                50,
                41,
                54,
                59,
                46,
                65,
                61,
                56,
                54,
                77,
                53,
                57,
                63,
                58,
                75,
                62,
                66,
                51,
                59,
                64,
                63,
                70,
                68,
                61,
                81,
                66,
                84,
                78,
                69,
                70,
                77,
                66,
                71,
                66,
                64,
                79,
                75,
                94,
                73,
                81,
                74,
                100,
                92,
                96,
                97,
                81,
                76,
                109,
                81,
                93,
                99,
                91,
                106,
                91,
                89,
                102,
                85,
                106,
                120,
                103,
                95,
                107,
                117,
                120,
                103,
                110,
                100,
                110,
                114,
                121,
                103,
                124,
                111,
                130,
                108,
                117,
                123,
                133,
                121,
                141,
                117,
                127,
                124,
                114,
                153,
                139,
                116,
                151,
                134,
                142,
                119,
                131,
                153,
                175,
                152,
                139,
                145,
                148,
                174,
                148,
                172,
                167,
                163,
                174,
                185,
                174,
                177,
                207,
                201,
                191,
                171,
                189,
                174,
                171,
                190,
                207,
                187,
                199,
                197,
                207,
                213,
                211,
                213,
                229,
                215,
                201,
                266,
                217,
                198,
                237,
                233,
                216,
                218,
                237,
                242,
                210,
                248,
                251,
                287,
                250,
                259,
                248,
                254,
                281,
                283,
                281,
                279,
                302,
                276,
                280,
                302,
                293,
                333,
                279,
                322,
                330,
                328,
                320,
                324,
                333,
                364,
                335,
                353,
                329,
                347,
                374,
                356,
                391,
                397,
                390,
                396,
                380,
                489,
                393,
                407,
                418,
                423,
                424,
                453,
                457,
                487,
                470,
                437,
                495,
                497,
                479,
                493,
                470,
                460,
                512,
                510,
                553,
                546,
                586,
                546,
                567,
                569,
                568,
                574,
                615,
                617,
                620,
                612,
                653,
                629,
                640,
                651,
                707,
                676,
                719,
                696,
                727,
                752,
                755,
                744,
                777,
                758,
                837,
                825,
                746,
                797,
                836,
                843,
                874,
                871,
                928,
                936,
                935,
                954,
                932,
                960,
                946,
                966,
                919,
                950,
                1016,
                1070,
                1074,
                1122,
                1100,
                1105,
                1115,
                1150,
                1177,
                1148,
                1202,
                1165,
                1183,
                1245,
                1238,
                1218,
                1225,
                1249,
                1279,
                1342,
                1248,
                1388,
                1409,
                1351,
                1348,
                1406,
                1363,
                1379,
                1376,
                1390,
                1427,
                1446,
                1409,
                1466,
                1497,
                1498,
                1442,
                1480,
                1570,
                1561,
                1596,
                1496,
                1554,
                1584,
                1597,
                1682,
                1623,
                1673,
                1636,
                1708,
                1636,
                1726,
                1759,
                1714,
                1781,
                1773,
                1790,
                1801,
                1759,
                1819,
                1720,
                1757,
                1719,
                1821,
                1779,
                1796,
                1782,
                1876,
                1829,
                1844,
                1923,
                1917,
                1983,
                1931,
                1909,
                2010,
                1951,
                1867,
                1882,
                1985,
                2039,
                1953,
                1962,
                2061,
                2000,
                1959,
                2074,
                2002,
                1988,
                2072,
                2083,
                2075,
                2076,
                2103,
                2129,
                2060,
                2115,
                2122,
                2220,
                2135,
                2192,
                2106,
                2175,
                2141,
                2199,
                2173,
                2147,
                2219,
                2242,
                2231,
                2201,
                2233,
                2363,
                2258,
                2229,
                2338,
                2355,
                2376,
                2387,
                2339,
                2321,
                2295,
                2355,
                2331,
                2364,
                2315,
                2402,
                2390,
                2419,
                2462,
                2383,
                2440,
                2466,
                2405,
                2368,
                2532,
                2378,
                2467,
                2401,
                2421,
                2498,
                2532,
                2516,
                2568,
                2574,
                2544,
                2549,
                2538,
                2612,
                2592,
                2589,
                2645,
                2620,
                2649,
                2478,
                2800,
                2618,
                2597,
                2557,
                2650,
                2660,
                2673,
                2635,
                2657,
                2709,
                2670,
                2648,
                2735,
                2638,
                2636,
                2608,
                2779,
                2677,
                2731,
                2672,
                2778,
                2682,
                2730,
                2785,
                2807,
                2733,
                2635,
                2714,
                2847,
                2834,
                2785,
                2746,
                2840,
                2657,
                2782,
                2839,
                2812,
                2764,
                2781,
                2795,
                2886,
                2754,
                2822,
                2885,
                2825,
                2901,
                2855,
                2713,
                2882,
                2899,
                2875,
                2895,
                2916,
                2877,
                2895,
                2932,
                2844,
                2914,
                2981,
                2855,
                2891,
                2875,
                2854,
                2923,
                2940,
                2857,
                2930,
                2951,
                3026,
                2970,
                2927,
                2985,
                2957,
                2870,
                3001,
                2931,
                2941,
                2901,
                3034,
                2960,
                2950,
                2939,
                2932,
                2988,
                2914,
                2950,
                2997,
                2948,
                2804,
                3101,
                3009,
                2984,
                2961,
                2915,
                2952,
                3066,
                2982,
                2825,
                2989,
                2951,
                2860,
                2928,
                2942,
                3037,
                2907,
                3072,
                3004,
                2992,
                2873,
                2972,
                2993,
                2885,
                2873,
                2938,
                2902,
                2994,
                3062,
                2874,
                2989,
                2981,
                2923,
                2882,
                2843,
                2918,
                2957,
                3001,
                2956,
                2888,
                2983,
                3019,
                2993,
                2924,
                2993,
                2945,
                2964,
                2908,
                2939,
                2955,
                2936,
                2903,
                2852,
                2944,
                2952,
                2952,
                2858,
                3026,
                3004,
                2972,
                2937,
                2961,
                2967,
                2935,
                2931,
                2957,
                3035,
                2903,
                2896,
                2895,
                2968,
                2882,
                2900,
                2866,
                2901,
                2963,
                2945,
                2905,
                2945,
                2878,
                2841,
                2922,
                2809,
                2891,
                2910,
                2905,
                2933,
                2940,
                2838,
                2894,
                2827,
                2826,
                2861,
                2821,
                2874,
                2800,
                2857,
                2909,
                2876,
                2733,
                2750,
                2881,
                2775,
                2822,
                2856,
                2822,
                2803,
                2851,
                2787,
                2772,
                2861,
                2793,
                2785,
                2734,
                2773,
                2821,
                2831,
                2763,
                2735,
                2772,
                2768,
                2673,
                2746,
                2799,
                2731,
                2717,
                2799,
                2730,
                2724,
                2744,
                2697,
                2774,
                2655,
                2739,
                2663,
                2747,
                2686,
                2675,
                2763,
                2670,
                2602,
                2635,
                2664,
                2689,
                2802,
                2639,
                2618,
                2669,
                2624,
                2682,
                2636,
                2622,
                2589,
                2634,
                2553,
                2545,
                2656,
                2607,
                2551,
                2525,
                2603,
                2573,
                2528,
                2561,
                2557,
                2532,
                2671,
                2580,
                2465,
                2617,
                2534,
                2477,
                2584,
                2605,
                2539,
                2484,
                2512,
                2497,
                2546,
                2554,
                2535,
                2478,
                2511,
                2473,
                2452,
                2486,
                2533,
                2470,
                2417,
                2348,
                2398,
                2432,
                2420,
                2275,
                2457,
                2423,
                2415,
                2388,
                2406,
                2335,
                2413,
                2402,
                2382,
                2388,
                2356,
                2410,
                2321,
                2337,
                2360,
                2340,
                2368,
                2329,
                2382,
                2352,
                2325,
                2271,
                2322,
                2227,
                2329,
                2207,
                2354,
                2250,
                2456,
                2289,
                2208,
                2265,
                2331,
                2239,
                2238,
                2256,
                2262,
                2190,
                2272,
                2299,
                2202,
                2239,
                2282,
                2238,
                2235,
                2148,
                2185,
                2202,
                2228,
                2303,
                2240,
                2182,
                2180,
                2168,
                2197,
                2214,
                2124,
                2185,
                2133,
                2151,
                2177,
                2112,
                2246,
                2228,
                2114,
                2147,
                2104,
                2134,
                2086,
                2152,
                2126,
                2087,
                2096,
                1992,
                2103,
                2051,
                2102,
                2010,
                2035,
                2070,
                2085,
                2017,
                2114,
                2008,
                2006,
                1960,
                2079,
                2013,
                1925,
                2031,
                1988,
                2014,
                1919,
                2026,
                2010,
                1983,
                2036,
                1971,
                2082,
                2028,
                1999,
                1929,
                1991,
                1989,
                2016,
                1927,
                1902,
                1910,
                1878,
                1949,
                1850,
                1899,
                2032,
                1894,
                1910,
                1942,
                1901,
                1877,
                1777,
                1950,
                1878,
                1926,
                1849,
                1921,
                1917,
                1826,
                1831,
                1847,
                1860,
                1873,
                1856,
                1696,
                1816,
                1868,
                1808,
                1784,
                1802,
                1854,
                1769,
                1820,
                1832,
                1735,
                1709,
                1750,
                1819,
                1750,
                1777,
                1823,
                1705,
                1726,
                1672,
                1703,
                1744,
                1706,
                1652,
                1746,
                1722,
                1698,
                1695,
                1696,
                1634,
                1628,
                1738,
                1608,
                1670,
                1659,
                1629,
                1605,
                1618,
                1651,
                1662,
                1689,
                1637,
                1543,
                1650,
                1608,
                1622,
                1680,
                1589,
                1573,
                1596,
                1593,
                1515,
                1533,
                1602,
                1525,
                1470,
                1518,
                1506,
                1482,
                1576,
                1504,
                1484,
                1434,
                1493,
                1442,
                1505,
                1453,
                1477,
                1419,
                1484,
                1410,
                1432,
                1457,
                1505,
                1481,
                1497,
                1491,
                1398,
                1427,
                1506,
                1432,
                1470,
                1451,
                1439,
                1464,
                1405,
                1381,
                1406,
                1321,
                1381,
                1360,
                1395,
                1393,
                1394,
                1361,
                1350,
                1348,
                1341,
                1357,
                1311,
                1330,
                1373,
                1373,
                1354,
                1315,
                1295,
                1254,
                1320,
                1303,
                1321,
                1302,
                1309,
                1305,
                1239,
                1336,
                1238,
                1221,
                1341,
                1273,
                1304,
                1306,
                1276,
                1304,
                1280,
                1313,
                1217,
                1242,
                1265,
                1210,
                1229,
                1174,
                1193,
                1220,
                1211,
                1208,
                1199,
                1237,
                1186,
                1236,
                1191,
                1223,
                1219,
                1243,
                1196,
                1175,
                1202,
                1135,
                1124,
                1213,
                1107,
                1111,
                1233,
                1146,
                1189,
                1119,
                1162,
                1095,
                1175,
                1140,
                1053,
                1143,
                1120,
                1103,
                1076,
                1106,
                1074,
                1123,
                1141,
                1080,
                1080,
                1074,
                1087,
                1089,
                1047,
                1062,
                1101,
                1098,
                1028,
                1034,
                1003,
                1084,
                1054,
                1082,
                1026,
                1017,
                1016,
                1041,
                1056,
                966,
                1042,
                981,
                1027,
                997,
                1002,
                978,
                971,
                971,
                980,
                955,
                1009,
                964,
                958,
                1034,
                1032,
                926,
                959,
                956,
                940,
                946,
                946,
                992,
                926,
                944,
                971,
                936,
                913,
                971,
                946,
                864,
                872,
                945,
                890,
                915,
                902,
                920,
                880,
                895,
                906,
                921,
                837,
                835,
                859,
                891,
                846,
                905,
                908,
                849,
                831,
                845,
                868,
                851,
                843,
                858,
                819,
                899,
                758,
                889,
                806,
                861,
                798,
                781,
                795,
                833,
                834,
                824,
                789,
                793,
                794,
                815,
                833,
                819,
                837,
                824,
                770,
                792,
                757,
                808,
                788,
                780,
                748,
                854,
                746,
                766,
                748,
                715,
                756,
                715,
                787,
                728,
                751,
                701,
                704,
                691,
                715,
                704,
                712,
                713,
                651,
                713,
                753,
                740,
                690,
                668,
                747,
                699,
                642,
                707,
                697,
                733,
                680,
                667,
                668,
                725,
                639,
                671,
                693,
                647,
                703,
                665,
                668,
                669,
                653,
                694,
                683,
                664,
                631,
                656,
                699,
                670,
                631,
                643,
                642,
                636,
                676,
                641,
                577,
                616,
                619,
                670,
                629,
                595,
                609,
                644,
                614,
                574,
                661,
                598,
                607,
                571,
                550,
                621,
                556,
                573,
                561,
                627,
                582,
                557,
                566,
                561,
                540,
                534,
                574,
                572,
                533,
                570,
                551,
                577,
                570,
                532,
                529,
                527,
                542,
                548,
                575,
                562,
                524,
                499,
                561,
                513,
                547,
                575,
                507,
                475,
                517,
                586,
                543,
                511,
                507,
                523,
                503,
                507,
                538,
                494,
                482,
                499,
                505,
                509,
                548,
                523,
                499,
                495,
                502,
                486,
                490,
                469,
                490,
                459,
                463,
                470,
                463,
                490,
                482,
                474,
                452,
                457,
                465,
                432,
                432,
                436,
                453,
                386,
                456,
                462,
                459,
                415,
                434,
                422,
                434,
                458,
                410,
                435,
                458,
                444,
                416,
                425,
                394,
                390,
                397,
                438,
                424,
                409,
                426,
                419,
                414,
                419,
                407,
                394,
                410,
                434,
                419,
                407,
                398,
                410,
                361,
                422,
                413,
                402,
                422,
                386,
                383,
                365,
                382,
                393,
                359,
                382,
                387,
                371,
                385,
                340,
                408,
                384,
                376,
                369,
                399,
                366,
                361,
                354,
                393,
                394,
                369,
                368,
                362,
                377,
                393,
                350,
                373,
                342,
                371,
                392,
                342,
                362,
                367,
                309,
                372,
                368,
                336,
                351,
                319,
                348,
                369,
                367,
                364,
                327,
                336,
                342,
                329,
                369,
                327,
                334,
                331,
                361,
                333,
                361,
                371,
                331,
                309,
                316,
                332,
                320,
                337,
                326,
                345,
                318,
                314,
                376,
                323,
                331,
                333,
                332,
                316,
                300,
                314,
                317,
                340,
                314,
                340,
                305,
                301,
                358,
                336,
                316,
                294,
                312,
                283,
                274,
                333,
                315,
                343,
                301,
                306,
                319,
                343,
                286,
                306,
                309,
                289,
                275,
                316,
                275,
                279,
                294,
                282,
                287,
                307,
                295,
                268,
                273,
                269,
                293,
                305,
                274,
                281,
                302,
                299,
                298,
                268,
                288,
                268,
                276,
                310,
                259,
                294,
                273,
                285,
                250,
                283,
                260,
                296,
                272,
                282,
                277,
                292,
                249,
                267,
                282,
                286,
                262,
                290,
                271,
                248,
                273,
                248,
                265,
                250,
                271,
                264,
                271,
                242,
                243,
                269,
                281,
                265,
                241,
                225,
                271,
                205,
                257,
                248,
                267,
                266,
                227,
                273,
                242,
                252,
                258,
                262,
                258,
                266,
                265,
                247,
                233,
                233,
                246,
                222,
                227,
                236,
                252,
                241,
                255,
                198,
                235,
                208,
                241,
                218,
                240,
                216,
                222,
                219,
                219,
                233,
                220,
                245,
                253,
                236,
                234,
                215,
                207,
                224,
                225,
                219,
                260,
                228,
                210,
                203,
                223,
                245,
                203,
                219,
                217,
                207,
                215,
                200,
                233,
                217,
                200,
                203,
                231,
                198,
                200,
                207,
                191,
                224,
                184,
                191,
                221,
                201,
                198,
                219,
                205,
                216,
                192,
                217,
                209,
                185,
                205,
                202,
                210,
                187,
                210,
                209,
                204,
                224,
                180,
                213,
                228,
                179,
                218,
                210,
                200,
                212,
                210,
                185,
                206,
                219,
                206,
                186,
                201,
                179,
                182,
                198,
                194,
                196,
                191,
                176,
                204,
                186,
                190,
                195,
                196,
                209,
                206,
                201,
                195,
                213,
                216,
                179,
                178,
                185,
                217,
                209,
                213,
                216,
                185,
                211,
                171,
                193,
                208,
                193,
                191,
                197,
                162,
                169,
                183,
                199,
                208,
                194,
                195,
                198,
                174,
                213,
                192,
                189,
                205,
                186,
                198,
                204,
                187,
                198,
                199,
                179,
                177,
                174,
                199,
                164,
                208,
                207,
                186,
                212,
                209,
                190,
                178,
                180,
                179,
                168,
                199,
                190,
                189,
                182,
                186,
                178,
                182,
                213,
                183,
                178,
                193,
                205,
                202,
                183,
                187,
                189,
                217,
                175,
                199,
                183,
                164,
                181,
                197,
                181,
                192,
                178,
                183,
                145,
                170,
                182,
                202,
                180,
                188,
                167,
                143,
                181,
                182,
                200,
                165,
                187,
                172,
                190,
                172,
                187,
                172,
                185,
                196,
                137,
                172,
                152,
                156,
                138,
                173,
                181,
                169,
                142,
                176,
                177,
                168,
                179,
                169,
                140,
                165,
                155,
                152,
                165,
                166,
                157,
                146,
                170,
                172,
                175,
                152,
                169,
                152,
                139,
                164,
                137,
                178,
                138,
                150,
                165,
                143,
                136,
                137,
                143,
                148,
                142,
                152,
                119,
                126,
                108,
                110,
                122,
                111,
                145,
                146,
                117,
                130,
                128,
                132,
                131,
                125,
                141,
                135,
                102,
                125,
                141,
                141,
                137,
                122,
                157,
                124,
                137,
                132,
                137,
                130,
                119,
                104,
                128,
                105,
                144,
                112,
                121,
                129,
                115,
                111,
                117,
                109,
                118,
                105,
                119,
                120,
                128,
                109,
                104,
                101,
                110,
                141,
                122,
                108,
                124,
                93,
                121,
                121,
                122,
                106,
                107,
                117,
                117,
                85,
                111,
                89,
                104,
                110,
                129,
                96,
                94,
                104,
                109,
                105,
                103,
                100,
                125,
                91,
                95,
                108,
                116,
                99,
                118,
                114,
                118,
                95,
                115,
                120,
                114,
                81,
                100,
                89,
                106,
                106,
                96,
                90,
                91,
                104,
                82,
                114,
                98,
                95,
                86,
                95,
                83,
                68,
                102,
                78,
                103,
                84,
                78,
                96,
                86,
                100,
                87,
                91,
                80,
                82,
                99,
                98,
                79,
                91,
                87,
                103,
                89,
                102,
                97,
                88,
                95,
                82,
                69,
                86,
                98,
                85,
                102,
                77,
                94,
                78,
                99,
                79,
                76,
                64,
                79,
                84,
                91,
                79,
                61,
                105,
                64,
                85,
                83,
                73,
                76,
                72,
                78,
                64,
                89,
                84,
                78,
                103,
                87,
                71,
                79,
                78,
                79,
                95,
                96,
                73,
                74,
                85,
                67,
                72,
                67,
                61,
                72,
                81,
                80,
                78,
                73,
                80,
                76,
                70,
                72,
                73,
                64,
                84,
                68,
                79,
                85,
                74,
                66,
                74,
                70,
                70,
                72,
                82,
                82,
                59,
                70,
                71,
                68,
                71,
                74,
                64,
                65,
                63,
                71,
                70,
                53,
                79,
                72,
                77,
                68,
                67,
                68,
                63,
                55,
                70,
                60,
                64,
                62,
                60,
                66,
                53,
                80,
                54,
                63,
                76,
                56,
                53,
                66,
                73,
                57,
                54,
                57,
                55,
                65,
                52,
                69,
                71,
                49,
                55,
                61,
                57,
                59,
                58,
                54,
                57,
                38,
                64,
                54,
                52,
                49,
                61,
                64,
                62,
                47,
                37,
                53,
                45,
                46,
                55,
                63,
                50,
                41,
                53,
                59,
                46,
                45,
                56,
                55,
                40,
                44,
                53,
                40,
                49,
                31,
                45,
                37,
                55,
                44,
                37,
                50,
                46,
                46,
                56,
                47,
                40,
                56,
                47,
                36,
                40,
                41,
                44,
                42,
                36,
                47,
                40,
                39,
                29,
                33,
                40,
                41,
                36,
                38,
                42,
                37,
                41,
                44,
                44,
                27,
                43,
                41,
                52,
                31,
                36,
                41,
                35,
                29,
                40,
                33,
                31,
                40,
                38,
                39,
                53,
                41,
                43,
                32,
                40,
                33,
                25,
                31,
                25,
                37,
                37,
                36,
                34,
                29,
                36,
                29,
                33,
                30,
                29,
                23,
                35,
                32,
                32,
                40,
                35,
                31,
                28,
                29,
                41,
                31,
                20,
                30,
                31,
                26,
                21,
                27,
                23,
                32,
                25,
                31,
                26,
                21,
                28,
                29,
                25,
                20,
                19,
                26,
                23,
                22,
                25,
                33,
                23,
                24,
                31,
                24,
                22,
                23,
                19,
                24,
                26,
                22,
                21,
                30,
                20,
                24,
                21,
                21,
                22,
                25,
                18,
                25,
                20,
                24,
                19,
                26,
                29,
                18,
                27,
                24,
                27,
                10,
                14,
                26,
                17,
                25,
                25,
                14,
                20,
                13,
                21,
                18,
                20,
                14,
                11,
                16,
                14,
                14,
                17,
                21,
                16,
                14,
                20,
                16,
                17,
                16,
                16,
                19,
                15,
                9,
                9,
                13,
                15,
                17,
                19,
                15,
                18,
                22,
                19,
                14,
                14,
                12,
                18,
                13,
                17,
                20,
                14,
                16,
                15,
                20,
                14,
                14,
                10,
                10,
                15,
                17,
                21,
                16,
                7,
                10,
                22,
                12,
                19,
                16,
                11,
                9,
                19,
                11,
                11,
                9,
                6,
                11,
                7,
                14,
                12,
                9,
                11,
                17,
                17,
                12,
                12,
                6,
                16,
                8,
                10,
                9,
                11,
                8,
                10,
                9,
                15,
                3,
                9,
                10,
                17,
                15,
                10,
                9,
                14,
                7,
                4,
                13,
                9,
                8,
                4,
                13,
                8,
                10,
                7,
                8,
                5,
                8,
                2,
                6,
                16,
                9,
                4,
                5,
                6,
                4,
                7,
                8,
                8,
                3,
                5,
                5,
                4,
                3,
                8,
                4,
                6,
                7,
                8,
                9,
                4,
                5,
                5,
                6,
                3,
                9,
                4,
                3,
                7,
                5,
                8,
                7,
                7,
                10,
                5,
                4,
                11,
                2,
                2,
                2,
                9,
                11,
                11,
                12,
                2,
                4,
                6,
                6,
                2,
                4,
                1,
                7,
                2,
                5,
                4,
                2,
                3,
                5,
                3,
                3,
                5,
                4,
                4,
                2,
                2,
                4,
                1,
                5,
                2,
                3,
                3,
                3,
                1,
                3,
                2,
                4,
                1,
                5,
                2,
                1,
                3,
                6,
                2,
                5,
                6,
                1,
                5,
                4,
                3,
                4,
                3,
                1,
                3,
                2,
                2,
                1,
                1,
                1,
                4,
                1,
                2,
                2,
                4,
                1,
                3,
                3,
                1,
                2,
                2,
                2,
                2,
                1,
                2,
                5,
                2,
                1,
                1,
                1,
                1,
                2,
                2,
                1,
                2,
                1,
                1,
                1,
                1,
                1,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                3,
                1,
                1,
                1,
                1,
                1,
                1,
                2,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ],
            "barCategoryGap": "20%",
            "label": {
                "show": false,
                "position": "top",
                "margin": 8
            },
            "areaStyle": {
                "opacity": 0.5
            },
            "rippleEffect": {
                "show": true,
                "brushType": "stroke",
                "scale": 2.5,
                "period": 4
            }
        }
    ],
    legend: [
        {
            "data": [
                "number"
            ],
            "selected": {
                "number": true
            },
            "show": false,
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14,
            "bottom": "bottom"
        }
    ],
    tooltip: {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        textStyle: {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        

        },
        borderWidth: 0
    },
    xAxis: [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "axisTick": {
                "show": true,
                "alignWithLabel": true,
                "inside": false
            },
            inverse: false,
            offset: 0,
            splitNumber: 5,
            boundaryGap: false,
            minInterval: 0,
            splitLine: {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            data: [
                144.0,
                147.0,
                171.0,
                181.0,
                193.0,
                195.0,
                198.0,
                204.0,
                220.0,
                222.0,
                234.0,
                235.0,
                237.0,
                246.0,
                247.0,
                248.0,
                253.0,
                254.0,
                255.0,
                257.0,
                259.0,
                260.0,
                265.0,
                267.0,
                270.0,
                271.0,
                273.0,
                277.0,
                278.0,
                280.0,
                281.0,
                282.0,
                283.0,
                285.0,
                286.0,
                287.0,
                289.0,
                290.0,
                291.0,
                293.0,
                294.0,
                295.0,
                298.0,
                299.0,
                300.0,
                301.0,
                302.0,
                303.0,
                304.0,
                305.0,
                306.0,
                308.0,
                309.0,
                310.0,
                311.0,
                312.0,
                313.0,
                314.0,
                315.0,
                317.0,
                318.0,
                319.0,
                320.0,
                321.0,
                322.0,
                323.0,
                324.0,
                326.0,
                327.0,
                328.0,
                329.0,
                330.0,
                331.0,
                332.0,
                333.0,
                334.0,
                335.0,
                336.0,
                337.0,
                339.0,
                340.0,
                341.0,
                342.0,
                343.0,
                344.0,
                345.0,
                346.0,
                347.0,
                348.0,
                349.0,
                350.0,
                351.0,
                352.0,
                353.0,
                354.0,
                355.0,
                356.0,
                357.0,
                358.0,
                359.0,
                360.0,
                361.0,
                362.0,
                363.0,
                364.0,
                365.0,
                366.0,
                367.0,
                368.0,
                369.0,
                370.0,
                371.0,
                372.0,
                373.0,
                374.0,
                375.0,
                376.0,
                377.0,
                378.0,
                379.0,
                380.0,
                381.0,
                382.0,
                383.0,
                384.0,
                385.0,
                386.0,
                387.0,
                388.0,
                389.0,
                390.0,
                391.0,
                392.0,
                393.0,
                394.0,
                395.0,
                396.0,
                397.0,
                398.0,
                399.0,
                400.0,
                401.0,
                402.0,
                403.0,
                404.0,
                405.0,
                406.0,
                407.0,
                408.0,
                409.0,
                410.0,
                411.0,
                412.0,
                413.0,
                414.0,
                415.0,
                416.0,
                417.0,
                418.0,
                419.0,
                420.0,
                421.0,
                422.0,
                423.0,
                424.0,
                425.0,
                426.0,
                427.0,
                428.0,
                429.0,
                430.0,
                431.0,
                432.0,
                433.0,
                434.0,
                435.0,
                436.0,
                437.0,
                438.0,
                439.0,
                440.0,
                441.0,
                442.0,
                443.0,
                444.0,
                445.0,
                446.0,
                447.0,
                448.0,
                449.0,
                450.0,
                451.0,
                452.0,
                453.0,
                454.0,
                455.0,
                456.0,
                457.0,
                458.0,
                459.0,
                460.0,
                461.0,
                462.0,
                463.0,
                464.0,
                465.0,
                466.0,
                467.0,
                468.0,
                469.0,
                470.0,
                471.0,
                472.0,
                473.0,
                474.0,
                475.0,
                476.0,
                477.0,
                478.0,
                479.0,
                480.0,
                481.0,
                482.0,
                483.0,
                484.0,
                485.0,
                486.0,
                487.0,
                488.0,
                489.0,
                490.0,
                491.0,
                492.0,
                493.0,
                494.0,
                495.0,
                496.0,
                497.0,
                498.0,
                499.0,
                500.0,
                501.0,
                502.0,
                503.0,
                504.0,
                505.0,
                506.0,
                507.0,
                508.0,
                509.0,
                510.0,
                511.0,
                512.0,
                513.0,
                514.0,
                515.0,
                516.0,
                517.0,
                518.0,
                519.0,
                520.0,
                521.0,
                522.0,
                523.0,
                524.0,
                525.0,
                526.0,
                527.0,
                528.0,
                529.0,
                530.0,
                531.0,
                532.0,
                533.0,
                534.0,
                535.0,
                536.0,
                537.0,
                538.0,
                539.0,
                540.0,
                541.0,
                542.0,
                543.0,
                544.0,
                545.0,
                546.0,
                547.0,
                548.0,
                549.0,
                550.0,
                551.0,
                552.0,
                553.0,
                554.0,
                555.0,
                556.0,
                557.0,
                558.0,
                559.0,
                560.0,
                561.0,
                562.0,
                563.0,
                564.0,
                565.0,
                566.0,
                567.0,
                568.0,
                569.0,
                570.0,
                571.0,
                572.0,
                573.0,
                574.0,
                575.0,
                576.0,
                577.0,
                578.0,
                579.0,
                580.0,
                581.0,
                582.0,
                583.0,
                584.0,
                585.0,
                586.0,
                587.0,
                588.0,
                589.0,
                590.0,
                591.0,
                592.0,
                593.0,
                594.0,
                595.0,
                596.0,
                597.0,
                598.0,
                599.0,
                600.0,
                601.0,
                602.0,
                603.0,
                604.0,
                605.0,
                606.0,
                607.0,
                608.0,
                609.0,
                610.0,
                611.0,
                612.0,
                613.0,
                614.0,
                615.0,
                616.0,
                617.0,
                618.0,
                619.0,
                620.0,
                621.0,
                622.0,
                623.0,
                624.0,
                625.0,
                626.0,
                627.0,
                628.0,
                629.0,
                630.0,
                631.0,
                632.0,
                633.0,
                634.0,
                635.0,
                636.0,
                637.0,
                638.0,
                639.0,
                640.0,
                641.0,
                642.0,
                643.0,
                644.0,
                645.0,
                646.0,
                647.0,
                648.0,
                649.0,
                650.0,
                651.0,
                652.0,
                653.0,
                654.0,
                655.0,
                656.0,
                657.0,
                658.0,
                659.0,
                660.0,
                661.0,
                662.0,
                663.0,
                664.0,
                665.0,
                666.0,
                667.0,
                668.0,
                669.0,
                670.0,
                671.0,
                672.0,
                673.0,
                674.0,
                675.0,
                676.0,
                677.0,
                678.0,
                679.0,
                680.0,
                681.0,
                682.0,
                683.0,
                684.0,
                685.0,
                686.0,
                687.0,
                688.0,
                689.0,
                690.0,
                691.0,
                692.0,
                693.0,
                694.0,
                695.0,
                696.0,
                697.0,
                698.0,
                699.0,
                700.0,
                701.0,
                702.0,
                703.0,
                704.0,
                705.0,
                706.0,
                707.0,
                708.0,
                709.0,
                710.0,
                711.0,
                712.0,
                713.0,
                714.0,
                715.0,
                716.0,
                717.0,
                718.0,
                719.0,
                720.0,
                721.0,
                722.0,
                723.0,
                724.0,
                725.0,
                726.0,
                727.0,
                728.0,
                729.0,
                730.0,
                731.0,
                732.0,
                733.0,
                734.0,
                735.0,
                736.0,
                737.0,
                738.0,
                739.0,
                740.0,
                741.0,
                742.0,
                743.0,
                744.0,
                745.0,
                746.0,
                747.0,
                748.0,
                749.0,
                750.0,
                751.0,
                752.0,
                753.0,
                754.0,
                755.0,
                756.0,
                757.0,
                758.0,
                759.0,
                760.0,
                761.0,
                762.0,
                763.0,
                764.0,
                765.0,
                766.0,
                767.0,
                768.0,
                769.0,
                770.0,
                771.0,
                772.0,
                773.0,
                774.0,
                775.0,
                776.0,
                777.0,
                778.0,
                779.0,
                780.0,
                781.0,
                782.0,
                783.0,
                784.0,
                785.0,
                786.0,
                787.0,
                788.0,
                789.0,
                790.0,
                791.0,
                792.0,
                793.0,
                794.0,
                795.0,
                796.0,
                797.0,
                798.0,
                799.0,
                800.0,
                801.0,
                802.0,
                803.0,
                804.0,
                805.0,
                806.0,
                807.0,
                808.0,
                809.0,
                810.0,
                811.0,
                812.0,
                813.0,
                814.0,
                815.0,
                816.0,
                817.0,
                818.0,
                819.0,
                820.0,
                821.0,
                822.0,
                823.0,
                824.0,
                825.0,
                826.0,
                827.0,
                828.0,
                829.0,
                830.0,
                831.0,
                832.0,
                833.0,
                834.0,
                835.0,
                836.0,
                837.0,
                838.0,
                839.0,
                840.0,
                841.0,
                842.0,
                843.0,
                844.0,
                845.0,
                846.0,
                847.0,
                848.0,
                849.0,
                850.0,
                851.0,
                852.0,
                853.0,
                854.0,
                855.0,
                856.0,
                857.0,
                858.0,
                859.0,
                860.0,
                861.0,
                862.0,
                863.0,
                864.0,
                865.0,
                866.0,
                867.0,
                868.0,
                869.0,
                870.0,
                871.0,
                872.0,
                873.0,
                874.0,
                875.0,
                876.0,
                877.0,
                878.0,
                879.0,
                880.0,
                881.0,
                882.0,
                883.0,
                884.0,
                885.0,
                886.0,
                887.0,
                888.0,
                889.0,
                890.0,
                891.0,
                892.0,
                893.0,
                894.0,
                895.0,
                896.0,
                897.0,
                898.0,
                899.0,
                900.0,
                901.0,
                902.0,
                903.0,
                904.0,
                905.0,
                906.0,
                907.0,
                908.0,
                909.0,
                910.0,
                911.0,
                912.0,
                913.0,
                914.0,
                915.0,
                916.0,
                917.0,
                918.0,
                919.0,
                920.0,
                921.0,
                922.0,
                923.0,
                924.0,
                925.0,
                926.0,
                927.0,
                928.0,
                929.0,
                930.0,
                931.0,
                932.0,
                933.0,
                934.0,
                935.0,
                936.0,
                937.0,
                938.0,
                939.0,
                940.0,
                941.0,
                942.0,
                943.0,
                944.0,
                945.0,
                946.0,
                947.0,
                948.0,
                949.0,
                950.0,
                951.0,
                952.0,
                953.0,
                954.0,
                955.0,
                956.0,
                957.0,
                958.0,
                959.0,
                960.0,
                961.0,
                962.0,
                963.0,
                964.0,
                965.0,
                966.0,
                967.0,
                968.0,
                969.0,
                970.0,
                971.0,
                972.0,
                973.0,
                974.0,
                975.0,
                976.0,
                977.0,
                978.0,
                979.0,
                980.0,
                981.0,
                982.0,
                983.0,
                984.0,
                985.0,
                986.0,
                987.0,
                988.0,
                989.0,
                990.0,
                991.0,
                992.0,
                993.0,
                994.0,
                995.0,
                996.0,
                997.0,
                998.0,
                999.0,
                1000.0,
                1001.0,
                1002.0,
                1003.0,
                1004.0,
                1005.0,
                1006.0,
                1007.0,
                1008.0,
                1009.0,
                1010.0,
                1011.0,
                1012.0,
                1013.0,
                1014.0,
                1015.0,
                1016.0,
                1017.0,
                1018.0,
                1019.0,
                1020.0,
                1021.0,
                1022.0,
                1023.0,
                1024.0,
                1025.0,
                1026.0,
                1027.0,
                1028.0,
                1029.0,
                1030.0,
                1031.0,
                1032.0,
                1033.0,
                1034.0,
                1035.0,
                1036.0,
                1037.0,
                1038.0,
                1039.0,
                1040.0,
                1041.0,
                1042.0,
                1043.0,
                1044.0,
                1045.0,
                1046.0,
                1047.0,
                1048.0,
                1049.0,
                1050.0,
                1051.0,
                1052.0,
                1053.0,
                1054.0,
                1055.0,
                1056.0,
                1057.0,
                1058.0,
                1059.0,
                1060.0,
                1061.0,
                1062.0,
                1063.0,
                1064.0,
                1065.0,
                1066.0,
                1067.0,
                1068.0,
                1069.0,
                1070.0,
                1071.0,
                1072.0,
                1073.0,
                1074.0,
                1075.0,
                1076.0,
                1077.0,
                1078.0,
                1079.0,
                1080.0,
                1081.0,
                1082.0,
                1083.0,
                1084.0,
                1085.0,
                1086.0,
                1087.0,
                1088.0,
                1089.0,
                1090.0,
                1091.0,
                1092.0,
                1093.0,
                1094.0,
                1095.0,
                1096.0,
                1097.0,
                1098.0,
                1099.0,
                1100.0,
                1101.0,
                1102.0,
                1103.0,
                1104.0,
                1105.0,
                1106.0,
                1107.0,
                1108.0,
                1109.0,
                1110.0,
                1111.0,
                1112.0,
                1113.0,
                1114.0,
                1115.0,
                1116.0,
                1117.0,
                1118.0,
                1119.0,
                1120.0,
                1121.0,
                1122.0,
                1123.0,
                1124.0,
                1125.0,
                1126.0,
                1127.0,
                1128.0,
                1129.0,
                1130.0,
                1131.0,
                1132.0,
                1133.0,
                1134.0,
                1135.0,
                1136.0,
                1137.0,
                1138.0,
                1139.0,
                1140.0,
                1141.0,
                1142.0,
                1143.0,
                1144.0,
                1145.0,
                1146.0,
                1147.0,
                1148.0,
                1149.0,
                1150.0,
                1151.0,
                1152.0,
                1153.0,
                1154.0,
                1155.0,
                1156.0,
                1157.0,
                1158.0,
                1159.0,
                1160.0,
                1161.0,
                1162.0,
                1163.0,
                1164.0,
                1165.0,
                1166.0,
                1167.0,
                1168.0,
                1169.0,
                1170.0,
                1171.0,
                1172.0,
                1173.0,
                1174.0,
                1175.0,
                1176.0,
                1177.0,
                1178.0,
                1179.0,
                1180.0,
                1181.0,
                1182.0,
                1183.0,
                1184.0,
                1185.0,
                1186.0,
                1187.0,
                1188.0,
                1189.0,
                1190.0,
                1191.0,
                1192.0,
                1193.0,
                1194.0,
                1195.0,
                1196.0,
                1197.0,
                1198.0,
                1199.0,
                1200.0,
                1201.0,
                1202.0,
                1203.0,
                1204.0,
                1205.0,
                1206.0,
                1207.0,
                1208.0,
                1209.0,
                1210.0,
                1211.0,
                1212.0,
                1213.0,
                1214.0,
                1215.0,
                1216.0,
                1217.0,
                1218.0,
                1219.0,
                1220.0,
                1221.0,
                1222.0,
                1223.0,
                1224.0,
                1225.0,
                1226.0,
                1227.0,
                1228.0,
                1229.0,
                1230.0,
                1231.0,
                1232.0,
                1233.0,
                1234.0,
                1235.0,
                1236.0,
                1237.0,
                1238.0,
                1239.0,
                1240.0,
                1241.0,
                1242.0,
                1243.0,
                1244.0,
                1245.0,
                1246.0,
                1247.0,
                1248.0,
                1249.0,
                1250.0,
                1251.0,
                1252.0,
                1253.0,
                1254.0,
                1255.0,
                1256.0,
                1257.0,
                1258.0,
                1259.0,
                1260.0,
                1261.0,
                1262.0,
                1263.0,
                1264.0,
                1265.0,
                1266.0,
                1267.0,
                1268.0,
                1269.0,
                1270.0,
                1271.0,
                1272.0,
                1273.0,
                1274.0,
                1275.0,
                1276.0,
                1277.0,
                1278.0,
                1279.0,
                1280.0,
                1281.0,
                1282.0,
                1283.0,
                1284.0,
                1285.0,
                1286.0,
                1287.0,
                1288.0,
                1289.0,
                1290.0,
                1291.0,
                1292.0,
                1293.0,
                1294.0,
                1295.0,
                1296.0,
                1297.0,
                1298.0,
                1299.0,
                1300.0,
                1301.0,
                1302.0,
                1303.0,
                1304.0,
                1305.0,
                1306.0,
                1307.0,
                1308.0,
                1309.0,
                1310.0,
                1311.0,
                1312.0,
                1313.0,
                1314.0,
                1315.0,
                1316.0,
                1317.0,
                1318.0,
                1319.0,
                1320.0,
                1321.0,
                1322.0,
                1323.0,
                1324.0,
                1325.0,
                1326.0,
                1327.0,
                1328.0,
                1329.0,
                1330.0,
                1331.0,
                1332.0,
                1333.0,
                1334.0,
                1335.0,
                1336.0,
                1337.0,
                1338.0,
                1339.0,
                1340.0,
                1341.0,
                1342.0,
                1343.0,
                1344.0,
                1345.0,
                1346.0,
                1347.0,
                1348.0,
                1349.0,
                1350.0,
                1351.0,
                1352.0,
                1353.0,
                1354.0,
                1355.0,
                1356.0,
                1357.0,
                1358.0,
                1359.0,
                1360.0,
                1361.0,
                1362.0,
                1363.0,
                1364.0,
                1365.0,
                1366.0,
                1367.0,
                1368.0,
                1369.0,
                1370.0,
                1371.0,
                1372.0,
                1373.0,
                1374.0,
                1375.0,
                1376.0,
                1377.0,
                1378.0,
                1379.0,
                1380.0,
                1381.0,
                1382.0,
                1383.0,
                1384.0,
                1385.0,
                1386.0,
                1387.0,
                1388.0,
                1389.0,
                1390.0,
                1391.0,
                1392.0,
                1393.0,
                1394.0,
                1395.0,
                1396.0,
                1397.0,
                1398.0,
                1399.0,
                1400.0,
                1401.0,
                1402.0,
                1403.0,
                1404.0,
                1405.0,
                1406.0,
                1407.0,
                1408.0,
                1409.0,
                1410.0,
                1411.0,
                1412.0,
                1413.0,
                1414.0,
                1415.0,
                1416.0,
                1417.0,
                1418.0,
                1419.0,
                1420.0,
                1421.0,
                1422.0,
                1423.0,
                1424.0,
                1425.0,
                1426.0,
                1427.0,
                1428.0,
                1429.0,
                1430.0,
                1431.0,
                1432.0,
                1433.0,
                1434.0,
                1435.0,
                1436.0,
                1437.0,
                1438.0,
                1439.0,
                1440.0,
                1441.0,
                1442.0,
                1443.0,
                1444.0,
                1445.0,
                1446.0,
                1447.0,
                1448.0,
                1449.0,
                1450.0,
                1451.0,
                1452.0,
                1453.0,
                1454.0,
                1455.0,
                1456.0,
                1457.0,
                1458.0,
                1459.0,
                1460.0,
                1461.0,
                1462.0,
                1463.0,
                1464.0,
                1465.0,
                1466.0,
                1467.0,
                1468.0,
                1469.0,
                1470.0,
                1471.0,
                1472.0,
                1473.0,
                1474.0,
                1475.0,
                1476.0,
                1477.0,
                1478.0,
                1479.0,
                1480.0,
                1481.0,
                1482.0,
                1483.0,
                1484.0,
                1485.0,
                1486.0,
                1487.0,
                1488.0,
                1489.0,
                1490.0,
                1491.0,
                1492.0,
                1493.0,
                1494.0,
                1495.0,
                1496.0,
                1497.0,
                1498.0,
                1499.0,
                1500.0,
                1501.0,
                1502.0,
                1503.0,
                1504.0,
                1505.0,
                1506.0,
                1507.0,
                1508.0,
                1509.0,
                1510.0,
                1511.0,
                1512.0,
                1513.0,
                1514.0,
                1515.0,
                1516.0,
                1517.0,
                1518.0,
                1519.0,
                1520.0,
                1521.0,
                1522.0,
                1523.0,
                1524.0,
                1525.0,
                1526.0,
                1527.0,
                1528.0,
                1529.0,
                1530.0,
                1531.0,
                1532.0,
                1533.0,
                1534.0,
                1535.0,
                1536.0,
                1537.0,
                1538.0,
                1539.0,
                1540.0,
                1541.0,
                1542.0,
                1543.0,
                1544.0,
                1545.0,
                1546.0,
                1547.0,
                1548.0,
                1549.0,
                1550.0,
                1551.0,
                1552.0,
                1553.0,
                1554.0,
                1555.0,
                1556.0,
                1557.0,
                1558.0,
                1559.0,
                1560.0,
                1561.0,
                1562.0,
                1563.0,
                1564.0,
                1565.0,
                1566.0,
                1567.0,
                1568.0,
                1569.0,
                1570.0,
                1571.0,
                1572.0,
                1573.0,
                1574.0,
                1575.0,
                1576.0,
                1577.0,
                1578.0,
                1579.0,
                1580.0,
                1581.0,
                1582.0,
                1583.0,
                1584.0,
                1585.0,
                1586.0,
                1587.0,
                1588.0,
                1589.0,
                1590.0,
                1591.0,
                1592.0,
                1593.0,
                1594.0,
                1595.0,
                1596.0,
                1597.0,
                1598.0,
                1599.0,
                1600.0,
                1601.0,
                1602.0,
                1603.0,
                1604.0,
                1605.0,
                1606.0,
                1607.0,
                1608.0,
                1609.0,
                1610.0,
                1611.0,
                1612.0,
                1613.0,
                1614.0,
                1615.0,
                1616.0,
                1617.0,
                1618.0,
                1619.0,
                1620.0,
                1621.0,
                1622.0,
                1623.0,
                1624.0,
                1625.0,
                1626.0,
                1627.0,
                1628.0,
                1629.0,
                1630.0,
                1631.0,
                1632.0,
                1633.0,
                1634.0,
                1635.0,
                1636.0,
                1637.0,
                1638.0,
                1639.0,
                1640.0,
                1641.0,
                1642.0,
                1643.0,
                1644.0,
                1645.0,
                1646.0,
                1647.0,
                1648.0,
                1649.0,
                1650.0,
                1651.0,
                1652.0,
                1653.0,
                1654.0,
                1655.0,
                1656.0,
                1657.0,
                1658.0,
                1659.0,
                1660.0,
                1661.0,
                1662.0,
                1663.0,
                1664.0,
                1665.0,
                1666.0,
                1667.0,
                1668.0,
                1669.0,
                1670.0,
                1671.0,
                1672.0,
                1673.0,
                1674.0,
                1675.0,
                1676.0,
                1677.0,
                1678.0,
                1679.0,
                1680.0,
                1681.0,
                1682.0,
                1683.0,
                1684.0,
                1685.0,
                1686.0,
                1687.0,
                1688.0,
                1689.0,
                1690.0,
                1691.0,
                1692.0,
                1693.0,
                1694.0,
                1695.0,
                1696.0,
                1697.0,
                1698.0,
                1699.0,
                1700.0,
                1701.0,
                1702.0,
                1703.0,
                1704.0,
                1705.0,
                1706.0,
                1707.0,
                1708.0,
                1709.0,
                1710.0,
                1711.0,
                1712.0,
                1713.0,
                1714.0,
                1715.0,
                1716.0,
                1717.0,
                1718.0,
                1719.0,
                1720.0,
                1721.0,
                1722.0,
                1723.0,
                1724.0,
                1725.0,
                1726.0,
                1727.0,
                1728.0,
                1729.0,
                1730.0,
                1731.0,
                1732.0,
                1733.0,
                1734.0,
                1735.0,
                1736.0,
                1737.0,
                1738.0,
                1739.0,
                1740.0,
                1741.0,
                1742.0,
                1743.0,
                1744.0,
                1745.0,
                1746.0,
                1747.0,
                1748.0,
                1749.0,
                1750.0,
                1751.0,
                1752.0,
                1753.0,
                1754.0,
                1755.0,
                1756.0,
                1757.0,
                1758.0,
                1759.0,
                1760.0,
                1761.0,
                1762.0,
                1763.0,
                1764.0,
                1765.0,
                1766.0,
                1767.0,
                1768.0,
                1769.0,
                1770.0,
                1771.0,
                1772.0,
                1773.0,
                1774.0,
                1775.0,
                1776.0,
                1777.0,
                1778.0,
                1779.0,
                1780.0,
                1781.0,
                1782.0,
                1783.0,
                1784.0,
                1785.0,
                1786.0,
                1787.0,
                1788.0,
                1789.0,
                1790.0,
                1791.0,
                1792.0,
                1793.0,
                1794.0,
                1795.0,
                1796.0,
                1797.0,
                1798.0,
                1799.0,
                1800.0,
                1801.0,
                1802.0,
                1803.0,
                1804.0,
                1805.0,
                1806.0,
                1807.0,
                1808.0,
                1809.0,
                1810.0,
                1811.0,
                1812.0,
                1813.0,
                1814.0,
                1815.0,
                1816.0,
                1817.0,
                1818.0,
                1819.0,
                1820.0,
                1821.0,
                1822.0,
                1823.0,
                1824.0,
                1825.0,
                1826.0,
                1827.0,
                1828.0,
                1829.0,
                1830.0,
                1831.0,
                1832.0,
                1833.0,
                1834.0,
                1835.0,
                1836.0,
                1837.0,
                1838.0,
                1839.0,
                1840.0,
                1841.0,
                1842.0,
                1843.0,
                1844.0,
                1845.0,
                1846.0,
                1847.0,
                1848.0,
                1849.0,
                1850.0,
                1851.0,
                1852.0,
                1853.0,
                1854.0,
                1855.0,
                1856.0,
                1857.0,
                1858.0,
                1859.0,
                1860.0,
                1861.0,
                1862.0,
                1863.0,
                1864.0,
                1865.0,
                1866.0,
                1867.0,
                1868.0,
                1869.0,
                1870.0,
                1871.0,
                1872.0,
                1873.0,
                1874.0,
                1875.0,
                1876.0,
                1877.0,
                1878.0,
                1879.0,
                1880.0,
                1881.0,
                1882.0,
                1883.0,
                1884.0,
                1885.0,
                1886.0,
                1887.0,
                1888.0,
                1889.0,
                1890.0,
                1891.0,
                1892.0,
                1893.0,
                1894.0,
                1895.0,
                1896.0,
                1897.0,
                1898.0,
                1899.0,
                1900.0,
                1901.0,
                1902.0,
                1903.0,
                1904.0,
                1905.0,
                1906.0,
                1907.0,
                1908.0,
                1909.0,
                1910.0,
                1911.0,
                1912.0,
                1913.0,
                1914.0,
                1915.0,
                1916.0,
                1917.0,
                1918.0,
                1919.0,
                1920.0,
                1921.0,
                1922.0,
                1923.0,
                1924.0,
                1925.0,
                1926.0,
                1927.0,
                1928.0,
                1929.0,
                1930.0,
                1931.0,
                1932.0,
                1933.0,
                1934.0,
                1935.0,
                1936.0,
                1937.0,
                1938.0,
                1939.0,
                1940.0,
                1941.0,
                1942.0,
                1943.0,
                1944.0,
                1945.0,
                1946.0,
                1947.0,
                1948.0,
                1949.0,
                1950.0,
                1951.0,
                1952.0,
                1953.0,
                1954.0,
                1955.0,
                1956.0,
                1957.0,
                1958.0,
                1959.0,
                1960.0,
                1961.0,
                1962.0,
                1963.0,
                1964.0,
                1965.0,
                1966.0,
                1967.0,
                1968.0,
                1969.0,
                1970.0,
                1971.0,
                1972.0,
                1973.0,
                1974.0,
                1975.0,
                1976.0,
                1977.0,
                1978.0,
                1979.0,
                1980.0,
                1981.0,
                1982.0,
                1983.0,
                1984.0,
                1985.0,
                1986.0,
                1987.0,
                1988.0,
                1989.0,
                1990.0,
                1991.0,
                1992.0,
                1993.0,
                1994.0,
                1995.0,
                1996.0,
                1997.0,
                1998.0,
                1999.0,
                2000.0,
                2001.0,
                2002.0,
                2003.0,
                2004.0,
                2005.0,
                2006.0,
                2007.0,
                2008.0,
                2009.0,
                2010.0,
                2011.0,
                2012.0,
                2013.0,
                2014.0,
                2015.0,
                2016.0,
                2017.0,
                2018.0,
                2019.0,
                2020.0,
                2021.0,
                2022.0,
                2023.0,
                2024.0,
                2025.0,
                2026.0,
                2027.0,
                2028.0,
                2029.0,
                2030.0,
                2031.0,
                2032.0,
                2033.0,
                2034.0,
                2035.0,
                2036.0,
                2037.0,
                2038.0,
                2039.0,
                2040.0,
                2041.0,
                2042.0,
                2043.0,
                2044.0,
                2045.0,
                2046.0,
                2047.0,
                2048.0,
                2049.0,
                2050.0,
                2051.0,
                2052.0,
                2053.0,
                2054.0,
                2055.0,
                2056.0,
                2057.0,
                2058.0,
                2059.0,
                2060.0,
                2061.0,
                2062.0,
                2063.0,
                2064.0,
                2065.0,
                2066.0,
                2067.0,
                2068.0,
                2069.0,
                2070.0,
                2071.0,
                2072.0,
                2073.0,
                2074.0,
                2075.0,
                2076.0,
                2077.0,
                2078.0,
                2079.0,
                2080.0,
                2081.0,
                2082.0,
                2083.0,
                2084.0,
                2085.0,
                2086.0,
                2087.0,
                2088.0,
                2089.0,
                2090.0,
                2091.0,
                2092.0,
                2093.0,
                2094.0,
                2095.0,
                2096.0,
                2097.0,
                2098.0,
                2099.0,
                2100.0,
                2101.0,
                2102.0,
                2103.0,
                2104.0,
                2105.0,
                2106.0,
                2107.0,
                2108.0,
                2109.0,
                2110.0,
                2111.0,
                2112.0,
                2113.0,
                2114.0,
                2115.0,
                2116.0,
                2117.0,
                2118.0,
                2119.0,
                2120.0,
                2121.0,
                2122.0,
                2123.0,
                2124.0,
                2125.0,
                2126.0,
                2127.0,
                2128.0,
                2129.0,
                2130.0,
                2131.0,
                2132.0,
                2133.0,
                2134.0,
                2135.0,
                2136.0,
                2137.0,
                2138.0,
                2139.0,
                2140.0,
                2141.0,
                2142.0,
                2143.0,
                2144.0,
                2145.0,
                2146.0,
                2147.0,
                2148.0,
                2149.0,
                2150.0,
                2151.0,
                2152.0,
                2153.0,
                2154.0,
                2155.0,
                2156.0,
                2157.0,
                2158.0,
                2159.0,
                2160.0,
                2161.0,
                2162.0,
                2163.0,
                2164.0,
                2165.0,
                2166.0,
                2167.0,
                2168.0,
                2169.0,
                2170.0,
                2171.0,
                2172.0,
                2173.0,
                2174.0,
                2175.0,
                2176.0,
                2177.0,
                2178.0,
                2179.0,
                2180.0,
                2181.0,
                2182.0,
                2183.0,
                2184.0,
                2185.0,
                2186.0,
                2187.0,
                2188.0,
                2189.0,
                2190.0,
                2191.0,
                2192.0,
                2193.0,
                2194.0,
                2195.0,
                2196.0,
                2197.0,
                2198.0,
                2199.0,
                2200.0,
                2201.0,
                2202.0,
                2203.0,
                2204.0,
                2205.0,
                2206.0,
                2207.0,
                2208.0,
                2209.0,
                2210.0,
                2211.0,
                2212.0,
                2213.0,
                2214.0,
                2215.0,
                2216.0,
                2217.0,
                2218.0,
                2219.0,
                2220.0,
                2221.0,
                2222.0,
                2223.0,
                2224.0,
                2225.0,
                2226.0,
                2227.0,
                2228.0,
                2229.0,
                2230.0,
                2231.0,
                2232.0,
                2233.0,
                2234.0,
                2235.0,
                2236.0,
                2237.0,
                2238.0,
                2239.0,
                2240.0,
                2241.0,
                2242.0,
                2243.0,
                2244.0,
                2245.0,
                2246.0,
                2247.0,
                2248.0,
                2249.0,
                2250.0,
                2251.0,
                2252.0,
                2253.0,
                2254.0,
                2255.0,
                2256.0,
                2257.0,
                2258.0,
                2259.0,
                2260.0,
                2261.0,
                2262.0,
                2263.0,
                2264.0,
                2265.0,
                2266.0,
                2267.0,
                2268.0,
                2269.0,
                2270.0,
                2271.0,
                2272.0,
                2273.0,
                2274.0,
                2275.0,
                2276.0,
                2277.0,
                2278.0,
                2279.0,
                2280.0,
                2281.0,
                2282.0,
                2283.0,
                2284.0,
                2285.0,
                2286.0,
                2287.0,
                2288.0,
                2289.0,
                2290.0,
                2291.0,
                2292.0,
                2293.0,
                2294.0,
                2295.0,
                2296.0,
                2297.0,
                2298.0,
                2299.0,
                2300.0,
                2301.0,
                2302.0,
                2303.0,
                2304.0,
                2305.0,
                2306.0,
                2307.0,
                2308.0,
                2309.0,
                2310.0,
                2311.0,
                2312.0,
                2313.0,
                2314.0,
                2315.0,
                2316.0,
                2317.0,
                2318.0,
                2319.0,
                2320.0,
                2321.0,
                2322.0,
                2323.0,
                2324.0,
                2325.0,
                2326.0,
                2327.0,
                2328.0,
                2329.0,
                2330.0,
                2331.0,
                2332.0,
                2333.0,
                2334.0,
                2335.0,
                2336.0,
                2337.0,
                2338.0,
                2339.0,
                2340.0,
                2341.0,
                2342.0,
                2343.0,
                2344.0,
                2345.0,
                2346.0,
                2347.0,
                2348.0,
                2349.0,
                2350.0,
                2351.0,
                2352.0,
                2353.0,
                2354.0,
                2355.0,
                2356.0,
                2357.0,
                2358.0,
                2359.0,
                2360.0,
                2361.0,
                2362.0,
                2363.0,
                2364.0,
                2365.0,
                2366.0,
                2367.0,
                2368.0,
                2369.0,
                2370.0,
                2371.0,
                2372.0,
                2373.0,
                2374.0,
                2375.0,
                2376.0,
                2377.0,
                2378.0,
                2379.0,
                2380.0,
                2381.0,
                2382.0,
                2383.0,
                2384.0,
                2385.0,
                2386.0,
                2387.0,
                2388.0,
                2389.0,
                2390.0,
                2391.0,
                2392.0,
                2393.0,
                2394.0,
                2395.0,
                2396.0,
                2397.0,
                2398.0,
                2399.0,
                2400.0,
                2401.0,
                2402.0,
                2403.0,
                2404.0,
                2405.0,
                2406.0,
                2407.0,
                2408.0,
                2409.0,
                2410.0,
                2411.0,
                2412.0,
                2413.0,
                2414.0,
                2415.0,
                2416.0,
                2417.0,
                2418.0,
                2419.0,
                2420.0,
                2421.0,
                2422.0,
                2423.0,
                2424.0,
                2425.0,
                2426.0,
                2427.0,
                2428.0,
                2429.0,
                2430.0,
                2431.0,
                2432.0,
                2433.0,
                2434.0,
                2435.0,
                2436.0,
                2437.0,
                2438.0,
                2439.0,
                2440.0,
                2441.0,
                2442.0,
                2443.0,
                2444.0,
                2445.0,
                2446.0,
                2447.0,
                2448.0,
                2449.0,
                2450.0,
                2451.0,
                2452.0,
                2453.0,
                2454.0,
                2455.0,
                2456.0,
                2457.0,
                2458.0,
                2459.0,
                2460.0,
                2461.0,
                2462.0,
                2463.0,
                2464.0,
                2465.0,
                2466.0,
                2467.0,
                2468.0,
                2469.0,
                2470.0,
                2471.0,
                2472.0,
                2473.0,
                2474.0,
                2475.0,
                2476.0,
                2477.0,
                2478.0,
                2479.0,
                2480.0,
                2481.0,
                2482.0,
                2483.0,
                2484.0,
                2485.0,
                2486.0,
                2487.0,
                2488.0,
                2489.0,
                2490.0,
                2491.0,
                2492.0,
                2493.0,
                2494.0,
                2495.0,
                2496.0,
                2497.0,
                2498.0,
                2499.0,
                2500.0,
                2501.0,
                2502.0,
                2503.0,
                2504.0,
                2505.0,
                2506.0,
                2507.0,
                2508.0,
                2509.0,
                2510.0,
                2511.0,
                2512.0,
                2513.0,
                2514.0,
                2515.0,
                2516.0,
                2517.0,
                2518.0,
                2519.0,
                2520.0,
                2521.0,
                2522.0,
                2523.0,
                2524.0,
                2525.0,
                2526.0,
                2527.0,
                2528.0,
                2529.0,
                2530.0,
                2531.0,
                2532.0,
                2533.0,
                2534.0,
                2535.0,
                2536.0,
                2537.0,
                2538.0,
                2539.0,
                2540.0,
                2541.0,
                2542.0,
                2543.0,
                2544.0,
                2545.0,
                2546.0,
                2547.0,
                2548.0,
                2549.0,
                2550.0,
                2551.0,
                2552.0,
                2553.0,
                2554.0,
                2555.0,
                2556.0,
                2557.0,
                2558.0,
                2559.0,
                2560.0,
                2561.0,
                2562.0,
                2563.0,
                2564.0,
                2565.0,
                2566.0,
                2567.0,
                2568.0,
                2569.0,
                2570.0,
                2571.0,
                2572.0,
                2573.0,
                2574.0,
                2575.0,
                2576.0,
                2577.0,
                2578.0,
                2579.0,
                2580.0,
                2581.0,
                2582.0,
                2583.0,
                2584.0,
                2585.0,
                2586.0,
                2587.0,
                2588.0,
                2589.0,
                2590.0,
                2591.0,
                2592.0,
                2593.0,
                2594.0,
                2595.0,
                2596.0,
                2597.0,
                2598.0,
                2599.0,
                2600.0,
                2601.0,
                2602.0,
                2603.0,
                2604.0,
                2605.0,
                2606.0,
                2607.0,
                2608.0,
                2609.0,
                2610.0,
                2611.0,
                2612.0,
                2613.0,
                2614.0,
                2615.0,
                2616.0,
                2617.0,
                2618.0,
                2619.0,
                2620.0,
                2621.0,
                2622.0,
                2623.0,
                2624.0,
                2625.0,
                2626.0,
                2627.0,
                2628.0,
                2629.0,
                2630.0,
                2631.0,
                2632.0,
                2633.0,
                2634.0,
                2635.0,
                2636.0,
                2637.0,
                2638.0,
                2639.0,
                2640.0,
                2641.0,
                2642.0,
                2643.0,
                2644.0,
                2645.0,
                2646.0,
                2647.0,
                2648.0,
                2649.0,
                2650.0,
                2651.0,
                2652.0,
                2653.0,
                2654.0,
                2655.0,
                2656.0,
                2657.0,
                2658.0,
                2659.0,
                2660.0,
                2661.0,
                2662.0,
                2663.0,
                2664.0,
                2665.0,
                2666.0,
                2667.0,
                2668.0,
                2669.0,
                2670.0,
                2671.0,
                2672.0,
                2673.0,
                2674.0,
                2675.0,
                2676.0,
                2677.0,
                2678.0,
                2679.0,
                2680.0,
                2681.0,
                2682.0,
                2683.0,
                2684.0,
                2685.0,
                2686.0,
                2687.0,
                2688.0,
                2689.0,
                2690.0,
                2691.0,
                2692.0,
                2693.0,
                2694.0,
                2695.0,
                2696.0,
                2697.0,
                2698.0,
                2699.0,
                2700.0,
                2701.0,
                2702.0,
                2703.0,
                2704.0,
                2705.0,
                2706.0,
                2707.0,
                2708.0,
                2709.0,
                2710.0,
                2711.0,
                2712.0,
                2713.0,
                2714.0,
                2715.0,
                2716.0,
                2717.0,
                2718.0,
                2719.0,
                2720.0,
                2721.0,
                2722.0,
                2723.0,
                2724.0,
                2725.0,
                2726.0,
                2727.0,
                2728.0,
                2729.0,
                2730.0,
                2731.0,
                2732.0,
                2733.0,
                2734.0,
                2735.0,
                2736.0,
                2737.0,
                2738.0,
                2739.0,
                2740.0,
                2741.0,
                2742.0,
                2743.0,
                2744.0,
                2745.0,
                2746.0,
                2747.0,
                2748.0,
                2749.0,
                2750.0,
                2751.0,
                2752.0,
                2753.0,
                2754.0,
                2755.0,
                2756.0,
                2757.0,
                2758.0,
                2759.0,
                2760.0,
                2761.0,
                2762.0,
                2763.0,
                2764.0,
                2765.0,
                2766.0,
                2767.0,
                2768.0,
                2769.0,
                2770.0,
                2771.0,
                2772.0,
                2773.0,
                2774.0,
                2775.0,
                2776.0,
                2777.0,
                2778.0,
                2779.0,
                2780.0,
                2781.0,
                2782.0,
                2783.0,
                2784.0,
                2785.0,
                2786.0,
                2787.0,
                2788.0,
                2789.0,
                2790.0,
                2791.0,
                2792.0,
                2793.0,
                2794.0,
                2795.0,
                2796.0,
                2797.0,
                2798.0,
                2799.0,
                2800.0,
                2801.0,
                2802.0,
                2803.0,
                2804.0,
                2805.0,
                2806.0,
                2807.0,
                2808.0,
                2809.0,
                2810.0,
                2811.0,
                2812.0,
                2813.0,
                2814.0,
                2815.0,
                2816.0,
                2817.0,
                2818.0,
                2819.0,
                2820.0,
                2821.0,
                2822.0,
                2823.0,
                2824.0,
                2825.0,
                2826.0,
                2827.0,
                2828.0,
                2829.0,
                2831.0,
                2832.0,
                2834.0,
                2835.0,
                2836.0,
                2837.0,
                2839.0,
                2841.0,
                2843.0,
                2844.0,
                2845.0,
                2846.0,
                2847.0,
                2848.0,
                2849.0,
                2850.0,
                2851.0,
                2852.0,
                2853.0,
                2854.0,
                2855.0,
                2856.0,
                2857.0,
                2858.0,
                2860.0,
                2862.0,
                2863.0,
                2864.0,
                2865.0,
                2867.0,
                2868.0,
                2869.0,
                2871.0,
                2872.0,
                2873.0,
                2874.0,
                2878.0,
                2879.0,
                2880.0,
                2882.0,
                2883.0,
                2884.0,
                2889.0,
                2893.0,
                2897.0,
                2899.0,
                2900.0,
                2901.0,
                2907.0,
                2909.0,
                2910.0,
                2913.0,
                2914.0,
                2915.0,
                2920.0,
                2922.0,
                2924.0,
                2925.0,
                2927.0,
                2931.0,
                2933.0,
                2935.0,
                2940.0,
                2943.0,
                2944.0
            ]
        }
    ],
    yAxis: [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            }
        }
    ],
    title: [
        {
            "text": "平均得分统计📈",
            "padding": 5,
            "itemGap": 10,
            "left": 'center',
            "textStyle": {
                "fontSize": 30,
                "fontFamily": "Fusion Pixel",
            },
        }
    ],
    dataZoom: {
        "show": true,
        "type": "slider",
        "realtime": true,
        "start": 20,
        "end": 80,
        "orient": "horizontal",
        "zoomLock": false
    }

};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts7526" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts7526'));

        // 指定图表的配置项和数据
        var option = 
{
    animation: true,
    animationThreshold: 2000,
    animationDuration: 1000,
    animationEasing: "cubicOut",
    animationDelay: 0,
    animationDurationUpdate: 300,
    animationEasingUpdate: "cubicOut",
    animationDelayUpdate: 0,
    color: [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    series: [
        {
            "type": "heatmap",
            "name": "Win Rate",
            "data": [
                [
                    0,
                    0,
                    "AztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecsAztecs"
                ],
                [
                    0,
                    1,
                    50.0
                ],
                [
                    0,
                    2,
                    51.940850277264325
                ],
                [
                    0,
                    3,
                    52.25779967159278
                ],
                [
                    0,
                    4,
                    44.48275862068966
                ],
                [
                    0,
                    5,
                    53.57142857142857
                ],
                [
                    0,
                    6,
                    57.21153846153846
                ],
                [
                    0,
                    7,
                    48.24253075571178
                ],
                [
                    0,
                    8,
                    55.30864197530864
                ],
                [
                    0,
                    9,
                    50.51449953227315
                ],
                [
                    0,
                    10,
                    51.5993265993266
                ],
                [
                    0,
                    11,
                    45.77294685990338
                ],
                [
                    0,
                    12,
                    45.60669456066946
                ],
                [
                    0,
                    13,
                    48.83195177091183
                ],
                [
                    0,
                    14,
                    45.42936288088642
                ],
                [
                    0,
                    15,
                    58.490566037735846
                ],
                [
                    0,
                    16,
                    53.52564102564102
                ],
                [
                    0,
                    17,
                    49.00249376558604
                ],
                [
                    0,
                    18,
                    53.36749633967789
                ],
                [
                    0,
                    19,
                    54.92227979274611
                ],
                [
                    0,
                    20,
                    51.83917878528656
                ],
                [
                    0,
                    21,
                    51.089799476896246
                ],
                [
                    0,
                    22,
                    51.19521912350598
                ],
                [
                    0,
                    23,
                    42.54278728606357
                ],
                [
                    0,
                    24,
                    49.154013015184375
                ],
                [
                    0,
                    25,
                    53.02114803625378
                ],
                [
                    0,
                    26,
                    52.190121155638394
                ],
                [
                    0,
                    27,
                    62.19931271477663
                ],
                [
                    0,
                    28,
                    55.70987654320988
                ],
                [
                    0,
                    29,
                    45.37313432835821
                ],
                [
                    0,
                    30,
                    50.760456273764255
                ],
                [
                    0,
                    31,
                    50.82236842105263
                ],
                [
                    0,
                    32,
                    44.366197183098585
                ],
                [
                    0,
                    33,
                    50.21707670043415
                ],
                [
                    0,
                    34,
                    55.1048951048951
                ],
                [
                    1,
                    0,
                    "BerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbersBerbers"
                ],
                [
                    1,
                    1,
                    48.059149722735675
                ],
                [
                    1,
                    2,
                    50.0
                ],
                [
                    1,
                    3,
                    51.31926121372031
                ],
                [
                    1,
                    4,
                    56.70498084291188
                ],
                [
                    1,
                    5,
                    54.19354838709678
                ],
                [
                    1,
                    6,
                    47.32142857142857
                ],
                [
                    1,
                    7,
                    44.81865284974093
                ],
                [
                    1,
                    8,
                    61.30268199233716
                ],
                [
                    1,
                    9,
                    54.22077922077923
                ],
                [
                    1,
                    10,
                    50.65274151436031
                ],
                [
                    1,
                    11,
                    45.83333333333333
                ],
                [
                    1,
                    12,
                    44.668008048289735
                ],
                [
                    1,
                    13,
                    47.393364928909946
                ],
                [
                    1,
                    14,
                    45.933014354066984
                ],
                [
                    1,
                    15,
                    50.3030303030303
                ],
                [
                    1,
                    16,
                    53.16455696202531
                ],
                [
                    1,
                    17,
                    52.81501340482574
                ],
                [
                    1,
                    18,
                    50.67750677506775
                ],
                [
                    1,
                    19,
                    53.284671532846716
                ],
                [
                    1,
                    20,
                    50.13623978201635
                ],
                [
                    1,
                    21,
                    56.666666666666664
                ],
                [
                    1,
                    22,
                    54.71698113207547
                ],
                [
                    1,
                    23,
                    44.692737430167604
                ],
                [
                    1,
                    24,
                    51.392757660167135
                ],
                [
                    1,
                    25,
                    54.58392101551481
                ],
                [
                    1,
                    26,
                    52.450980392156865
                ],
                [
                    1,
                    27,
                    55.208333333333336
                ],
                [
                    1,
                    28,
                    53.588516746411486
                ],
                [
                    1,
                    29,
                    47.76119402985074
                ],
                [
                    1,
                    30,
                    53.86904761904761
                ],
                [
                    1,
                    31,
                    57.466063348416284
                ],
                [
                    1,
                    32,
                    44.83870967741935
                ],
                [
                    1,
                    33,
                    51.162790697674424
                ],
                [
                    1,
                    34,
                    55.00000000000001
                ],
                [
                    2,
                    0,
                    "BritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritonsBritons"
                ],
                [
                    2,
                    1,
                    47.74220032840722
                ],
                [
                    2,
                    2,
                    48.68073878627968
                ],
                [
                    2,
                    3,
                    50.0
                ],
                [
                    2,
                    4,
                    53.21683876092137
                ],
                [
                    2,
                    5,
                    57.14285714285714
                ],
                [
                    2,
                    6,
                    50.165016501650165
                ],
                [
                    2,
                    7,
                    48.65038560411311
                ],
                [
                    2,
                    8,
                    52.53807106598985
                ],
                [
                    2,
                    9,
                    50.84322298563397
                ],
                [
                    2,
                    10,
                    53.2608695652174
                ],
                [
                    2,
                    11,
                    46.96755994358251
                ],
                [
                    2,
                    12,
                    45.317220543806656
                ],
                [
                    2,
                    13,
                    44.68196037539103
                ],
                [
                    2,
                    14,
                    44.99473129610116
                ],
                [
                    2,
                    15,
                    50.74626865671642
                ],
                [
                    2,
                    16,
                    52.95404814004377
                ],
                [
                    2,
                    17,
                    55.13557929334429
                ],
                [
                    2,
                    18,
                    44.33352906635349
                ],
                [
                    2,
                    19,
                    51.993355481727576
                ],
                [
                    2,
                    20,
                    48.78516624040921
                ],
                [
                    2,
                    21,
                    45.84450402144772
                ],
                [
                    2,
                    22,
                    50.15290519877675
                ],
                [
                    2,
                    23,
                    49.26590538336052
                ],
                [
                    2,
                    24,
                    49.79807393600497
                ],
                [
                    2,
                    25,
                    53.544440446203666
                ],
                [
                    2,
                    26,
                    49.69400244798042
                ],
                [
                    2,
                    27,
                    61.76470588235294
                ],
                [
                    2,
                    28,
                    57.65673175745118
                ],
                [
                    2,
                    29,
                    50.38674033149171
                ],
                [
                    2,
                    30,
                    53.333333333333336
                ],
                [
                    2,
                    31,
                    53.855421686746986
                ],
                [
                    2,
                    32,
                    52.75261324041812
                ],
                [
                    2,
                    33,
                    56.47921760391198
                ],
                [
                    2,
                    34,
                    50.04965243296922
                ],
                [
                    3,
                    0,
                    "BulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgariansBulgarians"
                ],
                [
                    3,
                    1,
                    55.51724137931034
                ],
                [
                    3,
                    2,
                    43.29501915708812
                ],
                [
                    3,
                    3,
                    46.78316123907864
                ],
                [
                    3,
                    4,
                    50.0
                ],
                [
                    3,
                    5,
                    44.83985765124555
                ],
                [
                    3,
                    6,
                    57.14285714285714
                ],
                [
                    3,
                    7,
                    54.0
                ],
                [
                    3,
                    8,
                    51.74825174825175
                ],
                [
                    3,
                    9,
                    53.46358792184724
                ],
                [
                    3,
                    10,
                    45.82677165354331
                ],
                [
                    3,
                    11,
                    46.87953555878085
                ],
                [
                    3,
                    12,
                    50.278706800445924
                ],
                [
                    3,
                    13,
                    44.58161865569273
                ],
                [
                    3,
                    14,
                    52.820512820512825
                ],
                [
                    3,
                    15,
                    48.708487084870846
                ],
                [
                    3,
                    16,
                    50.56818181818182
                ],
                [
                    3,
                    17,
                    46.50698602794411
                ],
                [
                    3,
                    18,
                    48.68421052631579
                ],
                [
                    3,
                    19,
                    51.541850220264315
                ],
                [
                    3,
                    20,
                    46.013289036544855
                ],
                [
                    3,
                    21,
                    48.92915980230642
                ],
                [
                    3,
                    22,
                    51.25448028673835
                ],
                [
                    3,
                    23,
                    43.85245901639344
                ],
                [
                    3,
                    24,
                    48.11568799298861
                ],
                [
                    3,
                    25,
                    47.85714285714286
                ],
                [
                    3,
                    26,
                    52.75459098497496
                ],
                [
                    3,
                    27,
                    52.121212121212125
                ],
                [
                    3,
                    28,
                    49.53846153846154
                ],
                [
                    3,
                    29,
                    48.90965732087227
                ],
                [
                    3,
                    30,
                    48.45173041894354
                ],
                [
                    3,
                    31,
                    50.0
                ],
                [
                    3,
                    32,
                    50.495049504950494
                ],
                [
                    3,
                    33,
                    54.27509293680297
                ],
                [
                    3,
                    34,
                    50.25380710659898
                ],
                [
                    4,
                    0,
                    "BurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmeseBurmese"
                ],
                [
                    4,
                    1,
                    46.42857142857143
                ],
                [
                    4,
                    2,
                    45.80645161290322
                ],
                [
                    4,
                    3,
                    42.857142857142854
                ],
                [
                    4,
                    4,
                    55.16014234875445
                ],
                [
                    4,
                    5,
                    50.0
                ],
                [
                    4,
                    6,
                    58.75706214689266
                ],
                [
                    4,
                    7,
                    51.344086021505376
                ],
                [
                    4,
                    8,
                    42.06008583690987
                ],
                [
                    4,
                    9,
                    56.88311688311688
                ],
                [
                    4,
                    10,
                    46.20253164556962
                ],
                [
                    4,
                    11,
                    53.65168539325843
                ],
                [
                    4,
                    12,
                    57.28952772073922
                ],
                [
                    4,
                    13,
                    46.192893401015226
                ],
                [
                    4,
                    14,
                    50.74626865671642
                ],
                [
                    4,
                    15,
                    61.48648648648649
                ],
                [
                    4,
                    16,
                    41.52542372881356
                ],
                [
                    4,
                    17,
                    44.89051094890511
                ],
                [
                    4,
                    18,
                    53.99159663865546
                ],
                [
                    4,
                    19,
                    51.470588235294116
                ],
                [
                    4,
                    20,
                    54.09356725146199
                ],
                [
                    4,
                    21,
                    54.421768707483
                ],
                [
                    4,
                    22,
                    52.87356321839081
                ],
                [
                    4,
                    23,
                    57.05128205128205
                ],
                [
                    4,
                    24,
                    45.47908232118759
                ],
                [
                    4,
                    25,
                    50.30211480362537
                ],
                [
                    4,
                    26,
                    49.5702005730659
                ],
                [
                    4,
                    27,
                    49.39759036144578
                ],
                [
                    4,
                    28,
                    42.08494208494208
                ],
                [
                    4,
                    29,
                    56.63265306122449
                ],
                [
                    4,
                    30,
                    53.37078651685393
                ],
                [
                    4,
                    31,
                    50.53763440860215
                ],
                [
                    4,
                    32,
                    54.385964912280706
                ],
                [
                    4,
                    33,
                    61.34453781512605
                ],
                [
                    4,
                    34,
                    44.67213114754098
                ],
                [
                    5,
                    0,
                    "ByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantinesByzantines"
                ],
                [
                    5,
                    1,
                    42.78846153846153
                ],
                [
                    5,
                    2,
                    52.67857142857143
                ],
                [
                    5,
                    3,
                    49.83498349834983
                ],
                [
                    5,
                    4,
                    42.857142857142854
                ],
                [
                    5,
                    5,
                    41.24293785310734
                ],
                [
                    5,
                    6,
                    50.0
                ],
                [
                    5,
                    7,
                    46.03174603174603
                ],
                [
                    5,
                    8,
                    52.255639097744364
                ],
                [
                    5,
                    9,
                    51.16822429906542
                ],
                [
                    5,
                    10,
                    50.1187648456057
                ],
                [
                    5,
                    11,
                    44.74474474474475
                ],
                [
                    5,
                    12,
                    43.89489953632149
                ],
                [
                    5,
                    13,
                    44.918699186991866
                ],
                [
                    5,
                    14,
                    46.484375
                ],
                [
                    5,
                    15,
                    51.0
                ],
                [
                    5,
                    16,
                    48.57142857142857
                ],
                [
                    5,
                    17,
                    39.812646370023415
                ],
                [
                    5,
                    18,
                    49.681528662420384
                ],
                [
                    5,
                    19,
                    53.1578947368421
                ],
                [
                    5,
                    20,
                    43.30357142857143
                ],
                [
                    5,
                    21,
                    46.534653465346544
                ],
                [
                    5,
                    22,
                    50.212765957446805
                ],
                [
                    5,
                    23,
                    45.21276595744681
                ],
                [
                    5,
                    24,
                    46.43734643734644
                ],
                [
                    5,
                    25,
                    49.494949494949495
                ],
                [
                    5,
                    26,
                    47.22222222222222
                ],
                [
                    5,
                    27,
                    50.806451612903224
                ],
                [
                    5,
                    28,
                    58.4
                ],
                [
                    5,
                    29,
                    44.827586206896555
                ],
                [
                    5,
                    30,
                    49.00990099009901
                ],
                [
                    5,
                    31,
                    51.98237885462555
                ],
                [
                    5,
                    32,
                    45.454545454545446
                ],
                [
                    5,
                    33,
                    44.522968197879855
                ],
                [
                    5,
                    34,
                    50.3875968992248
                ],
                [
                    6,
                    0,
                    "CeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCeltsCelts"
                ],
                [
                    6,
                    1,
                    51.757469244288224
                ],
                [
                    6,
                    2,
                    55.181347150259064
                ],
                [
                    6,
                    3,
                    51.34961439588689
                ],
                [
                    6,
                    4,
                    46.0
                ],
                [
                    6,
                    5,
                    48.65591397849463
                ],
                [
                    6,
                    6,
                    53.96825396825397
                ],
                [
                    6,
                    7,
                    50.0
                ],
                [
                    6,
                    8,
                    53.14685314685315
                ],
                [
                    6,
                    9,
                    57.082152974504254
                ],
                [
                    6,
                    10,
                    51.89873417721519
                ],
                [
                    6,
                    11,
                    48.08711006474397
                ],
                [
                    6,
                    12,
                    50.576752440106475
                ],
                [
                    6,
                    13,
                    49.89339019189765
                ],
                [
                    6,
                    14,
                    54.83193277310925
                ],
                [
                    6,
                    15,
                    57.28155339805825
                ],
                [
                    6,
                    16,
                    51.36186770428015
                ],
                [
                    6,
                    17,
                    51.187335092348285
                ],
                [
                    6,
                    18,
                    53.25443786982249
                ],
                [
                    6,
                    19,
                    60.357142857142854
                ],
                [
                    6,
                    20,
                    54.63659147869674
                ],
                [
                    6,
                    21,
                    54.71698113207547
                ],
                [
                    6,
                    22,
                    52.16346153846154
                ],
                [
                    6,
                    23,
                    48.951048951048946
                ],
                [
                    6,
                    24,
                    52.841677943166445
                ],
                [
                    6,
                    25,
                    47.76657060518732
                ],
                [
                    6,
                    26,
                    53.52480417754569
                ],
                [
                    6,
                    27,
                    51.010101010100996
                ],
                [
                    6,
                    28,
                    56.59090909090909
                ],
                [
                    6,
                    29,
                    50.36144578313253
                ],
                [
                    6,
                    30,
                    50.943396226415096
                ],
                [
                    6,
                    31,
                    53.19634703196348
                ],
                [
                    6,
                    32,
                    48.02919708029197
                ],
                [
                    6,
                    33,
                    44.61883408071749
                ],
                [
                    6,
                    34,
                    57.29847494553377
                ],
                [
                    7,
                    0,
                    "ChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChineseChinese"
                ],
                [
                    7,
                    1,
                    44.69135802469136
                ],
                [
                    7,
                    2,
                    38.69731800766283
                ],
                [
                    7,
                    3,
                    47.46192893401015
                ],
                [
                    7,
                    4,
                    48.25174825174825
                ],
                [
                    7,
                    5,
                    57.93991416309014
                ],
                [
                    7,
                    6,
                    47.744360902255636
                ],
                [
                    7,
                    7,
                    46.853146853146846
                ],
                [
                    7,
                    8,
                    50.0
                ],
                [
                    7,
                    9,
                    51.28205128205128
                ],
                [
                    7,
                    10,
                    53.529411764705884
                ],
                [
                    7,
                    11,
                    43.40277777777778
                ],
                [
                    7,
                    12,
                    43.85245901639344
                ],
                [
                    7,
                    13,
                    46.35658914728682
                ],
                [
                    7,
                    14,
                    52.80898876404494
                ],
                [
                    7,
                    15,
                    47.345132743362825
                ],
                [
                    7,
                    16,
                    51.80722891566265
                ],
                [
                    7,
                    17,
                    45.39007092198582
                ],
                [
                    7,
                    18,
                    44.62081128747795
                ],
                [
                    7,
                    19,
                    45.54973821989529
                ],
                [
                    7,
                    20,
                    48.0
                ],
                [
                    7,
                    21,
                    43.42105263157895
                ],
                [
                    7,
                    22,
                    52.752293577981646
                ],
                [
                    7,
                    23,
                    51.58371040723983
                ],
                [
                    7,
                    24,
                    46.99140401146132
                ],
                [
                    7,
                    25,
                    49.65986394557823
                ],
                [
                    7,
                    26,
                    45.880149812734075
                ],
                [
                    7,
                    27,
                    61.53846153846154
                ],
                [
                    7,
                    28,
                    43.90243902439024
                ],
                [
                    7,
                    29,
                    48.84488448844885
                ],
                [
                    7,
                    30,
                    46.37362637362638
                ],
                [
                    7,
                    31,
                    53.99239543726235
                ],
                [
                    7,
                    32,
                    48.86649874055416
                ],
                [
                    7,
                    33,
                    48.62745098039216
                ],
                [
                    7,
                    34,
                    49.14285714285714
                ],
                [
                    8,
                    0,
                    "CumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumansCumans"
                ],
                [
                    8,
                    1,
                    49.48550046772685
                ],
                [
                    8,
                    2,
                    45.77922077922078
                ],
                [
                    8,
                    3,
                    49.15677701436602
                ],
                [
                    8,
                    4,
                    46.536412078152765
                ],
                [
                    8,
                    5,
                    43.11688311688312
                ],
                [
                    8,
                    6,
                    48.831775700934585
                ],
                [
                    8,
                    7,
                    42.917847025495746
                ],
                [
                    8,
                    8,
                    48.71794871794872
                ],
                [
                    8,
                    9,
                    50.0
                ],
                [
                    8,
                    10,
                    48.71060171919771
                ],
                [
                    8,
                    11,
                    44.368388301182335
                ],
                [
                    8,
                    12,
                    44.35411020776874
                ],
                [
                    8,
                    13,
                    46.21951219512195
                ],
                [
                    8,
                    14,
                    41.54929577464789
                ],
                [
                    8,
                    15,
                    43.81270903010033
                ],
                [
                    8,
                    16,
                    48.0
                ],
                [
                    8,
                    17,
                    46.53284671532847
                ],
                [
                    8,
                    18,
                    48.32635983263598
                ],
                [
                    8,
                    19,
                    49.31972789115646
                ],
                [
                    8,
                    20,
                    43.916913946587535
                ],
                [
                    8,
                    21,
                    50.0
                ],
                [
                    8,
                    22,
                    47.27793696275072
                ],
                [
                    8,
                    23,
                    50.0
                ],
                [
                    8,
                    24,
                    48.16446402349486
                ],
                [
                    8,
                    25,
                    51.659919028340084
                ],
                [
                    8,
                    26,
                    48.875562218890565
                ],
                [
                    8,
                    27,
                    59.036144578313255
                ],
                [
                    8,
                    28,
                    49.10313901345291
                ],
                [
                    8,
                    29,
                    45.97156398104265
                ],
                [
                    8,
                    30,
                    47.77486910994764
                ],
                [
                    8,
                    31,
                    53.735632183908045
                ],
                [
                    8,
                    32,
                    46.72364672364672
                ],
                [
                    8,
                    33,
                    50.96359743040685
                ],
                [
                    8,
                    34,
                    49.89429175475687
                ],
                [
                    9,
                    0,
                    "EthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopiansEthiopians"
                ],
                [
                    9,
                    1,
                    48.400673400673405
                ],
                [
                    9,
                    2,
                    49.34725848563969
                ],
                [
                    9,
                    3,
                    46.73913043478261
                ],
                [
                    9,
                    4,
                    54.17322834645669
                ],
                [
                    9,
                    5,
                    53.79746835443038
                ],
                [
                    9,
                    6,
                    49.8812351543943
                ],
                [
                    9,
                    7,
                    48.10126582278481
                ],
                [
                    9,
                    8,
                    46.470588235294116
                ],
                [
                    9,
                    9,
                    51.28939828080229
                ],
                [
                    9,
                    10,
                    50.0
                ],
                [
                    9,
                    11,
                    47.57062146892656
                ],
                [
                    9,
                    12,
                    50.095238095238095
                ],
                [
                    9,
                    13,
                    47.78856526429342
                ],
                [
                    9,
                    14,
                    51.48936170212765
                ],
                [
                    9,
                    15,
                    49.18032786885246
                ],
                [
                    9,
                    16,
                    53.275109170305676
                ],
                [
                    9,
                    17,
                    48.04753820033956
                ],
                [
                    9,
                    18,
                    46.8371467025572
                ],
                [
                    9,
                    19,
                    52.362204724409445
                ],
                [
                    9,
                    20,
                    47.233468286099864
                ],
                [
                    9,
                    21,
                    48.79807692307692
                ],
                [
                    9,
                    22,
                    48.17275747508306
                ],
                [
                    9,
                    23,
                    49.08424908424908
                ],
                [
                    9,
                    24,
                    46.84278350515464
                ],
                [
                    9,
                    25,
                    49.23857868020304
                ],
                [
                    9,
                    26,
                    47.73299748110832
                ],
                [
                    9,
                    27,
                    51.85185185185185
                ],
                [
                    9,
                    28,
                    53.07017543859649
                ],
                [
                    9,
                    29,
                    44.24379232505644
                ],
                [
                    9,
                    30,
                    48.325358851674636
                ],
                [
                    9,
                    31,
                    50.59952038369304
                ],
                [
                    9,
                    32,
                    45.71428571428571
                ],
                [
                    9,
                    33,
                    52.910052910052904
                ],
                [
                    9,
                    34,
                    47.74193548387097
                ],
                [
                    10,
                    0,
                    "FranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranksFranks"
                ],
                [
                    10,
                    1,
                    54.227053140096615
                ],
                [
                    10,
                    2,
                    54.166666666666664
                ],
                [
                    10,
                    3,
                    53.032440056417485
                ],
                [
                    10,
                    4,
                    53.12046444121916
                ],
                [
                    10,
                    5,
                    46.34831460674158
                ],
                [
                    10,
                    6,
                    55.25525525525525
                ],
                [
                    10,
                    7,
                    51.91288993525603
                ],
                [
                    10,
                    8,
                    56.59722222222222
                ],
                [
                    10,
                    9,
                    55.63161169881767
                ],
                [
                    10,
                    10,
                    52.429378531073446
                ],
                [
                    10,
                    11,
                    50.0
                ],
                [
                    10,
                    12,
                    50.211457131872365
                ],
                [
                    10,
                    13,
                    48.63167339614177
                ],
                [
                    10,
                    14,
                    49.39647168059425
                ],
                [
                    10,
                    15,
                    49.514563106796125
                ],
                [
                    10,
                    16,
                    55.243445692883896
                ],
                [
                    10,
                    17,
                    52.78783490224475
                ],
                [
                    10,
                    18,
                    55.695443645083934
                ],
                [
                    10,
                    19,
                    61.61449752883031
                ],
                [
                    10,
                    20,
                    51.27758420441347
                ],
                [
                    10,
                    21,
                    53.86289445048966
                ],
                [
                    10,
                    22,
                    53.31412103746398
                ],
                [
                    10,
                    23,
                    54.112554112554115
                ],
                [
                    10,
                    24,
                    55.18783542039356
                ],
                [
                    10,
                    25,
                    56.516129032258064
                ],
                [
                    10,
                    26,
                    53.45016429353778
                ],
                [
                    10,
                    27,
                    57.932692307692314
                ],
                [
                    10,
                    28,
                    54.08997955010225
                ],
                [
                    10,
                    29,
                    54.43037974683544
                ],
                [
                    10,
                    30,
                    53.03535690460307
                ],
                [
                    10,
                    31,
                    57.26227795193313
                ],
                [
                    10,
                    32,
                    49.17382683410443
                ],
                [
                    10,
                    33,
                    56.65859564164649
                ],
                [
                    10,
                    34,
                    57.29068673565381
                ],
                [
                    11,
                    0,
                    "GothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGothsGoths"
                ],
                [
                    11,
                    1,
                    54.39330543933054
                ],
                [
                    11,
                    2,
                    55.33199195171026
                ],
                [
                    11,
                    3,
                    54.68277945619335
                ],
                [
                    11,
                    4,
                    49.72129319955407
                ],
                [
                    11,
                    5,
                    42.71047227926078
                ],
                [
                    11,
                    6,
                    56.10510046367851
                ],
                [
                    11,
                    7,
                    49.42324755989352
                ],
                [
                    11,
                    8,
                    56.14754098360656
                ],
                [
                    11,
                    9,
                    55.64588979223125
                ],
                [
                    11,
                    10,
                    49.904761904761905
                ],
                [
                    11,
                    11,
                    49.78854286812765
                ],
                [
                    11,
                    12,
                    50.0
                ],
                [
                    11,
                    13,
                    52.83018867924528
                ],
                [
                    11,
                    14,
                    50.847457627118644
                ],
                [
                    11,
                    15,
                    62.227074235807855
                ],
                [
                    11,
                    16,
                    60.273972602739725
                ],
                [
                    11,
                    17,
                    44.666666666666664
                ],
                [
                    11,
                    18,
                    56.91573926868044
                ],
                [
                    11,
                    19,
                    60.62052505966587
                ],
                [
                    11,
                    20,
                    56.49520488230165
                ],
                [
                    11,
                    21,
                    56.97674418604651
                ],
                [
                    11,
                    22,
                    54.25531914893617
                ],
                [
                    11,
                    23,
                    49.38574938574938
                ],
                [
                    11,
                    24,
                    57.17044905842587
                ],
                [
                    11,
                    25,
                    57.78947368421052
                ],
                [
                    11,
                    26,
                    58.65384615384615
                ],
                [
                    11,
                    27,
                    56.27376425855514
                ],
                [
                    11,
                    28,
                    52.483443708609265
                ],
                [
                    11,
                    29,
                    52.82131661442007
                ],
                [
                    11,
                    30,
                    50.0
                ],
                [
                    11,
                    31,
                    57.71812080536913
                ],
                [
                    11,
                    32,
                    45.557851239669425
                ],
                [
                    11,
                    33,
                    46.312178387650086
                ],
                [
                    11,
                    34,
                    56.48414985590778
                ],
                [
                    12,
                    0,
                    "HunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHunsHuns"
                ],
                [
                    12,
                    1,
                    51.16804822908817
                ],
                [
                    12,
                    2,
                    52.60663507109005
                ],
                [
                    12,
                    3,
                    55.31803962460897
                ],
                [
                    12,
                    4,
                    55.418381344307264
                ],
                [
                    12,
                    5,
                    53.80710659898477
                ],
                [
                    12,
                    6,
                    55.08130081300813
                ],
                [
                    12,
                    7,
                    50.10660980810234
                ],
                [
                    12,
                    8,
                    53.64341085271318
                ],
                [
                    12,
                    9,
                    53.78048780487805
                ],
                [
                    12,
                    10,
                    52.21143473570658
                ],
                [
                    12,
                    11,
                    51.36832660385823
                ],
                [
                    12,
                    12,
                    47.16981132075472
                ],
                [
                    12,
                    13,
                    50.0
                ],
                [
                    12,
                    14,
                    50.0
                ],
                [
                    12,
                    15,
                    50.50761421319797
                ],
                [
                    12,
                    16,
                    58.76623376623377
                ],
                [
                    12,
                    17,
                    55.111111111111114
                ],
                [
                    12,
                    18,
                    54.754440961337515
                ],
                [
                    12,
                    19,
                    52.13414634146341
                ],
                [
                    12,
                    20,
                    52.39551478083588
                ],
                [
                    12,
                    21,
                    51.6504854368932
                ],
                [
                    12,
                    22,
                    56.30630630630631
                ],
                [
                    12,
                    23,
                    54.621848739495796
                ],
                [
                    12,
                    24,
                    53.217011995637954
                ],
                [
                    12,
                    25,
                    54.83668341708543
                ],
                [
                    12,
                    26,
                    51.106833493743984
                ],
                [
                    12,
                    27,
                    60.273972602739725
                ],
                [
                    12,
                    28,
                    58.661417322834644
                ],
                [
                    12,
                    29,
                    52.282157676348554
                ],
                [
                    12,
                    30,
                    54.417952314165504
                ],
                [
                    12,
                    31,
                    56.37707948243993
                ],
                [
                    12,
                    32,
                    52.06391478029294
                ],
                [
                    12,
                    33,
                    55.81395348837209
                ],
                [
                    12,
                    34,
                    56.52173913043478
                ],
                [
                    13,
                    0,
                    "IncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncasIncas"
                ],
                [
                    13,
                    1,
                    54.57063711911358
                ],
                [
                    13,
                    2,
                    54.066985645933016
                ],
                [
                    13,
                    3,
                    55.00526870389885
                ],
                [
                    13,
                    4,
                    47.179487179487175
                ],
                [
                    13,
                    5,
                    49.25373134328358
                ],
                [
                    13,
                    6,
                    53.515625
                ],
                [
                    13,
                    7,
                    45.16806722689076
                ],
                [
                    13,
                    8,
                    47.19101123595506
                ],
                [
                    13,
                    9,
                    58.45070422535211
                ],
                [
                    13,
                    10,
                    48.51063829787234
                ],
                [
                    13,
                    11,
                    50.603528319405754
                ],
                [
                    13,
                    12,
                    49.152542372881356
                ],
                [
                    13,
                    13,
                    50.0
                ],
                [
                    13,
                    14,
                    50.0
                ],
                [
                    13,
                    15,
                    53.768844221105525
                ],
                [
                    13,
                    16,
                    55.55555555555556
                ],
                [
                    13,
                    17,
                    49.53703703703704
                ],
                [
                    13,
                    18,
                    53.17286652078774
                ],
                [
                    13,
                    19,
                    56.14973262032086
                ],
                [
                    13,
                    20,
                    53.075170842824605
                ],
                [
                    13,
                    21,
                    50.20576131687243
                ],
                [
                    13,
                    22,
                    51.243781094527364
                ],
                [
                    13,
                    23,
                    49.47916666666667
                ],
                [
                    13,
                    24,
                    53.46534653465347
                ],
                [
                    13,
                    25,
                    54.534883720930225
                ],
                [
                    13,
                    26,
                    53.952569169960476
                ],
                [
                    13,
                    27,
                    55.33980582524271
                ],
                [
                    13,
                    28,
                    51.369863013698634
                ],
                [
                    13,
                    29,
                    50.35460992907801
                ],
                [
                    13,
                    30,
                    50.947867298578196
                ],
                [
                    13,
                    31,
                    60.28708133971292
                ],
                [
                    13,
                    32,
                    50.859950859950864
                ],
                [
                    13,
                    33,
                    49.56140350877193
                ],
                [
                    13,
                    34,
                    55.05617977528089
                ],
                [
                    14,
                    0,
                    "IndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndiansIndians"
                ],
                [
                    14,
                    1,
                    41.50943396226415
                ],
                [
                    14,
                    2,
                    49.6969696969697
                ],
                [
                    14,
                    3,
                    49.25373134328358
                ],
                [
                    14,
                    4,
                    51.291512915129154
                ],
                [
                    14,
                    5,
                    38.51351351351352
                ],
                [
                    14,
                    6,
                    49.0
                ],
                [
                    14,
                    7,
                    42.71844660194175
                ],
                [
                    14,
                    8,
                    52.654867256637175
                ],
                [
                    14,
                    9,
                    56.187290969899664
                ],
                [
                    14,
                    10,
                    50.81967213114754
                ],
                [
                    14,
                    11,
                    50.48543689320388
                ],
                [
                    14,
                    12,
                    37.77292576419214
                ],
                [
                    14,
                    13,
                    49.49238578680203
                ],
                [
                    14,
                    14,
                    46.231155778894475
                ],
                [
                    14,
                    15,
                    50.0
                ],
                [
                    14,
                    16,
                    40.20618556701031
                ],
                [
                    14,
                    17,
                    44.48669201520912
                ],
                [
                    14,
                    18,
                    49.37106918238994
                ],
                [
                    14,
                    19,
                    50.0
                ],
                [
                    14,
                    20,
                    49.54128440366973
                ],
                [
                    14,
                    21,
                    52.03488372093024
                ],
                [
                    14,
                    22,
                    54.83870967741935
                ],
                [
                    14,
                    23,
                    48.872180451127825
                ],
                [
                    14,
                    24,
                    41.76182707993475
                ],
                [
                    14,
                    25,
                    55.80589254766031
                ],
                [
                    14,
                    26,
                    58.30815709969789
                ],
                [
                    14,
                    27,
                    63.63636363636363
                ],
                [
                    14,
                    28,
                    52.26130653266332
                ],
                [
                    14,
                    29,
                    47.159090909090914
                ],
                [
                    14,
                    30,
                    56.20689655172414
                ],
                [
                    14,
                    31,
                    53.714285714285715
                ],
                [
                    14,
                    32,
                    49.82698961937716
                ],
                [
                    14,
                    33,
                    49.056603773584904
                ],
                [
                    14,
                    34,
                    55.33980582524271
                ],
                [
                    15,
                    0,
                    "ItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItaliansItalians"
                ],
                [
                    15,
                    1,
                    46.47435897435898
                ],
                [
                    15,
                    2,
                    46.83544303797468
                ],
                [
                    15,
                    3,
                    47.04595185995624
                ],
                [
                    15,
                    4,
                    49.43181818181818
                ],
                [
                    15,
                    5,
                    58.47457627118644
                ],
                [
                    15,
                    6,
                    51.42857142857142
                ],
                [
                    15,
                    7,
                    48.638132295719835
                ],
                [
                    15,
                    8,
                    48.19277108433735
                ],
                [
                    15,
                    9,
                    52.0
                ],
                [
                    15,
                    10,
                    46.724890829694324
                ],
                [
                    15,
                    11,
                    44.7565543071161
                ],
                [
                    15,
                    12,
                    39.726027397260275
                ],
                [
                    15,
                    13,
                    41.23376623376624
                ],
                [
                    15,
                    14,
                    44.44444444444444
                ],
                [
                    15,
                    15,
                    59.79381443298969
                ],
                [
                    15,
                    16,
                    50.0
                ],
                [
                    15,
                    17,
                    53.21285140562249
                ],
                [
                    15,
                    18,
                    49.206349206349195
                ],
                [
                    15,
                    19,
                    47.647058823529406
                ],
                [
                    15,
                    20,
                    44.29223744292237
                ],
                [
                    15,
                    21,
                    39.47368421052632
                ],
                [
                    15,
                    22,
                    50.0
                ],
                [
                    15,
                    23,
                    47.55244755244755
                ],
                [
                    15,
                    24,
                    49.04306220095694
                ],
                [
                    15,
                    25,
                    46.913580246913575
                ],
                [
                    15,
                    26,
                    46.29156010230179
                ],
                [
                    15,
                    27,
                    59.88023952095808
                ],
                [
                    15,
                    28,
                    48.96907216494845
                ],
                [
                    15,
                    29,
                    46.15384615384616
                ],
                [
                    15,
                    30,
                    49.596774193548384
                ],
                [
                    15,
                    31,
                    49.61832061068702
                ],
                [
                    15,
                    32,
                    49.717514124293785
                ],
                [
                    15,
                    33,
                    48.93617021276596
                ],
                [
                    15,
                    34,
                    52.44755244755245
                ],
                [
                    16,
                    0,
                    "JapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapaneseJapanese"
                ],
                [
                    16,
                    1,
                    50.99750623441397
                ],
                [
                    16,
                    2,
                    47.18498659517426
                ],
                [
                    16,
                    3,
                    44.864420706655714
                ],
                [
                    16,
                    4,
                    53.49301397205589
                ],
                [
                    16,
                    5,
                    55.1094890510949
                ],
                [
                    16,
                    6,
                    60.187353629976585
                ],
                [
                    16,
                    7,
                    48.81266490765172
                ],
                [
                    16,
                    8,
                    54.60992907801418
                ],
                [
                    16,
                    9,
                    53.46715328467153
                ],
                [
                    16,
                    10,
                    51.95246179966044
                ],
                [
                    16,
                    11,
                    47.212165097755246
                ],
                [
                    16,
                    12,
                    55.333333333333336
                ],
                [
                    16,
                    13,
                    44.888888888888886
                ],
                [
                    16,
                    14,
                    50.46296296296296
                ],
                [
                    16,
                    15,
                    55.51330798479087
                ],
                [
                    16,
                    16,
                    46.787148594377506
                ],
                [
                    16,
                    17,
                    50.0
                ],
                [
                    16,
                    18,
                    52.05930807248764
                ],
                [
                    16,
                    19,
                    54.0268456375839
                ],
                [
                    16,
                    20,
                    51.42857142857142
                ],
                [
                    16,
                    21,
                    50.14925373134328
                ],
                [
                    16,
                    22,
                    52.76381909547738
                ],
                [
                    16,
                    23,
                    51.89873417721519
                ],
                [
                    16,
                    24,
                    47.943925233644855
                ],
                [
                    16,
                    25,
                    50.43478260869565
                ],
                [
                    16,
                    26,
                    46.117424242424235
                ],
                [
                    16,
                    27,
                    54.36893203883495
                ],
                [
                    16,
                    28,
                    53.316326530612244
                ],
                [
                    16,
                    29,
                    49.07407407407408
                ],
                [
                    16,
                    30,
                    49.91423670668954
                ],
                [
                    16,
                    31,
                    48.99328859060403
                ],
                [
                    16,
                    32,
                    54.31654676258992
                ],
                [
                    16,
                    33,
                    49.83050847457627
                ],
                [
                    16,
                    34,
                    50.0
                ],
                [
                    17,
                    0,
                    "KhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmerKhmer"
                ],
                [
                    17,
                    1,
                    46.63250366032211
                ],
                [
                    17,
                    2,
                    49.32249322493225
                ],
                [
                    17,
                    3,
                    55.66647093364651
                ],
                [
                    17,
                    4,
                    51.31578947368421
                ],
                [
                    17,
                    5,
                    46.00840336134454
                ],
                [
                    17,
                    6,
                    50.318471337579616
                ],
                [
                    17,
                    7,
                    46.74556213017752
                ],
                [
                    17,
                    8,
                    55.379188712522044
                ],
                [
                    17,
                    9,
                    51.67364016736402
                ],
                [
                    17,
                    10,
                    53.1628532974428
                ],
                [
                    17,
                    11,
                    44.304556354916066
                ],
                [
                    17,
                    12,
                    43.08426073131956
                ],
                [
                    17,
                    13,
                    45.24555903866249
                ],
                [
                    17,
                    14,
                    46.82713347921225
                ],
                [
                    17,
                    15,
                    50.62893081761006
                ],
                [
                    17,
                    16,
                    50.79365079365079
                ],
                [
                    17,
                    17,
                    47.94069192751236
                ],
                [
                    17,
                    18,
                    50.0
                ],
                [
                    17,
                    19,
                    52.5
                ],
                [
                    17,
                    20,
                    47.482014388489205
                ],
                [
                    17,
                    21,
                    50.402144772117964
                ],
                [
                    17,
                    22,
                    55.09708737864077
                ],
                [
                    17,
                    23,
                    55.15151515151515
                ],
                [
                    17,
                    24,
                    51.8925518925519
                ],
                [
                    17,
                    25,
                    52.393442622950815
                ],
                [
                    17,
                    26,
                    52.185089974293064
                ],
                [
                    17,
                    27,
                    58.63453815261044
                ],
                [
                    17,
                    28,
                    56.03112840466926
                ],
                [
                    17,
                    29,
                    46.415770609319004
                ],
                [
                    17,
                    30,
                    47.55501222493888
                ],
                [
                    17,
                    31,
                    52.60770975056689
                ],
                [
                    17,
                    32,
                    46.4200477326969
                ],
                [
                    17,
                    33,
                    50.07898894154819
                ],
                [
                    17,
                    34,
                    56.63551401869159
                ],
                [
                    18,
                    0,
                    "KoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreansKoreans"
                ],
                [
                    18,
                    1,
                    45.07772020725389
                ],
                [
                    18,
                    2,
                    46.71532846715328
                ],
                [
                    18,
                    3,
                    48.00664451827242
                ],
                [
                    18,
                    4,
                    48.458149779735685
                ],
                [
                    18,
                    5,
                    48.529411764705884
                ],
                [
                    18,
                    6,
                    46.842105263157904
                ],
                [
                    18,
                    7,
                    39.64285714285714
                ],
                [
                    18,
                    8,
                    54.45026178010471
                ],
                [
                    18,
                    9,
                    50.68027210884354
                ],
                [
                    18,
                    10,
                    47.63779527559055
                ],
                [
                    18,
                    11,
                    38.385502471169694
                ],
                [
                    18,
                    12,
                    39.37947494033413
                ],
                [
                    18,
                    13,
                    47.86585365853658
                ],
                [
                    18,
                    14,
                    43.85026737967914
                ],
                [
                    18,
                    15,
                    50.0
                ],
                [
                    18,
                    16,
                    52.352941176470594
                ],
                [
                    18,
                    17,
                    45.97315436241611
                ],
                [
                    18,
                    18,
                    47.5
                ],
                [
                    18,
                    19,
                    50.0
                ],
                [
                    18,
                    20,
                    41.015625
                ],
                [
                    18,
                    21,
                    45.96774193548387
                ],
                [
                    18,
                    22,
                    40.10989010989011
                ],
                [
                    18,
                    23,
                    47.32142857142857
                ],
                [
                    18,
                    24,
                    40.56603773584906
                ],
                [
                    18,
                    25,
                    44.080145719489984
                ],
                [
                    18,
                    26,
                    45.5072463768116
                ],
                [
                    18,
                    27,
                    55.33980582524271
                ],
                [
                    18,
                    28,
                    48.69109947643979
                ],
                [
                    18,
                    29,
                    35.22012578616352
                ],
                [
                    18,
                    30,
                    46.59498207885305
                ],
                [
                    18,
                    31,
                    50.331125827814574
                ],
                [
                    18,
                    32,
                    40.74074074074074
                ],
                [
                    18,
                    33,
                    47.10144927536232
                ],
                [
                    18,
                    34,
                    53.57142857142857
                ],
                [
                    19,
                    0,
                    "LithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuaniansLithuanians"
                ],
                [
                    19,
                    1,
                    48.16082121471343
                ],
                [
                    19,
                    2,
                    49.863760217983646
                ],
                [
                    19,
                    3,
                    51.214833759590796
                ],
                [
                    19,
                    4,
                    53.98671096345515
                ],
                [
                    19,
                    5,
                    45.90643274853801
                ],
                [
                    19,
                    6,
                    56.69642857142857
                ],
                [
                    19,
                    7,
                    45.36340852130326
                ],
                [
                    19,
                    8,
                    52.0
                ],
                [
                    19,
                    9,
                    56.083086053412465
                ],
                [
                    19,
                    10,
                    52.766531713900136
                ],
                [
                    19,
                    11,
                    48.722415795586535
                ],
                [
                    19,
                    12,
                    43.50479511769834
                ],
                [
                    19,
                    13,
                    47.604485219164125
                ],
                [
                    19,
                    14,
                    46.9248291571754
                ],
                [
                    19,
                    15,
                    50.45871559633027
                ],
                [
                    19,
                    16,
                    55.70776255707762
                ],
                [
                    19,
                    17,
                    48.57142857142857
                ],
                [
                    19,
                    18,
                    52.51798561151079
                ],
                [
                    19,
                    19,
                    58.984375
                ],
                [
                    19,
                    20,
                    50.0
                ],
                [
                    19,
                    21,
                    52.785145888594165
                ],
                [
                    19,
                    22,
                    52.64623955431755
                ],
                [
                    19,
                    23,
                    50.153846153846146
                ],
                [
                    19,
                    24,
                    48.27127659574468
                ],
                [
                    19,
                    25,
                    54.29992892679459
                ],
                [
                    19,
                    26,
                    51.16822429906542
                ],
                [
                    19,
                    27,
                    60.40609137055838
                ],
                [
                    19,
                    28,
                    55.55555555555556
                ],
                [
                    19,
                    29,
                    48.21428571428572
                ],
                [
                    19,
                    30,
                    54.36746987951807
                ],
                [
                    19,
                    31,
                    57.49385749385749
                ],
                [
                    19,
                    32,
                    47.66917293233083
                ],
                [
                    19,
                    33,
                    52.77777777777778
                ],
                [
                    19,
                    34,
                    56.53061224489796
                ],
                [
                    20,
                    0,
                    "MagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyarsMagyars"
                ],
                [
                    20,
                    1,
                    48.91020052310375
                ],
                [
                    20,
                    2,
                    43.333333333333336
                ],
                [
                    20,
                    3,
                    54.15549597855228
                ],
                [
                    20,
                    4,
                    51.07084019769358
                ],
                [
                    20,
                    5,
                    45.57823129251701
                ],
                [
                    20,
                    6,
                    53.46534653465347
                ],
                [
                    20,
                    7,
                    45.28301886792453
                ],
                [
                    20,
                    8,
                    56.57894736842105
                ],
                [
                    20,
                    9,
                    50.0
                ],
                [
                    20,
                    10,
                    51.20192307692307
                ],
                [
                    20,
                    11,
                    46.13710554951034
                ],
                [
                    20,
                    12,
                    43.02325581395349
                ],
                [
                    20,
                    13,
                    48.3495145631068
                ],
                [
                    20,
                    14,
                    49.79423868312757
                ],
                [
                    20,
                    15,
                    47.96511627906977
                ],
                [
                    20,
                    16,
                    60.526315789473685
                ],
                [
                    20,
                    17,
                    49.85074626865672
                ],
                [
                    20,
                    18,
                    49.597855227882036
                ],
                [
                    20,
                    19,
                    54.03225806451613
                ],
                [
                    20,
                    20,
                    47.21485411140584
                ],
                [
                    20,
                    21,
                    50.0
                ],
                [
                    20,
                    22,
                    55.38922155688623
                ],
                [
                    20,
                    23,
                    46.64536741214058
                ],
                [
                    20,
                    24,
                    52.016129032258064
                ],
                [
                    20,
                    25,
                    54.36479772888574
                ],
                [
                    20,
                    26,
                    50.46480743691899
                ],
                [
                    20,
                    27,
                    58.563535911602195
                ],
                [
                    20,
                    28,
                    53.789731051344745
                ],
                [
                    20,
                    29,
                    48.12206572769953
                ],
                [
                    20,
                    30,
                    51.84563758389261
                ],
                [
                    20,
                    31,
                    53.182751540041075
                ],
                [
                    20,
                    32,
                    47.95144157814871
                ],
                [
                    20,
                    33,
                    48.58156028368794
                ],
                [
                    20,
                    34,
                    52.652259332023576
                ],
                [
                    21,
                    0,
                    "MalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalayMalay"
                ],
                [
                    21,
                    1,
                    48.80478087649402
                ],
                [
                    21,
                    2,
                    45.28301886792453
                ],
                [
                    21,
                    3,
                    49.84709480122324
                ],
                [
                    21,
                    4,
                    48.74551971326165
                ],
                [
                    21,
                    5,
                    47.126436781609186
                ],
                [
                    21,
                    6,
                    49.787234042553195
                ],
                [
                    21,
                    7,
                    47.83653846153846
                ],
                [
                    21,
                    8,
                    47.24770642201835
                ],
                [
                    21,
                    9,
                    52.72206303724928
                ],
                [
                    21,
                    10,
                    51.82724252491694
                ],
                [
                    21,
                    11,
                    46.68587896253602
                ],
                [
                    21,
                    12,
                    45.744680851063826
                ],
                [
                    21,
                    13,
                    43.69369369369369
                ],
                [
                    21,
                    14,
                    48.75621890547264
                ],
                [
                    21,
                    15,
                    45.16129032258064
                ],
                [
                    21,
                    16,
                    50.0
                ],
                [
                    21,
                    17,
                    47.23618090452261
                ],
                [
                    21,
                    18,
                    44.90291262135923
                ],
                [
                    21,
                    19,
                    59.89010989010989
                ],
                [
                    21,
                    20,
                    47.353760445682454
                ],
                [
                    21,
                    21,
                    44.61077844311377
                ],
                [
                    21,
                    22,
                    50.0
                ],
                [
                    21,
                    23,
                    43.24324324324325
                ],
                [
                    21,
                    24,
                    50.07587253414264
                ],
                [
                    21,
                    25,
                    48.58044164037855
                ],
                [
                    21,
                    26,
                    43.859649122807014
                ],
                [
                    21,
                    27,
                    50.6578947368421
                ],
                [
                    21,
                    28,
                    48.325358851674636
                ],
                [
                    21,
                    29,
                    45.10869565217392
                ],
                [
                    21,
                    30,
                    46.08433734939759
                ],
                [
                    21,
                    31,
                    48.044692737430175
                ],
                [
                    21,
                    32,
                    52.41157556270096
                ],
                [
                    21,
                    33,
                    59.22330097087378
                ],
                [
                    21,
                    34,
                    51.42857142857142
                ],
                [
                    22,
                    0,
                    "MaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMaliansMalians"
                ],
                [
                    22,
                    1,
                    57.457212713936435
                ],
                [
                    22,
                    2,
                    55.3072625698324
                ],
                [
                    22,
                    3,
                    50.73409461663948
                ],
                [
                    22,
                    4,
                    56.14754098360656
                ],
                [
                    22,
                    5,
                    42.94871794871795
                ],
                [
                    22,
                    6,
                    54.78723404255319
                ],
                [
                    22,
                    7,
                    51.048951048951054
                ],
                [
                    22,
                    8,
                    48.41628959276018
                ],
                [
                    22,
                    9,
                    50.0
                ],
                [
                    22,
                    10,
                    50.91575091575091
                ],
                [
                    22,
                    11,
                    45.887445887445885
                ],
                [
                    22,
                    12,
                    50.61425061425061
                ],
                [
                    22,
                    13,
                    45.378151260504204
                ],
                [
                    22,
                    14,
                    50.520833333333336
                ],
                [
                    22,
                    15,
                    51.127819548872175
                ],
                [
                    22,
                    16,
                    52.44755244755245
                ],
                [
                    22,
                    17,
                    48.10126582278481
                ],
                [
                    22,
                    18,
                    44.84848484848485
                ],
                [
                    22,
                    19,
                    52.67857142857143
                ],
                [
                    22,
                    20,
                    49.846153846153854
                ],
                [
                    22,
                    21,
                    53.35463258785943
                ],
                [
                    22,
                    22,
                    56.75675675675676
                ],
                [
                    22,
                    23,
                    50.0
                ],
                [
                    22,
                    24,
                    50.91575091575091
                ],
                [
                    22,
                    25,
                    48.821548821548824
                ],
                [
                    22,
                    26,
                    53.01204819277109
                ],
                [
                    22,
                    27,
                    50.0
                ],
                [
                    22,
                    28,
                    50.72463768115942
                ],
                [
                    22,
                    29,
                    43.58974358974359
                ],
                [
                    22,
                    30,
                    45.73643410852713
                ],
                [
                    22,
                    31,
                    54.21052631578947
                ],
                [
                    22,
                    32,
                    45.8955223880597
                ],
                [
                    22,
                    33,
                    51.388888888888886
                ],
                [
                    22,
                    34,
                    53.03030303030303
                ],
                [
                    23,
                    0,
                    "MayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayansMayans"
                ],
                [
                    23,
                    1,
                    50.84598698481562
                ],
                [
                    23,
                    2,
                    48.607242339832865
                ],
                [
                    23,
                    3,
                    50.20192606399503
                ],
                [
                    23,
                    4,
                    51.88431200701139
                ],
                [
                    23,
                    5,
                    54.52091767881242
                ],
                [
                    23,
                    6,
                    53.56265356265356
                ],
                [
                    23,
                    7,
                    47.15832205683356
                ],
                [
                    23,
                    8,
                    53.00859598853869
                ],
                [
                    23,
                    9,
                    51.83553597650514
                ],
                [
                    23,
                    10,
                    53.15721649484536
                ],
                [
                    23,
                    11,
                    44.81216457960644
                ],
                [
                    23,
                    12,
                    42.82955094157412
                ],
                [
                    23,
                    13,
                    46.78298800436205
                ],
                [
                    23,
                    14,
                    46.534653465346544
                ],
                [
                    23,
                    15,
                    58.23817292006525
                ],
                [
                    23,
                    16,
                    50.95693779904307
                ],
                [
                    23,
                    17,
                    52.056074766355145
                ],
                [
                    23,
                    18,
                    48.10744810744811
                ],
                [
                    23,
                    19,
                    59.43396226415094
                ],
                [
                    23,
                    20,
                    51.72872340425532
                ],
                [
                    23,
                    21,
                    47.983870967741936
                ],
                [
                    23,
                    22,
                    49.92412746585736
                ],
                [
                    23,
                    23,
                    49.08424908424908
                ],
                [
                    23,
                    24,
                    50.0
                ],
                [
                    23,
                    25,
                    54.45321307779031
                ],
                [
                    23,
                    26,
                    50.31802120141343
                ],
                [
                    23,
                    27,
                    54.43037974683544
                ],
                [
                    23,
                    28,
                    57.68799102132436
                ],
                [
                    23,
                    29,
                    52.103960396039604
                ],
                [
                    23,
                    30,
                    49.46483180428135
                ],
                [
                    23,
                    31,
                    58.51318944844125
                ],
                [
                    23,
                    32,
                    49.70760233918128
                ],
                [
                    23,
                    33,
                    59.280575539568346
                ],
                [
                    23,
                    34,
                    54.09652076318743
                ],
                [
                    24,
                    0,
                    "MongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongolsMongols"
                ],
                [
                    24,
                    1,
                    46.97885196374622
                ],
                [
                    24,
                    2,
                    45.41607898448519
                ],
                [
                    24,
                    3,
                    46.45555955379633
                ],
                [
                    24,
                    4,
                    52.142857142857146
                ],
                [
                    24,
                    5,
                    49.69788519637463
                ],
                [
                    24,
                    6,
                    50.505050505050505
                ],
                [
                    24,
                    7,
                    52.23342939481268
                ],
                [
                    24,
                    8,
                    50.34013605442177
                ],
                [
                    24,
                    9,
                    48.34008097165992
                ],
                [
                    24,
                    10,
                    50.76142131979695
                ],
                [
                    24,
                    11,
                    43.48387096774193
                ],
                [
                    24,
                    12,
                    42.21052631578948
                ],
                [
                    24,
                    13,
                    45.16331658291458
                ],
                [
                    24,
                    14,
                    45.465116279069775
                ],
                [
                    24,
                    15,
                    44.19410745233969
                ],
                [
                    24,
                    16,
                    53.086419753086425
                ],
                [
                    24,
                    17,
                    49.56521739130435
                ],
                [
                    24,
                    18,
                    47.60655737704918
                ],
                [
                    24,
                    19,
                    55.919854280510016
                ],
                [
                    24,
                    20,
                    45.70007107320539
                ],
                [
                    24,
                    21,
                    45.63520227111426
                ],
                [
                    24,
                    22,
                    51.419558359621455
                ],
                [
                    24,
                    23,
                    51.178451178451176
                ],
                [
                    24,
                    24,
                    45.546786922209705
                ],
                [
                    24,
                    25,
                    50.0
                ],
                [
                    24,
                    26,
                    47.64826175869121
                ],
                [
                    24,
                    27,
                    57.994579945799465
                ],
                [
                    24,
                    28,
                    48.57468643101482
                ],
                [
                    24,
                    29,
                    50.06839945280438
                ],
                [
                    24,
                    30,
                    51.217532467532465
                ],
                [
                    24,
                    31,
                    51.515151515151516
                ],
                [
                    24,
                    32,
                    51.09060402684564
                ],
                [
                    24,
                    33,
                    53.45744680851063
                ],
                [
                    24,
                    34,
                    50.63583815028901
                ],
                [
                    25,
                    0,
                    "PersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersiansPersians"
                ],
                [
                    25,
                    1,
                    47.8098788443616
                ],
                [
                    25,
                    2,
                    47.54901960784314
                ],
                [
                    25,
                    3,
                    50.30599755201959
                ],
                [
                    25,
                    4,
                    47.24540901502504
                ],
                [
                    25,
                    5,
                    50.429799426934096
                ],
                [
                    25,
                    6,
                    52.77777777777778
                ],
                [
                    25,
                    7,
                    46.47519582245431
                ],
                [
                    25,
                    8,
                    54.11985018726592
                ],
                [
                    25,
                    9,
                    51.12443778110944
                ],
                [
                    25,
                    10,
                    52.267002518891694
                ],
                [
                    25,
                    11,
                    46.54983570646221
                ],
                [
                    25,
                    12,
                    41.34615384615384
                ],
                [
                    25,
                    13,
                    48.89316650625602
                ],
                [
                    25,
                    14,
                    46.04743083003953
                ],
                [
                    25,
                    15,
                    41.69184290030212
                ],
                [
                    25,
                    16,
                    53.70843989769821
                ],
                [
                    25,
                    17,
                    53.88257575757576
                ],
                [
                    25,
                    18,
                    47.81491002570694
                ],
                [
                    25,
                    19,
                    54.492753623188406
                ],
                [
                    25,
                    20,
                    48.831775700934585
                ],
                [
                    25,
                    21,
                    49.53519256308101
                ],
                [
                    25,
                    22,
                    56.14035087719298
                ],
                [
                    25,
                    23,
                    46.987951807228924
                ],
                [
                    25,
                    24,
                    49.68197879858658
                ],
                [
                    25,
                    25,
                    52.3517382413088
                ],
                [
                    25,
                    26,
                    50.0
                ],
                [
                    25,
                    27,
                    56.709956709956714
                ],
                [
                    25,
                    28,
                    52.55319148936171
                ],
                [
                    25,
                    29,
                    48.70466321243523
                ],
                [
                    25,
                    30,
                    51.1177347242921
                ],
                [
                    25,
                    31,
                    52.02863961813843
                ],
                [
                    25,
                    32,
                    45.014245014245006
                ],
                [
                    25,
                    33,
                    50.89058524173028
                ],
                [
                    25,
                    34,
                    47.346072186836516
                ],
                [
                    26,
                    0,
                    "PortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguesePortuguese"
                ],
                [
                    26,
                    1,
                    37.800687285223376
                ],
                [
                    26,
                    2,
                    44.79166666666667
                ],
                [
                    26,
                    3,
                    38.23529411764706
                ],
                [
                    26,
                    4,
                    47.87878787878788
                ],
                [
                    26,
                    5,
                    50.602409638554214
                ],
                [
                    26,
                    6,
                    49.19354838709678
                ],
                [
                    26,
                    7,
                    48.98989898989899
                ],
                [
                    26,
                    8,
                    38.46153846153847
                ],
                [
                    26,
                    9,
                    40.963855421686745
                ],
                [
                    26,
                    10,
                    48.14814814814815
                ],
                [
                    26,
                    11,
                    42.06730769230769
                ],
                [
                    26,
                    12,
                    43.72623574144487
                ],
                [
                    26,
                    13,
                    39.726027397260275
                ],
                [
                    26,
                    14,
                    44.66019417475728
                ],
                [
                    26,
                    15,
                    36.36363636363637
                ],
                [
                    26,
                    16,
                    40.11976047904192
                ],
                [
                    26,
                    17,
                    45.63106796116505
                ],
                [
                    26,
                    18,
                    41.36546184738956
                ],
                [
                    26,
                    19,
                    44.66019417475728
                ],
                [
                    26,
                    20,
                    39.59390862944163
                ],
                [
                    26,
                    21,
                    41.43646408839779
                ],
                [
                    26,
                    22,
                    49.34210526315789
                ],
                [
                    26,
                    23,
                    50.0
                ],
                [
                    26,
                    24,
                    45.56962025316456
                ],
                [
                    26,
                    25,
                    42.00542005420054
                ],
                [
                    26,
                    26,
                    43.290043290043286
                ],
                [
                    26,
                    27,
                    50.0
                ],
                [
                    26,
                    28,
                    39.25925925925926
                ],
                [
                    26,
                    29,
                    40.35087719298245
                ],
                [
                    26,
                    30,
                    40.0
                ],
                [
                    26,
                    31,
                    35.0
                ],
                [
                    26,
                    32,
                    49.33333333333334
                ],
                [
                    26,
                    33,
                    33.87096774193548
                ],
                [
                    26,
                    34,
                    47.96747967479675
                ],
                [
                    27,
                    0,
                    "SaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracensSaracens"
                ],
                [
                    27,
                    1,
                    44.290123456790134
                ],
                [
                    27,
                    2,
                    46.41148325358852
                ],
                [
                    27,
                    3,
                    42.343268242548824
                ],
                [
                    27,
                    4,
                    50.46153846153846
                ],
                [
                    27,
                    5,
                    57.91505791505791
                ],
                [
                    27,
                    6,
                    41.6
                ],
                [
                    27,
                    7,
                    43.40909090909091
                ],
                [
                    27,
                    8,
                    56.09756097560976
                ],
                [
                    27,
                    9,
                    50.89686098654709
                ],
                [
                    27,
                    10,
                    46.92982456140351
                ],
                [
                    27,
                    11,
                    45.91002044989775
                ],
                [
                    27,
                    12,
                    47.516556291390735
                ],
                [
                    27,
                    13,
                    41.33858267716535
                ],
                [
                    27,
                    14,
                    48.63013698630137
                ],
                [
                    27,
                    15,
                    47.73869346733669
                ],
                [
                    27,
                    16,
                    51.03092783505154
                ],
                [
                    27,
                    17,
                    46.68367346938776
                ],
                [
                    27,
                    18,
                    43.96887159533074
                ],
                [
                    27,
                    19,
                    51.30890052356021
                ],
                [
                    27,
                    20,
                    44.44444444444444
                ],
                [
                    27,
                    21,
                    46.21026894865526
                ],
                [
                    27,
                    22,
                    51.674641148325364
                ],
                [
                    27,
                    23,
                    49.275362318840585
                ],
                [
                    27,
                    24,
                    42.31200897867565
                ],
                [
                    27,
                    25,
                    51.425313568985175
                ],
                [
                    27,
                    26,
                    47.44680851063829
                ],
                [
                    27,
                    27,
                    60.74074074074074
                ],
                [
                    27,
                    28,
                    50.0
                ],
                [
                    27,
                    29,
                    52.94117647058824
                ],
                [
                    27,
                    30,
                    48.08988764044944
                ],
                [
                    27,
                    31,
                    50.38167938931297
                ],
                [
                    27,
                    32,
                    49.717514124293785
                ],
                [
                    27,
                    33,
                    49.50980392156863
                ],
                [
                    27,
                    34,
                    47.70318021201414
                ],
                [
                    28,
                    0,
                    "SlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavsSlavs"
                ],
                [
                    28,
                    1,
                    54.62686567164179
                ],
                [
                    28,
                    2,
                    52.23880597014925
                ],
                [
                    28,
                    3,
                    49.61325966850829
                ],
                [
                    28,
                    4,
                    51.09034267912772
                ],
                [
                    28,
                    5,
                    43.36734693877551
                ],
                [
                    28,
                    6,
                    55.172413793103445
                ],
                [
                    28,
                    7,
                    49.63855421686747
                ],
                [
                    28,
                    8,
                    51.15511551155115
                ],
                [
                    28,
                    9,
                    54.02843601895735
                ],
                [
                    28,
                    10,
                    55.75620767494357
                ],
                [
                    28,
                    11,
                    45.56962025316456
                ],
                [
                    28,
                    12,
                    47.17868338557994
                ],
                [
                    28,
                    13,
                    47.71784232365146
                ],
                [
                    28,
                    14,
                    49.64539007092199
                ],
                [
                    28,
                    15,
                    52.84090909090909
                ],
                [
                    28,
                    16,
                    53.84615384615385
                ],
                [
                    28,
                    17,
                    50.92592592592593
                ],
                [
                    28,
                    18,
                    53.584229390681
                ],
                [
                    28,
                    19,
                    64.77987421383648
                ],
                [
                    28,
                    20,
                    51.78571428571429
                ],
                [
                    28,
                    21,
                    51.87793427230047
                ],
                [
                    28,
                    22,
                    54.891304347826086
                ],
                [
                    28,
                    23,
                    56.41025641025641
                ],
                [
                    28,
                    24,
                    47.896039603960396
                ],
                [
                    28,
                    25,
                    49.93160054719562
                ],
                [
                    28,
                    26,
                    51.29533678756477
                ],
                [
                    28,
                    27,
                    59.64912280701754
                ],
                [
                    28,
                    28,
                    47.05882352941176
                ],
                [
                    28,
                    29,
                    50.0
                ],
                [
                    28,
                    30,
                    47.58269720101781
                ],
                [
                    28,
                    31,
                    47.511312217194565
                ],
                [
                    28,
                    32,
                    47.38154613466334
                ],
                [
                    28,
                    33,
                    49.01185770750988
                ],
                [
                    28,
                    34,
                    54.48028673835126
                ],
                [
                    29,
                    0,
                    "SpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanishSpanish"
                ],
                [
                    29,
                    1,
                    49.23954372623574
                ],
                [
                    29,
                    2,
                    46.13095238095238
                ],
                [
                    29,
                    3,
                    46.666666666666664
                ],
                [
                    29,
                    4,
                    51.54826958105647
                ],
                [
                    29,
                    5,
                    46.62921348314607
                ],
                [
                    29,
                    6,
                    50.99009900990099
                ],
                [
                    29,
                    7,
                    49.056603773584904
                ],
                [
                    29,
                    8,
                    53.62637362637363
                ],
                [
                    29,
                    9,
                    52.22513089005235
                ],
                [
                    29,
                    10,
                    51.674641148325364
                ],
                [
                    29,
                    11,
                    46.96464309539693
                ],
                [
                    29,
                    12,
                    50.0
                ],
                [
                    29,
                    13,
                    45.582047685834496
                ],
                [
                    29,
                    14,
                    49.0521327014218
                ],
                [
                    29,
                    15,
                    43.79310344827586
                ],
                [
                    29,
                    16,
                    50.403225806451616
                ],
                [
                    29,
                    17,
                    50.08576329331046
                ],
                [
                    29,
                    18,
                    52.44498777506112
                ],
                [
                    29,
                    19,
                    53.40501792114696
                ],
                [
                    29,
                    20,
                    45.63253012048193
                ],
                [
                    29,
                    21,
                    48.15436241610738
                ],
                [
                    29,
                    22,
                    53.915662650602414
                ],
                [
                    29,
                    23,
                    54.263565891472865
                ],
                [
                    29,
                    24,
                    50.53516819571865
                ],
                [
                    29,
                    25,
                    48.782467532467535
                ],
                [
                    29,
                    26,
                    48.8822652757079
                ],
                [
                    29,
                    27,
                    60.0
                ],
                [
                    29,
                    28,
                    51.91011235955057
                ],
                [
                    29,
                    29,
                    52.41730279898219
                ],
                [
                    29,
                    30,
                    50.0
                ],
                [
                    29,
                    31,
                    51.540616246498594
                ],
                [
                    29,
                    32,
                    50.079744816586924
                ],
                [
                    29,
                    33,
                    51.05386416861827
                ],
                [
                    29,
                    34,
                    49.64871194379391
                ],
                [
                    30,
                    0,
                    "TatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatarsTatars"
                ],
                [
                    30,
                    1,
                    49.17763157894737
                ],
                [
                    30,
                    2,
                    42.53393665158371
                ],
                [
                    30,
                    3,
                    46.14457831325301
                ],
                [
                    30,
                    4,
                    50.0
                ],
                [
                    30,
                    5,
                    49.46236559139785
                ],
                [
                    30,
                    6,
                    48.01762114537445
                ],
                [
                    30,
                    7,
                    46.80365296803653
                ],
                [
                    30,
                    8,
                    46.00760456273764
                ],
                [
                    30,
                    9,
                    46.26436781609196
                ],
                [
                    30,
                    10,
                    49.40047961630695
                ],
                [
                    30,
                    11,
                    42.737722048066864
                ],
                [
                    30,
                    12,
                    42.281879194630875
                ],
                [
                    30,
                    13,
                    43.62292051756008
                ],
                [
                    30,
                    14,
                    39.71291866028708
                ],
                [
                    30,
                    15,
                    46.28571428571429
                ],
                [
                    30,
                    16,
                    50.38167938931297
                ],
                [
                    30,
                    17,
                    51.006711409395976
                ],
                [
                    30,
                    18,
                    47.39229024943311
                ],
                [
                    30,
                    19,
                    49.668874172185426
                ],
                [
                    30,
                    20,
                    42.50614250614251
                ],
                [
                    30,
                    21,
                    46.81724845995893
                ],
                [
                    30,
                    22,
                    51.955307262569825
                ],
                [
                    30,
                    23,
                    45.78947368421053
                ],
                [
                    30,
                    24,
                    41.48681055155875
                ],
                [
                    30,
                    25,
                    48.48484848484849
                ],
                [
                    30,
                    26,
                    47.97136038186158
                ],
                [
                    30,
                    27,
                    65.0
                ],
                [
                    30,
                    28,
                    49.61832061068702
                ],
                [
                    30,
                    29,
                    52.488687782805435
                ],
                [
                    30,
                    30,
                    48.4593837535014
                ],
                [
                    30,
                    31,
                    50.0
                ],
                [
                    30,
                    32,
                    46.176470588235304
                ],
                [
                    30,
                    33,
                    55.11363636363637
                ],
                [
                    30,
                    34,
                    54.18326693227091
                ],
                [
                    31,
                    0,
                    "TeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutonsTeutons"
                ],
                [
                    31,
                    1,
                    55.633802816901415
                ],
                [
                    31,
                    2,
                    55.16129032258065
                ],
                [
                    31,
                    3,
                    47.24738675958188
                ],
                [
                    31,
                    4,
                    49.50495049504951
                ],
                [
                    31,
                    5,
                    45.6140350877193
                ],
                [
                    31,
                    6,
                    54.54545454545454
                ],
                [
                    31,
                    7,
                    51.97080291970803
                ],
                [
                    31,
                    8,
                    51.13350125944584
                ],
                [
                    31,
                    9,
                    53.27635327635327
                ],
                [
                    31,
                    10,
                    54.285714285714285
                ],
                [
                    31,
                    11,
                    50.82617316589557
                ],
                [
                    31,
                    12,
                    54.442148760330575
                ],
                [
                    31,
                    13,
                    47.93608521970706
                ],
                [
                    31,
                    14,
                    49.14004914004914
                ],
                [
                    31,
                    15,
                    50.17301038062284
                ],
                [
                    31,
                    16,
                    50.282485875706215
                ],
                [
                    31,
                    17,
                    45.68345323741007
                ],
                [
                    31,
                    18,
                    53.5799522673031
                ],
                [
                    31,
                    19,
                    59.25925925925925
                ],
                [
                    31,
                    20,
                    52.33082706766917
                ],
                [
                    31,
                    21,
                    52.04855842185129
                ],
                [
                    31,
                    22,
                    47.58842443729904
                ],
                [
                    31,
                    23,
                    54.1044776119403
                ],
                [
                    31,
                    24,
                    50.29239766081871
                ],
                [
                    31,
                    25,
                    48.90939597315436
                ],
                [
                    31,
                    26,
                    54.98575498575499
                ],
                [
                    31,
                    27,
                    50.66666666666667
                ],
                [
                    31,
                    28,
                    50.282485875706215
                ],
                [
                    31,
                    29,
                    52.618453865336654
                ],
                [
                    31,
                    30,
                    49.92025518341308
                ],
                [
                    31,
                    31,
                    53.8235294117647
                ],
                [
                    31,
                    32,
                    50.0
                ],
                [
                    31,
                    33,
                    49.77973568281938
                ],
                [
                    31,
                    34,
                    54.196642685851316
                ],
                [
                    32,
                    0,
                    "TurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurksTurks"
                ],
                [
                    32,
                    1,
                    49.78292329956585
                ],
                [
                    32,
                    2,
                    48.837209302325576
                ],
                [
                    32,
                    3,
                    43.52078239608802
                ],
                [
                    32,
                    4,
                    45.724907063197016
                ],
                [
                    32,
                    5,
                    38.655462184873954
                ],
                [
                    32,
                    6,
                    55.47703180212014
                ],
                [
                    32,
                    7,
                    55.381165919282516
                ],
                [
                    32,
                    8,
                    51.37254901960784
                ],
                [
                    32,
                    9,
                    49.03640256959314
                ],
                [
                    32,
                    10,
                    47.08994708994709
                ],
                [
                    32,
                    11,
                    43.34140435835352
                ],
                [
                    32,
                    12,
                    53.687821612349914
                ],
                [
                    32,
                    13,
                    44.18604651162791
                ],
                [
                    32,
                    14,
                    50.43859649122807
                ],
                [
                    32,
                    15,
                    50.943396226415096
                ],
                [
                    32,
                    16,
                    51.06382978723404
                ],
                [
                    32,
                    17,
                    50.16949152542372
                ],
                [
                    32,
                    18,
                    49.92101105845181
                ],
                [
                    32,
                    19,
                    52.89855072463768
                ],
                [
                    32,
                    20,
                    47.22222222222222
                ],
                [
                    32,
                    21,
                    51.41843971631206
                ],
                [
                    32,
                    22,
                    40.77669902912621
                ],
                [
                    32,
                    23,
                    48.61111111111111
                ],
                [
                    32,
                    24,
                    40.719424460431654
                ],
                [
                    32,
                    25,
                    46.54255319148936
                ],
                [
                    32,
                    26,
                    49.10941475826972
                ],
                [
                    32,
                    27,
                    66.12903225806451
                ],
                [
                    32,
                    28,
                    50.49019607843137
                ],
                [
                    32,
                    29,
                    50.988142292490124
                ],
                [
                    32,
                    30,
                    48.94613583138173
                ],
                [
                    32,
                    31,
                    44.88636363636364
                ],
                [
                    32,
                    32,
                    50.22026431718062
                ],
                [
                    32,
                    33,
                    50.0
                ],
                [
                    32,
                    34,
                    53.00751879699248
                ],
                [
                    33,
                    0,
                    "VietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnameseVietnamese"
                ],
                [
                    33,
                    1,
                    44.89510489510489
                ],
                [
                    33,
                    2,
                    45.0
                ],
                [
                    33,
                    3,
                    49.950347567030775
                ],
                [
                    33,
                    4,
                    49.74619289340101
                ],
                [
                    33,
                    5,
                    55.32786885245902
                ],
                [
                    33,
                    6,
                    49.61240310077519
                ],
                [
                    33,
                    7,
                    42.70152505446623
                ],
                [
                    33,
                    8,
                    50.857142857142854
                ],
                [
                    33,
                    9,
                    50.10570824524313
                ],
                [
                    33,
                    10,
                    52.25806451612903
                ],
                [
                    33,
                    11,
                    42.709313264346186
                ],
                [
                    33,
                    12,
                    43.51585014409222
                ],
                [
                    33,
                    13,
                    43.47826086956522
                ],
                [
                    33,
                    14,
                    44.943820224719104
                ],
                [
                    33,
                    15,
                    44.66019417475728
                ],
                [
                    33,
                    16,
                    47.55244755244755
                ],
                [
                    33,
                    17,
                    50.0
                ],
                [
                    33,
                    18,
                    43.36448598130841
                ],
                [
                    33,
                    19,
                    46.42857142857143
                ],
                [
                    33,
                    20,
                    43.46938775510204
                ],
                [
                    33,
                    21,
                    47.34774066797642
                ],
                [
                    33,
                    22,
                    48.57142857142857
                ],
                [
                    33,
                    23,
                    46.96969696969697
                ],
                [
                    33,
                    24,
                    45.90347923681257
                ],
                [
                    33,
                    25,
                    49.36416184971098
                ],
                [
                    33,
                    26,
                    52.653927813163484
                ],
                [
                    33,
                    27,
                    52.03252032520326
                ],
                [
                    33,
                    28,
                    52.29681978798587
                ],
                [
                    33,
                    29,
                    45.51971326164875
                ],
                [
                    33,
                    30,
                    50.35128805620609
                ],
                [
                    33,
                    31,
                    45.81673306772908
                ],
                [
                    33,
                    32,
                    45.80335731414868
                ],
                [
                    33,
                    33,
                    46.99248120300752
                ],
                [
                    33,
                    34,
                    50.0
                ],
                [
                    34,
                    0,
                    "VikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikingsVikings"
                ],
                [
                    34,
                    1,
                    56.87160940325497
                ],
                [
                    34,
                    2,
                    49.87834549878346
                ],
                [
                    34,
                    3,
                    49.770792403405366
                ],
                [
                    34,
                    4,
                    56.87285223367697
                ],
                [
                    34,
                    5,
                    50.5586592178771
                ],
                [
                    34,
                    6,
                    56.209150326797385
                ],
                [
                    34,
                    7,
                    49.45717732207479
                ],
                [
                    34,
                    8,
                    52.307692307692314
                ],
                [
                    34,
                    9,
                    52.99270072992701
                ],
                [
                    34,
                    10,
                    52.493438320209975
                ],
                [
                    34,
                    11,
                    49.51865222623346
                ],
                [
                    34,
                    12,
                    56.22641509433962
                ],
                [
                    34,
                    13,
                    46.99286442405709
                ],
                [
                    34,
                    14,
                    51.20967741935484
                ],
                [
                    34,
                    15,
                    51.90615835777126
                ],
                [
                    34,
                    16,
                    48.68585732165207
                ],
                [
                    34,
                    17,
                    50.08880994671403
                ],
                [
                    34,
                    18,
                    47.98488664987405
                ],
                [
                    34,
                    19,
                    58.139534883720934
                ],
                [
                    34,
                    20,
                    52.219321148825074
                ],
                [
                    34,
                    21,
                    50.78125
                ],
                [
                    34,
                    22,
                    52.32198142414861
                ],
                [
                    34,
                    23,
                    51.3595166163142
                ],
                [
                    34,
                    24,
                    50.03422313483915
                ],
                [
                    34,
                    25,
                    53.61730899256254
                ],
                [
                    34,
                    26,
                    52.29946524064171
                ],
                [
                    34,
                    27,
                    50.42492917847026
                ],
                [
                    34,
                    28,
                    52.707581227436826
                ],
                [
                    34,
                    29,
                    53.379953379953385
                ],
                [
                    34,
                    30,
                    53.333333333333336
                ],
                [
                    34,
                    31,
                    56.19047619047619
                ],
                [
                    34,
                    32,
                    51.2630014858841
                ],
                [
                    34,
                    33,
                    50.28409090909091
                ],
                [
                    34,
                    34,
                    52.76679841897233
                ]
            ],
            "label": {
                "show": true,
                "position": "top",
                "margin": 8
            }
        }
    ],
    legend: [
        {
            "data": [
                "Win Rate"
            ],
            "selected": {
                "Win Rate": true
            },
            "show": false,
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14
        }
    ],
    tooltip: {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "textStyle": {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        },
        "borderWidth": 0
    },
    xAxis: [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "axisLabel": {
                "show": true,
                "position": "top",
                "rotate": -45,
                "margin": 8
            },
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                "Aztecs",
                "Berbers",
                "Britons",
                "Bulgarians",
                "Burmese",
                "Byzantines",
                "Celts",
                "Chinese",
                "Cumans",
                "Ethiopians",
                "Franks",
                "Goths",
                "Huns",
                "Incas",
                "Indians",
                "Italians",
                "Japanese",
                "Khmer",
                "Koreans",
                "Lithuanians",
                "Magyars",
                "Malay",
                "Malians",
                "Mayans",
                "Mongols",
                "Persians",
                "Portuguese",
                "Saracens",
                "Slavs",
                "Spanish",
                "Tatars",
                "Teutons",
                "Turks",
                "Vietnamese",
                "Vikings"
            ]
        }
    ],
    yAxis: [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "axisLabel": {
                "show": true,
                "position": "top",
                "rotate": 45,
                "margin": 8
            },
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                "Aztecs",
                "Berbers",
                "Britons",
                "Bulgarians",
                "Burmese",
                "Byzantines",
                "Celts",
                "Chinese",
                "Cumans",
                "Ethiopians",
                "Franks",
                "Goths",
                "Huns",
                "Incas",
                "Indians",
                "Italians",
                "Japanese",
                "Khmer",
                "Koreans",
                "Lithuanians",
                "Magyars",
                "Malay",
                "Malians",
                "Mayans",
                "Mongols",
                "Persians",
                "Portuguese",
                "Saracens",
                "Slavs",
                "Spanish",
                "Tatars",
                "Teutons",
                "Turks",
                "Vietnamese",
                "Vikings"
            ]
        }
    ],
    title: [
        {
            "text": "文明互殴胜率⚒️",
            "padding": 10,
            "left": 'center',
            "itemGap": 10,
            "textStyle": {
                "fontSize": 30,
                "fontFamily": "Fusion Pixel",
            },
            "subtextStyle": {
                "fontSize": 30
            }
        }
    ],
    visualMap: {
        "show": true,
        "type": "continuous",
        "min": 30,
        "max": 70,
        "inRange": {
            "color": [
                "yellow",
                "orange",
                "red"
            ]
        },
        "calculable": true,
        "inverse": false,
        "splitNumber": 5,
        "orient": "horizontal",
        "left": "center",
        "showLabel": true,
        "itemWidth": 20,
        "itemHeight": 140,
        "borderWidth": 0
    }
};


        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts5555" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts5555'));

        // 指定图表的配置项和数据
        var option = {
    backgroundColor: "white",
    animation: true,
    animationThreshold: 2000,
    animationDuration: 1000,
    animationEasing: "cubicOut",
    animationDelay: 0,
    animationDurationUpdate: 300,
    animationEasingUpdate: "cubicOut",
    animationDelayUpdate: 0,
    color: [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    series: [
        {
            "type": "pie",
            "name": "Server",
            "clockwise": true,
            "data": [
                {
                    "name": "australiasoutheast",
                    "value": 65894
                },
                {
                    "name": "brazilsouth",
                    "value": 402240
                },
                {
                    "name": "eastus",
                    "value": 432171
                },
                {
                    "name": "koreacentral",
                    "value": 25220
                },
                {
                    "name": "southeastasia",
                    "value": 114114
                },
                {
                    "name": "ukwest",
                    "value": 1085889
                },
                {
                    "name": "westeurope",
                    "value": 51915
                },
                {
                    "name": "westindia",
                    "value": 114281
                },
                {
                    "name": "westus2",
                    "value": 82254
                }
            ],
            "radius": [
                "40%",
                "55%"
            ],
            "center": [
                "50%",
                "50%"
            ],
            "roseType": "radius",
            "label": {
                "show": true,
                "position": "Outside",
                "margin": 8,
                "formatter": "{a|{a}}{abg|}\n{hr|}\n {b|{b}: }{c}  {per|{d}%}  ",
                "backgroundColor": "#eee",
                "borderColor": "#aaa",
                "borderWidth": 1,
                "borderRadius": 4,
                "rich": {
                    "a": {
                        "color": "#999",
                        "lineHeight": 22,
                        "align": "center"
                    },
                    "abg": {
                        "backgroundColor": "#e3e3e3",
                        "width": "100%",
                        "align": "right",
                        "height": 22,
                        "borderRadius": [
                            4,
                            4,
                            0,
                            0
                        ]
                    },
                    "hr": {
                        "borderColor": "#aaa",
                        "width": "100%",
                        "borderWidth": 0.5,
                        "height": 0
                    },
                    "b": {
                        "fontSize": 16,
                        "lineHeight": 33
                    },
                    "per": {
                        "color": "#eee",
                        "backgroundColor": "#334455",
                        "padding": [
                            2,
                            4
                        ],
                        "borderRadius": 2
                    }
                }
            }
        }
    ],
    legend: [
        {
            "data": [
                "australiasoutheast",
                "brazilsouth",
                "eastus",
                "koreacentral",
                "southeastasia",
                "ukwest",
                "westeurope",
                "westindia",
                "westus2"
            ],
            "selected": {},
            "show": true,
            "bottom": "bottom",
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14,
            "textStyle": {
                "color": "black",
                "fontSize": 20,
                "fontFamily": "Fusion Pixel",
            }
        }
    ],
    tooltip: {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "textStyle": {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        },
        "borderWidth": 0
    },
    title: [
        {
            "text": "服务器热度🧲",
            "left": "center",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "color": "black",
                "fontSize": 30,
                "fontFamily": "Fusion Pixel",
            }
        }
    ]
}

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts7696" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts7696'));

        // 指定图表的配置项和数据
        var option = {
    "animation": true,
    "animationThreshold": 2000,
    "animationDuration": 1000,
    "animationEasing": "cubicOut",
    "animationDelay": 0,
    "animationDurationUpdate": 300,
    "animationEasingUpdate": "cubicOut",
    "animationDelayUpdate": 0,
    "color": [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    "series": [
        {
            "type": "bar",
            "name": "RM_TEAM",
            "data": [
                265458,
                225940,
                196599,
                192815,
                186104,
                159150,
                150228,
                116617,
                107173,
                102177,
                99226,
                96342,
                96054,
                93913,
                93497,
                87729,
                84323,
                77615,
                74428,
                70549,
                66742,
                64482,
                58644,
                55065,
                51395,
                50743,
                47740,
                45941,
                43281,
                41814,
                39886,
                39326,
                35228,
                32868,
                32784
            ],
            "stack": "stack1",
            "barCategoryGap": "20%",
            "label": {
                "show": false,
                "position": "top",
                "margin": 8
            }
        },
        {
            "type": "bar",
            "name": "RM_1v1",
            "data": [
                193375,
                166356,
                164468,
                163484,
                126912,
                125738,
                120366,
                109739,
                100385,
                95724,
                86457,
                85859,
                84650,
                84298,
                83741,
                77420,
                75760,
                71471,
                63386,
                62078,
                55867,
                55730,
                53742,
                49075,
                46694,
                44299,
                43869,
                43438,
                41098,
                38553,
                38470,
                38003,
                33701,
                32494,
                26098
            ],
            "stack": "stack1",
            "barCategoryGap": "20%",
            "label": {
                "show": false,
                "position": "top",
                "margin": 8
            }
        }
    ],
    "legend": [
        {
            "data": [
                "RM_TEAM",
                "RM_1v1"
            ],
            "selected": {
                "RM_TEAM": true,
                "RM_1v1": true
            },
            "show": true,
            

            "left": "center",
            "top":100,
            "bottom": "bottom",
            "orient": "vertical",
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14,
            "textStyle": {
                "color": "black",
                "fontSize": 10
            }
            
        }
    ],
    "tooltip": {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "textStyle": {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        },
        "borderWidth": 0
    },
    "xAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "axisLabel": {
                "show": true,
                "position": "top",
                "rotate": -45,
                "margin": 8
            },
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                "Franks",
                "Britons",
                "Mayans",
                "Mongols",
                "Persians",
                "Aztecs",
                "Goths",
                "Huns",
                "Khmer",
                "Vikings",
                "Japanese",
                "Lithuanians",
                "Celts",
                "Ethiopians",
                "Magyars",
                "Cumans",
                "Spanish",
                "Teutons",
                "Chinese",
                "Bulgarians",
                "Incas",
                "Byzantines",
                "Vietnamese",
                "Saracens",
                "Slavs",
                "Tatars",
                "Berbers",
                "Turks",
                "Malay",
                "Burmese",
                "Italians",
                "Malians",
                "Indians",
                "Koreans",
                "Portuguese"
            ]
        }
    ],
    "yAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            }
        }
    ],
    "title": [
        {
            "text": "文明游玩情况⛳",
            "left": "center",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "color": "black",
                "fontSize": 30,
                "fontFamily": "Fusion Pixel",
            }
        }
    ]

};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts8301" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts8301'));

        // 指定图表的配置项和数据
        var option = {
    "animation": true,
    "animationThreshold": 2000,
    "animationDuration": 1000,
    "animationEasing": "cubicOut",
    "animationDelay": 0,
    "animationDurationUpdate": 300,
    "animationEasingUpdate": "cubicOut",
    "animationDelayUpdate": 0,
    "color": [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    "series": [
        {
            "type": "boxplot",
            "name": "RM_TEAM",
            "data": [
                [
                    301.0,
                    1600.0,
                    2039.0000000000002,
                    2462.0,
                    7130.0
                ],
                [
                    301.0,
                    1688.0,
                    2088.0,
                    2550.0,
                    7199.0
                ],
                [
                    301.0,
                    1719.0,
                    2092.0,
                    2539.0,
                    7199.0
                ],
                [
                    301.0,
                    1713.0,
                    2153.0,
                    2607.0,
                    7193.0
                ]
            ],
            "label": {
                "show": true,
                "position": "top",
                "margin": 8
            },
            "markPoint": {
                "label": {
                    "show": true,
                    "position": "inside",
                    "color": "#fff",
                    "margin": 8
                }
            },
            "markLine": {
                "silent": false,
                "precision": 2,
                "label": {
                    "show": true,
                    "position": "top",
                    "margin": 8
                }
            }
        },
        {
            "type": "boxplot",
            "name": "RM_1v1",
            "data": [
                [
                    301.0,
                    1349.0,
                    1819.0,
                    2205.0,
                    7188.0
                ],
                [
                    301.0,
                    1339.0,
                    1779.0,
                    2169.0,
                    7182.0
                ],
                [
                    301.0,
                    1369.0,
                    1799.0,
                    2179.0,
                    7199.0
                ],
                [
                    301.0,
                    1455.0,
                    1885.0000000000002,
                    2279.0,
                    7198.0
                ]
            ],
            "label": {
                "show": true,
                "position": "top",
                "margin": 8
            },
            "markPoint": {
                "label": {
                    "show": true,
                    "position": "inside",
                    "color": "#fff",
                    "margin": 8
                }
            },
            "markLine": {
                "silent": false,
                "precision": 2,
                "label": {
                    "show": true,
                    "position": "top",
                    "margin": 8
                }
            }
        }
    ],
    "legend": [
        {
            "data": [
                "RM_TEAM",
                "RM_1v1"
            ],
            "selected": {
                "RM_TEAM": true,
                "RM_1v1": true
            },
            "show": true,
            "bottom" : "bottom",
            "orient": "vertical",
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14,
            "textStyle": {
                "color": "black",
                "fontSize": 10,
                "fontFamily": "Fusion Pixel",
            }
        }
    ],
    "tooltip": {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "textStyle": {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        },
        "borderWidth": 0
    },
    "xAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                35584,
                36202,
                36906,
                37906
            ]
        }
    ],
    "yAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            }
        }
    ],
    "title": [
        {
            "text": "版本积分分布情况📋",
            "subtext": "*Have been flittered",
            "left": "center",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "color": "black",
                "fontSize": 30,
                "fontFamily": "Fusion Pixel",
            }
        }
    ]
};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts8318" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts8318'));

        // 指定图表的配置项和数据
        var option = {
    "backgroundColor": "white",
    "animation": true,
    "animationThreshold": 2000,
    "animationDuration": 1000,
    "animationEasing": "cubicOut",
    "animationDelay": 0,
    "animationDurationUpdate": 300,
    "animationEasingUpdate": "cubicOut",
    "animationDelayUpdate": 0,
    "color": [
        "Grey",
        "#FF8C00",
        "Cyan",
        "#BA55D3",
        "#FFD700",
        "#98FB98",
        "#4169E1",
        "#DC143C"
    ],
    "series": [
        {
            "type": "pie",
            "clockwise": true,
            "data": [
                {
                    "name": "Grey",
                    "value": 0.03648808734492026
                },
                {
                    "name": "Orange",
                    "value": 0.043011912035428625
                },
                {
                    "name": "Cyan",
                    "value": 0.06841333924342802
                },
                {
                    "name": "Purple",
                    "value": 0.0686110768007991
                },
                {
                    "name": "Yellow",
                    "value": 0.10676478635599838
                },
                {
                    "name": "Green",
                    "value": 0.11360793792362621
                },
                {
                    "name": "Blue",
                    "value": 0.28146825919758484
                },
                {
                    "name": "Red",
                    "value": 0.2816346010982146
                }
            ],
            "radius": "70%",
            "center": [
                "50%",
                "50%"
            ],
            "roseType": "radius",
            "label": {
                "show": true,
                "position": "top",
                "color": "rgba(0,0,0,0.8)",
                "margin": 8,
                "fontSize": 15
            },
            "tooltip": {
                "show": true,
                "trigger": "item",
                "triggerOn": "mousemove|click",
                "axisPointer": {
                    "type": "line"
                },
                "formatter": "{a} <br/>{b}: {c} ({d}%)",
                "textStyle": {
                    "fontSize": 14,
                    "fontFamily": "Fusion Pixel",
                },
                "borderWidth": 0
            },
            "rippleEffect": {
                "show": true,
                "brushType": "stroke",
                "scale": 2.5,
                "period": 4
            }
        }
    ],
    "legend": [
        {
            "data": [
                "Grey",
                "Orange",
                "Cyan",
                "Purple",
                "Yellow",
                "Green",
                "Blue",
                "Red"
            ],
            "selected": {},
            "show": false,
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14
        }
    ],
    "tooltip": {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "textStyle": {
            "fontSize": 14,
            "fontFamily": "Fusion Pixel",
        },
        "borderWidth": 0
    },
    "title": [
        {
            "text": "颜色使用情况🎨",
            "left": "center",
            "top": "0",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "fontFamily": "Fusion Pixel",
                "color": "rgba(0,0,0)",
                "fontSize": 30,
            }
        }
    ]
};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">win_team_count = joined_df[(joined_df.winner==<span class="literal">True</span>) &amp; (joined_df.ladder==<span class="string">&quot;RM_TEAM&quot;</span>)].value_counts(<span class="string">&#x27;civ&#x27;</span>,sort=<span class="literal">False</span>)</span><br><span class="line">win_team_count_data = win_team_count.to_list()</span><br><span class="line">win_team_count_index = win_team_count.index.to_list()</span><br><span class="line"></span><br><span class="line">bar = (</span><br><span class="line">    Bar()</span><br><span class="line">    .add_xaxis(win_team_count_index)</span><br><span class="line">    .add_yaxis(<span class="string">&quot;team&quot;</span>,win_team_count_data,category_gap=<span class="number">12</span>)</span><br><span class="line">    .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">&quot;示例&quot;</span>,pos_left=<span class="string">&#x27;center&#x27;</span>),</span><br><span class="line">                    legend_opts=opts.LegendOpts(pos_bottom=<span class="string">&#x27;bottom&#x27;</span>,is_show=<span class="literal">False</span>),</span><br><span class="line">                    xaxis_opts=opts.AxisOpts(</span><br><span class="line">                        axislabel_opts=&#123;<span class="string">&quot;rotate&quot;</span>:-<span class="number">45</span>,<span class="string">&#x27;interval&#x27;</span>: <span class="number">0</span> &#125;,</span><br><span class="line">                        name_textstyle_opts=opts.TextStyleOpts(</span><br><span class="line">                            font_size=<span class="number">2</span></span><br><span class="line">                        )),)</span><br><span class="line">    .set_series_opts(label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bar.render(<span class="string">&#x27;win_team_count.html&#x27;</span>)</span><br></pre></td></tr></table></figure>
<div id="echarts7120" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts7120'));

        // 指定图表的配置项和数据
        var option = {
    "animation": true,
    "animationThreshold": 2000,
    "animationDuration": 1000,
    "animationEasing": "cubicOut",
    "animationDelay": 0,
    "animationDurationUpdate": 300,
    "animationEasingUpdate": "cubicOut",
    "animationDelayUpdate": 0,
    "color": [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    "series": [
        {
            "type": "bar",
            "name": "1v1",
            "legendHoverLink": true,
            "data": [
                91022,
                33941,
                111515,
                46748,
                29658,
                37237,
                65383,
                43138,
                51391,
                63503,
                159227,
                89151,
                76998,
                43401,
                24603,
                25789,
                57209,
                72422,
                21081,
                74500,
                61684,
                27138,
                30889,
                115448,
                108927,
                80711,
                18398,
                34346,
                35160,
                48385,
                32423,
                54620,
                32286,
                36930,
                69902
            ],
            "showBackground": false,
            "barMinHeight": 0,
            "barCategoryGap": 12,
            "barGap": "30%",
            "large": false,
            "largeThreshold": 400,
            "seriesLayoutBy": "column",
            "datasetIndex": 0,
            "clip": true,
            "zlevel": 0,
            "z": 2,
            "label": {
                "show": false,
                "position": "top",
                "margin": 8
            },
            "rippleEffect": {
                "show": true,
                "brushType": "stroke",
                "scale": 2.5,
                "period": 4
            }
        }
    ],
    "legend": [
        {
            "data": [
                "1v1"
            ],
            "selected": {
                "1v1": true
            },
            "show": false,
            "bottom": "bottom",
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14
        }
    ],
    "tooltip": {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "showContent": true,
        "alwaysShowContent": false,
        "showDelay": 0,
        "hideDelay": 100,
        "textStyle": {
            "fontSize": 14
        },
        "borderWidth": 0,
        "padding": 5
    },
    "xAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "nameTextStyle": {
                "fontSize": 2
            },
            "gridIndex": 0,
            "axisLabel": {
                "rotate": -45,
                "interval": 0
            },
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                "Aztecs",
                "Berbers",
                "Britons",
                "Bulgarians",
                "Burmese",
                "Byzantines",
                "Celts",
                "Chinese",
                "Cumans",
                "Ethiopians",
                "Franks",
                "Goths",
                "Huns",
                "Incas",
                "Indians",
                "Italians",
                "Japanese",
                "Khmer",
                "Koreans",
                "Lithuanians",
                "Magyars",
                "Malay",
                "Malians",
                "Mayans",
                "Mongols",
                "Persians",
                "Portuguese",
                "Saracens",
                "Slavs",
                "Spanish",
                "Tatars",
                "Teutons",
                "Turks",
                "Vietnamese",
                "Vikings"
            ]
        }
    ],
    "yAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            }
        }
    ],
    "title": [
        {
            "text": "1v1文明胜利场次统计🎊",
            "left": "center",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "fontFamily": "Fusion Pixel",
                "color": "rgba(0,0,0)",
                "fontSize": 30,
            }
        }
    ]
};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>


<div id="echarts9520" style="width: 81%;height: 400px;margin: 0 auto"></div>
<script src="https://cdn.bootcss.com/echarts/4.1.0-release/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts9520'));

        // 指定图表的配置项和数据
        var option = {
    "animation": true,
    "animationThreshold": 2000,
    "animationDuration": 1000,
    "animationEasing": "cubicOut",
    "animationDelay": 0,
    "animationDurationUpdate": 300,
    "animationEasingUpdate": "cubicOut",
    "animationDelayUpdate": 0,
    "color": [
        "#c23531",
        "#2f4554",
        "#61a0a8",
        "#d48265",
        "#749f83",
        "#ca8622",
        "#bda29a",
        "#6e7074",
        "#546570",
        "#c4ccd3",
        "#f05b72",
        "#ef5b9c",
        "#f47920",
        "#905a3d",
        "#fab27b",
        "#2a5caa",
        "#444693",
        "#726930",
        "#b2d235",
        "#6d8346",
        "#ac6767",
        "#1d953f",
        "#6950a1",
        "#918597"
    ],
    "series": [
        {
            "type": "bar",
            "name": "team",
            "legendHoverLink": true,
            "data": [
                65469,
                35800,
                160502,
                42498,
                37732,
                42589,
                61284,
                47902,
                68633,
                81460,
                207523,
                138029,
                79585,
                35575,
                63769,
                28364,
                45380,
                115044,
                23620,
                80297,
                65188,
                27862,
                25801,
                145722,
                136307,
                100364,
                27956,
                45416,
                37761,
                70665,
                30384,
                69509,
                37228,
                53787,
                70863
            ],
            "showBackground": false,
            "barMinHeight": 0,
            "barCategoryGap": 12,
            "barGap": "30%",
            "large": false,
            "largeThreshold": 400,
            "seriesLayoutBy": "column",
            "datasetIndex": 0,
            "clip": true,
            "zlevel": 0,
            "z": 2,
            "label": {
                "show": false,
                "position": "top",
                "margin": 8
            },
            "rippleEffect": {
                "show": true,
                "brushType": "stroke",
                "scale": 2.5,
                "period": 4
            }
        }
    ],
    "legend": [
        {
            "data": [
                "team"
            ],
            "selected": {
                "team": true
            },
            "show": false,
            "bottom": "bottom",
            "padding": 5,
            "itemGap": 10,
            "itemWidth": 25,
            "itemHeight": 14
        }
    ],
    "tooltip": {
        "show": true,
        "trigger": "item",
        "triggerOn": "mousemove|click",
        "axisPointer": {
            "type": "line"
        },
        "showContent": true,
        "alwaysShowContent": false,
        "showDelay": 0,
        "hideDelay": 100,
        "textStyle": {
            "fontSize": 14
        },
        "borderWidth": 0,
        "padding": 5
    },
    "xAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "nameTextStyle": {
                "fontSize": 2
            },
            "gridIndex": 0,
            "axisLabel": {
                "rotate": -45,
                "interval": 0
            },
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            },
            "data": [
                "Aztecs",
                "Berbers",
                "Britons",
                "Bulgarians",
                "Burmese",
                "Byzantines",
                "Celts",
                "Chinese",
                "Cumans",
                "Ethiopians",
                "Franks",
                "Goths",
                "Huns",
                "Incas",
                "Indians",
                "Italians",
                "Japanese",
                "Khmer",
                "Koreans",
                "Lithuanians",
                "Magyars",
                "Malay",
                "Malians",
                "Mayans",
                "Mongols",
                "Persians",
                "Portuguese",
                "Saracens",
                "Slavs",
                "Spanish",
                "Tatars",
                "Teutons",
                "Turks",
                "Vietnamese",
                "Vikings"
            ]
        }
    ],
    "yAxis": [
        {
            "show": true,
            "scale": false,
            "nameLocation": "end",
            "nameGap": 15,
            "gridIndex": 0,
            "inverse": false,
            "offset": 0,
            "splitNumber": 5,
            "minInterval": 0,
            "splitLine": {
                "show": false,
                "lineStyle": {
                    "show": true,
                    "width": 1,
                    "opacity": 1,
                    "curveness": 0,
                    "type": "solid"
                }
            }
        }
    ],
    "title": [
        {
            "text": "团战文明胜利场次统计🎊",
            "left": "center",
            "padding": 5,
            "itemGap": 10,
            "textStyle": {
                "fontFamily": "Fusion Pixel",
                "color": "rgba(0,0,0)",
                "fontSize": 30,
            }
        }
    ]
};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>

]]></content>
      <categories>
        <category>学习记录</category>
      </categories>
      <tags>
        <tag>帝国时代2</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
</search>
